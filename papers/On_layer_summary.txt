The Transformer model, introduced by Vaswani et al. (2017), has revolutionized natural language processing (NLP) tasks, such as language modeling and machine translation, due to its effective architecture. A critical component of this architecture is the layer normalization (LayerNorm), which plays a significant role in stabilizing and speeding up the training of deep neural networks. Traditionally, the Transformer model employs Post-Layer Normalization (Post-LN), where LayerNorm is applied between the residual blocks. Despite its success, training the Post-LN Transformer from scratch necessitates a learning rate warm-up stage, which starts with an extremely small learning rate and gradually increases it. This warm-up stage is crucial for avoiding training instability but increases optimization time and requires extensive hyper-parameter tuning.

The paper addresses the necessity of the learning rate warm-up stage and investigates whether this step can be safely removed. Using mean field theory, the authors demonstrate that for Post-LN Transformers, the expected gradients of parameters near the output layer are large at initialization. This can make training unstable if a large learning rate is used from the start, thereby necessitating the warm-up stage. Specifically, the gradients in the Post-LN Transformer are of the order \(O(d\sqrt{\ln d})\), independent of the depth \(L\), leading to large gradient scales near the output layer.

In contrast, the paper explores the Pre-Layer Normalization (Pre-LN) Transformer, which positions LayerNorm inside the residual blocks and adds a final LayerNorm before prediction. The theoretical analysis reveals that in the Pre-LN Transformer, the gradients are well-behaved at initialization, meaning they do not exhibit the same large-scale behavior as in Post-LN Transformers. This is evidenced by the expected gradients being normalized by \(\sqrt{L}\), thus remaining small and stable irrespective of the model's depth.

Motivated by these theoretical insights, the authors propose removing the warm-up stage for Pre-LN Transformers. They empirically validate this hypothesis through a series of experiments on various NLP tasks, including IWSLT14 German-English translation, WMT14 English-German translation, and BERT pretraining tasks. The results demonstrate that Pre-LN Transformers trained without the warm-up stage achieve performance comparable to their Post-LN counterparts. For instance, on the IWSLT14 De-En task, the Pre-LN Transformer achieved around 34 BLEU score without the warm-up stage, similar to the Post-LN model but with significantly less training time. Additionally, the Pre-LN model showed a 40% speed-up rate in BERT pretraining, achieving comparable validation loss much faster than the Post-LN model.

The findings also suggest that removing the warm-up stage reduces the need for extensive hyper-parameter tuning. For example, the sensitivity of the Post-LN Transformer to the warm-up duration \(T_{\text{warmup}}\) was evident, where the final performance varied significantly based on \(T_{\text{warmup}}\). In contrast, the Pre-LN Transformer did not exhibit such sensitivity, simplifying the training process.

Overall, the paper concludes that the position of LayerNorm significantly impacts the gradient behavior at initialization. By adopting Pre-LN architecture, the learning rate warm-up stage can be safely eliminated, leading to faster and more efficient training, with reduced hyper-parameter tuning requirements. Future work will further explore alternative LayerNorm placements and deepen the theoretical understanding of Transformer optimization.
