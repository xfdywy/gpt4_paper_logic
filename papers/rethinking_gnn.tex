\pdfoutput=1
\documentclass{article} % For LaTeX2e

\usepackage[dvipsnames]{xcolor}
\usepackage{iclr2023_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\usepackage[colorlinks, anchorcolor=white, linkcolor=mydarkblue, urlcolor=mydarkblue, citecolor=mydarkblue]{hyperref}
\usepackage{url}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{booktabs}       % professional-quality tables
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{array}
\usepackage[noabbrev,capitalise]{cleveref}
\usepackage{crossreftools}
\usepackage{multirow}
\usepackage{makecell}
\usepackage[toc,page,header]{appendix}
\usepackage{minitoc}
\usepackage{footnote}
\usepackage{letterspace}
\usepackage{setspace}
\makesavenoteenv{table}
\makesavenoteenv{tabular}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\renewcommand \thepart{}
\renewcommand \partname{}
% \newcommand{\nocontentsline}[3]{}
% \newcommand{}[2]{\bgroup\let\addcontentsline=\nocontentsline#1{#2}\egroup}
% \renewcommand\contentsname{\huge Appendix}

\pdfstringdefDisableCommands{%
    \let\Cref\crtCref
    \let\cref\crtcref
}
\hypersetup{%
    bookmarksnumbered, bookmarksopen=true, bookmarksopenlevel=1,%
}
\renewcommand\RSsmallest{6.9pt}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}[theorem]{Example}
% \crefname{assumption}{Assumption}{Assumptions}
% \crefname{equation}{}{}
% \crefname{lemma}{Lemma}{Lemmas}
\crefname{definition}{Definition}{Definitions}
% \crefname{proposition}{Proposition}{Propositions}
% \crefname{corollary}{Corollary}{Corollaries}
% \crefname{example}{Example}{Examples}

\newcommand*{\ldblbrace}{\{\mskip-5mu\{}
\newcommand*{\rdblbrace}{\}\mskip-5mu\}}
\newcommand*{\Ldblbrace}{\left\{\mskip-5mu\left\{}
\newcommand*{\Rdblbrace}{\right\}\mskip-5mu\right\}}
\newcommand*{\dis}{{\operatorname{dis}}}
\newcommand*{\disR}{\operatorname{dis}^\mathrm{R}}
\newcommand*{\disC}{\operatorname{dis}^\mathrm{C}}
\newcommand*{\diag}{\operatorname{diag}}
% \newcommand*{\deg}{{\operatorname{deg}}}

\newcommand{\sj}[1]{\textcolor{blue}{SJ: #1}}

\title{Rethinking the Expressive Power of GNNs via Graph Biconnectivity}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Bohang Zhang\thanks{Equal Contribution.} \qquad Shengjie Luo$^*$ \qquad Liwei Wang \qquad Di He\\
\small{\texttt{zhangbohang@pku.edu.cn}, \quad\texttt{luosj@stu.pku.edu.cn}, \quad
\texttt{\{wanglw,dihe\}@pku.edu.cn} }\\
Peking University
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle

\doparttoc % Tell to minitoc to generate a toc for the parts
\faketableofcontents % Run a fake tableofcontents command for the partocs
% \part{} % Start the document part
% \parttoc % Insert the document TOC

\vspace{-5pt}

\begin{abstract}
Designing expressive Graph Neural Networks (GNNs) is a central topic in learning graph-structured data. While numerous approaches have been proposed to improve GNNs in terms of the Weisfeiler-Lehman (WL) test, generally there is still a lack of deep understanding of what additional power they can \emph{systematically} and \emph{provably} gain. In this paper, we take a fundamentally different perspective to study the expressive power of GNNs beyond the WL test. Specifically, we introduce a novel class of expressivity metrics via \emph{graph biconnectivity} and highlight their importance in both theory and practice. As biconnectivity can be easily calculated using simple algorithms that have linear computational costs, it is natural to expect that popular GNNs can learn it easily as well. However, after a thorough review of prior GNN architectures, we surprisingly find that most of them are \emph{not} expressive for \emph{any} of these metrics. The only exception is the ESAN framework \citep{bevilacqua2022equivariant}, for which we give a theoretical justification of its power. We proceed to introduce a principled and more efficient approach, called the Generalized Distance Weisfeiler-Lehman (GD-WL), which is provably expressive for all biconnectivity metrics. Practically, we show GD-WL can be implemented by a Transformer-like architecture that preserves expressiveness and enjoys full parallelizability. A set of experiments on both synthetic and real datasets demonstrates that our approach can consistently outperform prior GNN architectures.
\end{abstract}

\vspace{-5pt}

\section{Introduction}
\label{sec:introduction}
Graph neural networks (GNNs) have recently become the dominant approach for graph representation learning. Among numerous architectures, message-passing neural networks (MPNNs) are arguably the most popular design paradigm and have achieved great success in various fields \citep{gilmer2017neural,hamilton2017inductive,kipf2017semisupervised,velivckovic2018graph}. However, one major drawback of MPNNs lies in the limited expressiveness: as pointed out by \citet{xu2019powerful,morris2019weisfeiler}, they can never be more powerful than the classic 1-dimensional Weisfeiler-Lehman (1-WL) test in distinguishing non-isomorphic graphs \citep{weisfeiler1968reduction}. This inspired a variety of works to design provably more powerful GNNs that go beyond the 1-WL test.

% \vspace{-1pt}

One line of subsequent works aimed to propose GNNs that match the \emph{higher-order} WL variants \citep{morris2019weisfeiler,morris2020weisfeiler,maron2019universality,maron2019provably,geerts2022expressiveness}. While being highly expressive, such an approach suffers from severe computation/memory costs. Moreover, there have been concerns about whether the achieved expressiveness is necessary for real-world tasks \citep{velivckovic2022message}. In light of this, other recent works sought to develop new GNN architectures with improved expressiveness while still keeping the message-passing framework for efficiency \citep[and see \cref{sec:related_work_expressive_gnn} for more recent advances]{bouritsas2022improving,bodnar2021topological,bodnar2021cellular,bevilacqua2022equivariant,wijesinghe2022new}. However, %while all of the proposed architectures are claimed to be strictly more exprelssive than 1-WL, the ideas for these improvements typically come from heuristics, and on the theoretical side it is quite unclear what additional power they can provably gain. Besides, 
most of these works mainly justify their expressiveness by giving \emph{toy examples} where WL algorithms fail to distinguish, e.g., by focusing on regular graphs. On the theoretical side, it is quite unclear what additional power they can systematically and provably gain. More fundamentally,
to the best of our knowledge (see \cref{sec:other_metrics}),
there is still a lack of \emph{principled} and \emph{convincing} metrics beyond the WL hierarchy to formally measure the expressive power and to guide the design of provably better GNN architectures.

% \vspace{-1pt}

\begin{figure}[t]
    \small
    \vspace{-5pt}
    \centering
    \setlength\tabcolsep{12pt}
    \begin{tabular}{c c c}
        \includegraphics[height=9.5em]{figure/original.pdf} & \includegraphics[height=9.5em]{figure/bcetree.pdf} & \includegraphics[height=9.5em]{figure/bcvtree.pdf}\\
        (a) Original graph & (b) Block cut-edge tree & (c) Block cut-vertex tree
    \end{tabular}
    \vspace{-7pt}
    \caption{An illustration of edge-biconnectivity and vertex-biconnectivity. Cut vertices/edges are outlined in bold red. Gray nodes in (b)/(c) are edge/vertex-biconnected components, respectively. }
    \label{fig:block_cut_tree}
    \vspace{-10pt}
\end{figure}

In this paper, we systematically study the problem of designing expressive GNNs from a novel perspective of \emph{graph biconnectivity}. Biconnectivity has long been a central topic in graph theory \citep{bollobas1998modern}. It comprises a series of important concepts such as cut vertex (articulation point), cut edge (bridge), biconnected component, and block cut tree (see \cref{sec:preliminary} for formal definitions). %We will present formal definitions of these concepts in \cref{sec:preliminary}.
Intuitively, biconnectivity provides a structural description of a graph by decomposing it into disjoint sub-components and linking them 
% into a whole
via cut vertices/edges to form a \emph{tree} structure (cf. \cref{fig:block_cut_tree}(b,c)). As can be seen, biconnectivity purely captures the intrinsic structure of a graph.

% \vspace{-1pt}

The significance of graph biconnectivity can be reflected in various aspects. \emph{Firstly}, from a theoretical point of view, it is a basic graph property and is linked to many fundamental topics in graph theory, ranging from path-related problems to network flow \citep{granot1985substitutes} and spanning trees \citep{kapoor1995algorithms}, and is highly relevant to planar graph isomorphism \citep{hopcroft1972isomorphism}. %Therefore, the ability of distinguishing biconnectivity structures such as cut vertices/edges may generally help various graph tasks in application. 
\emph{Secondly}, from a practical point of view, cut vertices/edges have substantial values in many real applications. For example, chemical reactions are highly related to edge-biconnectivity of the molecule graph, where the breakage of molecular bonds usually occurs at the cut edges and each biconnected component often remains unchanged after the reaction. As another example, social networks are related to vertex-biconnectivity, where cut vertices play an important role in linking between different groups of people (biconnected components). 
\emph{Finally}, from a computational point of view, the problems related to biconnectivity (e.g., finding cut vertices/edges or constructing block cut trees) can all be efficiently solved using classic algorithms \citep{tarjan1972depth}, with a computation complexity \emph{equal to graph size} (which is the same as an MPNN). Therefore, one may naturally expect that popular GNNs should be able to learn all things related to biconnectivity without difficulty.

% \vspace{-1pt}

Unfortunately, we show this is not the case. After a thorough analysis of four classes of representative GNN architectures in literature (see \cref{sec:counterexamples}), we find that surprisingly, none of them could even solve the \emph{easiest} biconnectivity problem: to distinguish whether a graph has cut vertices/edges or not (corresponding to a graph-level binary classification). As a result, they obviously failed in the following harder tasks: $(\mathrm{i})$ identifying all cut vertices (a node-level task); $(\mathrm{ii})$ identifying all cut edges (an edge-level task); $(\mathrm{iii})$ the graph-level task for general biconnectivity problems, e.g., distinguishing a pair of graphs that have non-isomorphic block cut trees. This raises the following question: \emph{can we design GNNs with provable expressiveness for biconnectivity problems?}

% \vspace{-1pt}

We first give an \emph{affirmative} answer to the above question. By conducting a deep analysis of the recently proposed Equivariant Subgraph Aggregation Network (ESAN) \citep{bevilacqua2022equivariant}, we prove that the DSS-WL algorithm with \emph{node marking} policy can precisely identify both cut vertices and cut edges. This provides a new understanding as well as a strong theoretical justification for the expressive power of DSS-WL and its recent extensions \citep{frasca2022Understanding}. Furthermore, we give a fine-grained analysis of several key factors in the framework, such as the graph generation policy and the aggregation scheme, by showing that \emph{neither} $(\mathrm{i})$ the ego-network policy without marking \emph{nor} $(\mathrm{ii})$ a variant of the weaker DS-WL algorithm can identify cut vertices. 
% This partially answers an open problem raised in \citet[Section 5]{bevilacqua2022equivariant}.

% \vspace{-1pt}

However, GNNs designed based on DSS-WL are usually sophisticated and suffer from high computation/memory costs. The \textbf{main contribution} in this paper is then to give a \emph{principled} and \emph{efficient} way to design GNNs that are expressive for biconnectivity problems.
% In fact, DSS-WL requires $n$ time more complexity in both space and time than the classic 1-WL for a graph with $n$ nodes, which hinders its applicability in large-scale tasks and calls into question whether more efficient approaches exist.
Targeting this question, we restart from the classic 1-WL algorithm and figure out a major weakness in distinguishing biconnectivity: the lack of \emph{distance information} between nodes. Indeed, the importance of distance information is theoretically justified in our proof for analyzing the expressive power of DSS-WL. To this end, we introduce a novel color refinement framework, formalized as Generalized Distance Weisfeiler-Lehman (GD-WL), by directly encoding a general distance metric into the WL aggregation procedure. We first prove that as a special case, the Shortest Path Distance WL (SPD-WL) is expressive for all edge-biconnectivity problems, thus providing a novel understanding of its empirical success. However, it still cannot identify cut vertices. We further suggest an alternative called the Resistance Distance WL (RD-WL) for vertex-biconnectivity. To sum up, all biconnectivity problems can be provably solved within our proposed GD-WL framework.

Finally, we give a worst-case analysis of the proposed GD-WL framework. We discuss its limitations by proving that the expressive power of both SPD-WL and RD-WL can be bounded by the standard 2-FWL test \citep{cai1992optimal}. Consequently, 2-FWL is fully expressive for all biconnectivity metrics.  Besides, since GD-WL heavily relies on distance information, we proceed to analyze its power in distinguishing the class of \emph{distance-regular graphs} \citep{brouwer1989distance}. Surprisingly, we show GD-WL \emph{matches} the power of 2-FWL in this case, which strongly justifies its high expressiveness in distinguishing hard graphs. A summary of our theoretical contributions is given in \cref{tab:summary_of_results}. 

% \vspace{-1pt}

\begin{table}[t]
    \vspace{-12pt}
    \centering
    \small
    \setlength\tabcolsep{2pt}
    \caption{Summary of theoretical results on the expressive power of different GNN models for various biconnectivity problems. We also list the time/space complexity (per WL iteration) for each WL algorithm, where $n$ and $m$ are the number of nodes and edges of a graph, respectively.}
    \label{tab:summary_of_results}
    \vspace{2pt}
    \begin{tabular}{c|cccc|cc|cc|c}
    \Xhline{0.75pt}
     & \multicolumn{4}{c|}{\cref{sec:counterexamples}} & \multicolumn{2}{c|}{\cref{sec:esan}}  & \multicolumn{3}{c}{\cref{sec:gdwl}}\\
    \cline{2-10}
    Model & MPNN & GSN & CWN & GraphSNN & \multicolumn{2}{c|}{ESAN} & \multicolumn{2}{c|}{Ours} & 3-IGN\\
    WL variant & 1-WL & SC-WL & CWL & OS-WL & DSS-WL & DS-WL & SPD-WL & GD-WL & 2-FWL\\
    \hline
    Cut vertex & \xmark & \xmark & \xmark & \xmark & \cmark & \xmark & \xmark & \cmark & \cmark \\
    Cut edge & \xmark & \xmark & \xmark & \xmark & \cmark & Unknown & \cmark & \cmark & \cmark \\
    BCVTree & \xmark & \xmark & \xmark & \xmark & \cmark & Unknown & \xmark & \cmark & \cmark \\
    BCETree & \xmark & \xmark & \xmark & \xmark & \cmark & Unknown & \cmark & \cmark & \cmark \\
    \hline
    Ref. Theorem & - & \ref{thm:scwl} & \ref{thm:swl_cwl} & \ref{thm:oswl} & \ref{thm:dsswl} & \ref{thm:dswl_adaptation} & \ref{thm:spdwl} & \ref{thm:rdwl}, \ref{thm:gdwl} & \ref{thm:2fwl_biconnectivity}  \\
    \hline
    Time & $n\!+\!m$ & $n\!+\!m$ & - & $n\!+\!m$ & $n(n\!+\!m)$ & $n(n\!+\!m)$ & $n^2$ & $n^2$ & $n^3$\\
    Space\footnote{The space complexity of WL algorithms may differ from the corresponding GNN models in training, e.g., for DS-WL and GD-WL, due to the need to store intermediate results for back-propagation.} & $n$ & $n$ & - & $n$ & $n^2$ & $n$ & $n$ & $n$ & $n^2$\\
    \Xhline{0.75pt}
    \end{tabular}
    \vspace{-8pt}
\end{table}

\textbf{Practical Implementation}.
The main advantage of GD-WL lies in its simplicity, efficiency and \emph{parallelizability}. We show it can be easily implemented using a Transformer-like architecture by injecting the distance into Multi-head Attention \citep{vaswani2017attention}, similar to \citet{ying2021transformers}. Importantly, we prove that the resulting Graph Transformer (called Graphormer-GD) is \emph{as expressive as} GD-WL. This offers strong theoretical insights into the power and limits of Graph Transformers. Empirically, we show Graphormer-GD not only achieves perfect accuracy in detecting cut vertices and cut edges, but also outperforms prior GNN achitectures on popular benchmark datasets.
\vspace{-2pt}

\section{Preliminary}
\label{sec:preliminary}
\vspace{-1pt}
\textbf{Notations}. We use $\{\ \}$ to denote sets and use $\ldblbrace\ \rdblbrace$ to denote multisets. The cardinality of (multi)set $\gS$ is denoted as $|\gS|$. The index set is denoted as $[n]:=\{1,\cdots,n\}$. Throughout this paper, we consider simple undirected graphs $G=(\gV,\gE)$ with no repeated edges or self-loops. Therefore, each edge $\{u,v\}\in\gE$ can be expressed as a set of two elements. For a node $u\in\gV$, denote its \emph{neighbors} as $\gN_G(u):=\{v\in\gV:\{u,v\}\in\gE\}$ and denote its \emph{degree} as $\deg_G(u):=|\gN_G(u)|$. A \emph{path} $P=(u_0,\cdots,u_d)$ is a tuple of nodes satisfying $\{u_{i-1},u_i\}\in\gE$ for all $i\in[d]$, and its length is denoted as $|P|:=d$. A path $P$ is said to be \emph{simple} if it does not go through a node more than once, i.e. $u_i\neq u_j$ for $i\neq j$. The shortest path distance between two nodes $u$ and $v$ is denoted to be $\dis_G(u,v):=\min\{|P|:P\text{ is a path from }u\text{ to }v\}$. The \emph{induced subgraph} with vertex subset $\gS\subset\gV$ is defined as $G[\gS]=(\gS,\gE_\gS)$ where $\gE_\gS:=\{\{u,v\}\in\gE:u,v\in\gS\}$.


We next introduce the concepts of connectivity, vertex-biconnectivity and edge-biconnectivity.

\begin{definition}
\label{connectivity}
\normalfont (\textbf{Connectivity}) A graph $G$ is \emph{connected} if for any two nodes $u,v\in\gV$, there is a path from $u$ to $v$. A vertex set $\gS\subset\gV$ is a \emph{connected component} of $G$ if $G[\gS]$ is connected and for any proper superset $\gT\supsetneq\gS$, $G[\gT]$ is disconnected. Denote $\mathrm{CC}(G)$ as the set of all connected components, then $\mathrm{CC}(G)$ forms a \emph{partition} of the vertex set $\gV$. Clearly, $G$ is connected iff $|\mathrm{CC}(G)|=1$.
\end{definition}

\begin{definition}
\label{vertex_biconnectivity}
\normalfont (\textbf{Biconnectivity}) A node $v\in\gV$ is a \emph{cut vertex} (or \emph{articulation point}) of $G$ if removing $v$ increases the number of connected components, i.e., $|\mathrm{CC}(G[\gV\backslash\{v\}])|>|\mathrm{CC}(G)|$. A graph is \emph{vertex-biconnected} if it is connected and does not have any cut vertex. A vertex set $\gS\subset\gV$ is a \emph{vertex-biconnected component} of $G$ if $G[\gS]$ is vertex-biconnected and for any proper superset $\gT\supsetneq\gS$, $G[\gT]$ is not vertex-biconnected. We can similarly define the concepts of \emph{cut edge} (or \emph{bridge}) and \emph{edge-biconnected component} (we omit them for brevity). Finally, denote $\mathrm{BCC}^\mathrm{V}(G)$ (resp. $\mathrm{BCC}^\mathrm{E}(G)$) as the set of all vertex-biconnected (resp. edge-biconnected) components.
\end{definition}

\vspace{-4pt}

Two non-adjacent nodes $u,v\in\gV$ are in the same vertex-biconnected component iff there are two paths from $u$ to $v$ that do not intersect (except at endpoints). Two nodes $u,v$ are in the same edge-biconnected component iff there are two paths from $u$ to $v$ that do not share an edge. On the other hand, if two nodes are in different vertex/edge-biconnected components, any path between them must go through some cut vertex/edge. Therefore, cut vertices/edges can be regarded as ``hubs'' in a graph that link different subgraphs into a whole. Furthermore, the link between cut vertices/edges and biconnected components forms a \emph{tree} structure, which are called the \emph{block cut tree} (cf. \cref{fig:block_cut_tree}).

\begin{definition}
\label{def:bcetree}
\normalfont (\textbf{Block cut-edge tree}) The block cut-edge tree of graph $G=(\gV,\gE)$ is defined as follows: $\operatorname{BCETree}(G):=(\mathrm{BCC}^\mathrm{E}(G),\gE^\mathrm{E})$, where
\begin{equation*}
\setlength{\abovedisplayskip}{2pt}
\setlength{\belowdisplayskip}{0pt}
    \gE^\mathrm{E}:=\left\{\{\gS_1,\gS_2\}:\gS_1,\gS_2\in\mathrm{BCC}^\mathrm{E}(G),\exists u\in\gS_1,v\in\gS_2,\text{s.t. }\{u,v\}\in\gE\right\}.
\end{equation*}
\end{definition}
\begin{definition}
\label{def:bcvtree}
\normalfont (\textbf{Block cut-vertex tree}) The block cut-vertex tree of graph $G=(\gV,\gE)$ is defined as follows: $\operatorname{BCVTree}(G):=(\mathrm{BCC}^\mathrm{V}(G)\cup\gV^\mathrm{Cut},\gE^\mathrm{V})$, where $\gV^\mathrm{Cut}\subset\gV$ is the set containing all cut vertices of $G$ and
\begin{equation*}
\setlength{\abovedisplayskip}{0pt}
\setlength{\belowdisplayskip}{0pt}
    \gE^\mathrm{V}:=\left\{\{\gS,v\}:\gS\in\mathrm{BCC}^\mathrm{V}(G),v\in\gV^\mathrm{Cut},v\in\gS\right\}.
\end{equation*}
\end{definition}
\vspace{-5pt}

The following theorem shows that all concepts related to biconnectivity can be efficiently computed.

\begin{theorem}
\label{thm:tarjan}
\citep{tarjan1972depth} The problems related to biconnectivity, including identifying all cut vertices/edges, finding all biconnected components ($\mathrm{BCC}^\mathrm{V}(G)$ and $\mathrm{BCC}^\mathrm{E}(G)$), and building block cut trees ($\operatorname{BCVTree}(G)$ and $\operatorname{BCETree}(G)$), can all be solved using the Depth-First Search algorithm, within a computation complexity linear in the graph size, i.e. $\Theta(|\gV|+|\gE|)$.
\end{theorem}

\vspace{-4pt}

\textbf{Isomorphism and color refinement algorithms}. Two graphs $G=(\gV_G,\gE_G)$ and $H=(\gV_H,\gE_H)$ are \emph{isomorphic} (denoted as $G\simeq H$) if there is an \emph{isomorphism} (bijective mapping) $f:\gV_G\to\gV_H$ such that for any nodes $u,v\in\gV_G$, $\{u,v\}\in\gE_G$ iff $\{f(u),f(v)\}\in\gE_H$. A color refinement algorithm is an algorithm that outputs a \emph{color mapping} $\chi_G:\gV_G\to\gC$ when taking graph $G$ as input, where $\gC$ is called the \emph{color set}. A valid color refinement algorithm must preserve \emph{invariance} under isomorphism, i.e., $\chi_G(u)=\chi_H(f(u))$ for isomorphism $f$ and node $u\in\gV_G$. As a result, it can be used as a necessary test for graph isomorphism by comparing the multisets $\ldblbrace \chi_G(u):u\in\gV_G\rdblbrace$ and $\ldblbrace \chi_H(u):u\in\gV_H\rdblbrace$, which we call the \emph{graph representations}. Similarly, $\chi_G(u)$ can be seen as the \emph{node feature} of $u\in\gV_G$, and $\ldblbrace \chi_G(u),\chi_G(v)\rdblbrace$ corresponds to the {edge feature} of $\{u,v\}\in\gE_G$. All algorithms studied in this paper fit the color refinement framework, and please refer to \cref{sec:algorithms} for a precise description of several representatives (e.g., the classic 1-WL and $k$-FWL algorithms).

\textbf{Problem setup}. This paper focuses on the following three types of problems with increasing difficulties. \emph{Firstly}, we say a color refinement algorithm can distinguish whether a graph is vertex/edge-biconnected, if for any graphs $G,H$ where $G$ is vertex/edge-biconnected but $H$ is not, their graph representations are different, i.e. $\ldblbrace \chi_G(u):u\in\gV_G\rdblbrace\neq\ldblbrace \chi_H(u):u\in\gV_H\rdblbrace$. \emph{Secondly}, we say a color refinement algorithm can identify cut vertices if for any graphs $G, H$ and nodes $u\in\gV_G,v\in\gV_H$ where $u$ is a cut vertex but $v$ is not, their node features are different, i.e. $\chi_G(u)\neq \chi_H(v)$. Similarly, it can identify cut edges if for any $\{u,v\}\in\gE_G$ and $\{w,x\}\in\gE_H$ where $\{u,v\}$ is a cut edge but $\{w,x\}$ is not, their edge features are different, i.e. $\ldblbrace\chi_G(u),\chi_G(v)\rdblbrace\neq \ldblbrace\chi_H(w),\chi_H(x)\rdblbrace$. \emph{Finally}, we say a color refinement algorithm can distinguish block cut-vertex/edge trees, if for any graphs $G,H$ satisfying $\operatorname{BCVTree}(G)\not\simeq\operatorname{BCVTree}(H)$ (or $\operatorname{BCETree}(G)\not\simeq\operatorname{BCETree}(H)$), their graph representations are different, i.e. $\ldblbrace \chi_G(u):u\in\gV_G\rdblbrace\neq\ldblbrace \chi_H(u):u\in\gV_H\rdblbrace$.

% For brevity, we assume that graphs have no node features or edge features, yet all the presented algorithms in this paper can be straightforwardly extended to the general setting that enables node/edge features.

\section{Investigating Known GNN Architectures via Biconnectivity}
\label{sec:biconnect}
In this section, we provide a comprehensive investigation of popular GNN variants in literature, including the classic MPNNs, Graph Substructure Networks (GSN) \citep{bouritsas2022improving} and its variant \citep{barcelo2021graph}, GNN with lifting transformations (MPSN and CWN) \citep{bodnar2021topological,bodnar2021cellular}, GraphSNN \citep{wijesinghe2022new}, and Subgraph GNNs (e.g., \citet{bevilacqua2022equivariant}). Surprisingly, we find most of these works are not expressive for \emph{any} biconnectivity problems listed above. The only exceptions are the ESAN \citep{bevilacqua2022equivariant} and several variants, where we give a rigorous justification of their expressive power for both vertex/edge-biconnectivity.


\subsection{Counterexamples}
\label{sec:counterexamples}
\textbf{1-WL/MPNNs}. We first consider the classic 1-WL. We provide two principled class of counterexamples which are formally defined in \cref{example:1,example:2}, with a few special cases illustrated in \cref{fig:counterexamples}. For each pair of graphs in \cref{fig:counterexamples}, the color of each node is drawn according to the 1-WL color mapping. It can be seen that the two graph representations are the same. Therefore,  1-WL cannot distinguish any biconnectivity problem listed in \cref{sec:preliminary}.

\textbf{Substructure Counting WL/GSN}. \citet{bouritsas2022improving} developed a principled approach to boost the expressiveness of MPNNs by incorporating \emph{substructure counts} into node features or the 1-WL aggregation procedure. The resulting algorithm, which we call the SC-WL, is detailed in \cref{sec:scwl}. However, we show no matter what sub-structures are used, the corresponding GSN still cannot solve any biconnectivity problem listed in \cref{sec:preliminary}. We give a proof in \cref{sec:counterexample_proof} for the \emph{general} case that allows arbitrary substructures, based on \cref{example:1,example:2}. We also point out that our negative result applies to the similar GNN variant in \citet{barcelo2021graph}.

\begin{theorem}
\label{thm:scwl}
Let $\gH=\{H_1,\cdots,H_k\}$, $H_i=(\gV_i,\gE_i)$ be any set of connected graphs and denote $n=\max_{i\in[k]}|\gV_i|$. Then SC-WL (\cref{sec:scwl}) using the substructure set $\gH$ cannot solve any vertex/edge-biconnectivity problem listed in \cref{sec:preliminary}. Moreover, there exist counterexample graphs whose sizes (both in terms of vertices and edges) are $O(n)$.
\end{theorem}

\vspace{-3pt}

\textbf{GNNs with lifting transformations (MPSN/CWN)}. \citet{bodnar2021topological,bodnar2021cellular} considered another approach to design powerful GNNs by using graph \emph{lifting} transformations. In a nutshell, these approaches exploit higher-order graph structures such as cliques and cycles to design new WL aggregation procedures. Unfortunately, we show the resulting algorithms, called the SWL and CWL, still cannot solve any biconnectivity problem. Please see \cref{sec:counterexample_proof} (\cref{thm:swl_cwl}) for details.
% We give a proof in \cref{sec:counterexample_proof}.

% \begin{proposition}
% Consider the SWL algorithm \citep{bodnar2021topological}, or more generally, the CWL algorithms with either $k$-CL, $k$-IC, or $k$-C as lifting maps \citep[Definition 14]{bodnar2021cellular}. These algorithms can neither distinguish vertex-biconnectivity nor edge-biconnectivity.
% \end{proposition}

% \vspace{-3pt}

\textbf{Other GNN variants}. In \cref{sec:counterexample_proof}, we discuss other recently proposed GNNs, such as GraphSNN \citep{wijesinghe2022new}, GNN-AK \citep{zhao2022stars}, and NGNN \citep{zhang2021nested}. Due to space limit, we defer the corresponding negative results in \cref{thm:oswl,thm:gnnak,thm:dswl_adaptation}.


\begin{figure}[t]
    \vspace{-10pt}
    \centering
    \small
    \setlength\tabcolsep{12pt}
    \begin{tabular}{cccc}
        \includegraphics[height=5.5em]{figure/counterexample1_up.pdf} & \includegraphics[height=5.5em]{figure/counterexample2_up.pdf} & \includegraphics[height=5.5em]{figure/counterexample3_up.pdf} & \includegraphics[height=5.5em]{figure/counterexample4_up.pdf}\\
        \includegraphics[height=5.5em]{figure/counterexample1_down.pdf} & \includegraphics[height=5.5em]{figure/counterexample2_down.pdf} & \includegraphics[height=5.5em]{figure/counterexample3_down.pdf} & \includegraphics[height=5.5em]{figure/counterexample4_down.pdf}\\
        (a) & (b) & (c) & (d)
    \end{tabular}
    \vspace{-7pt}
    \caption{\looseness=-1 Illustration of four representative counterexamples (see \cref{example:1,example:2} for general definitions). Graphs in the first row have cut vertices (outlined in bold red) and some also have cut edges (denoted as red lines), while graphs in the second row do not have any cut vertex or cut edge.}
    \label{fig:counterexamples}
    \vspace{-10pt}
\end{figure}

\subsection{Provable expressiveness of ESAN and DSS-WL}
\label{sec:esan}
We next switch our attention to a new type of GNN framework proposed in \citet{bevilacqua2022equivariant}, called the Equivariant Subgraph Aggregation Networks (ESAN). The central algorithm in EASN is called the DSS-WL. Given a graph $G$, DSS-WL first generates a bag of vertex-shared (sub)graphs $\gB^\pi_G=\ldblbrace G_1,\cdots,G_m\rdblbrace$ according to a graph generation policy $\pi$. Then in each iteration $t$, the algorithm refines the color of each node $v$ in each subgraph $G_i$ by jointly aggregating its neighboring colors in the own subgraph and across all subgraphs. The aggregation formula can be written as:
\begin{align}
\label{eq:dsswl_aggregation}
    \chi_{G_i}^t(v)&:= \operatorname{hash}\left(\chi_{G_i}^{t-1}(v),\ldblbrace \chi_{G_i}^{t-1}(u):u\in \mathcal N_{G_i}(v) \rdblbrace,\chi_{G}^{t-1}(v),\ldblbrace \chi_{G}^{t-1}(u):u\in \mathcal N_G(v) \rdblbrace\right),\\
    \chi_G^t(v)&:= \operatorname{hash}\left(\ldblbrace \chi_{G_i}^{t}(v):i\in [m]\rdblbrace\right),
\end{align}
where $\operatorname{hash}$ is a perfect hash function. DSS-WL terminates when $\chi_G^t$ induces a stable vertex partition. In this paper, we consider \emph{node-based} graph generation policies, for which each subgraph is associated to a specific node, i.e. $\gB^\pi_G=\ldblbrace G_v:v\in\gV\rdblbrace$. Some popular choices are node deletion $\pi_\mathrm{ND}$, node marking $\pi_\mathrm{NM}$, $k$-ego-network $\pi_{\mathrm{EGO}(k)}$, and its node marking version $\pi_{\mathrm{EGOM}(k)}$. A full description of DSS-WL as well as different policies can be found in \cref{sec:dsswl} (\cref{alg:dsswl}).

A fundamental question regarding DSS-WL is how expressive it is. While a straightforward analysis shows that DSS-WL is strictly more powerful than 1-WL, an in-depth understanding on \emph{what additional power} DSS-WL gains over 1-WL is still limited. The only new result is the very recent work of \citet{frasca2022Understanding}, who showed a 3-WL \emph{upper bound} for the expressivity of DSS-WL. Yet, such a result actually gives a limitation of DSS-WL rather than showing its power. Moreover, there is a large gap between the highly strong 3-WL and the weak 1-WL. In the following, we take a different perspective and prove that DSS-WL is expressive for both types of biconnectivity problems.

\begin{theorem}
\label{thm:dsswl}
Let $G=(\gV_G,\gE_G)$ and $H=(\gV_H,\gE_H)$ be two graphs, and let $\chi_G$ and $\chi_H$ be the corresponding DSS-WL color mapping with node marking policy. Then the following holds:
\begin{itemize}[topsep=0pt,leftmargin=30pt]
\setlength{\itemsep}{0pt}
    \vspace{-3pt}
    \item For any two nodes $w\in\gV_G$ and $x\in\gV_H$, if $\chi_G(w)=\chi_H(x)$, then $w$ is a cut vertex if and only if $x$ is a cut vertex.
    \vspace{-3pt}
    \item For any two edges $\{w_1,w_2\}\in\gE_G$ and $\{x_1,x_2\}\in\gE_H$, if $\ldblbrace \chi_G(w_1),\chi_G(w_2)\rdblbrace=\ldblbrace \chi_H(x_1),\chi_H(x_2)\rdblbrace$, then $\{w_1,w_2\}$ is a cut edge if and only if $\{x_1,x_2\}$ is a cut edge.
\end{itemize}
\end{theorem}
\vspace{-3pt}

The proof of \cref{thm:dsswl} is highly technical and is deferred to \cref{sec:proof_dsswl}. By using the basic results derived in \cref{sec:property_of_wl}, we conduct a careful analysis of the DSS-WL color mapping and discover several important properties. They give insights on why DSS-WL can succeed in distinguishing biconnectivity, as we will discuss below.

\textbf{How can DSS-WL distinguish biconnectivity?} We find that a crucial advantage of DSS-WL over the classic 1-WL is that DSS-WL color mapping \emph{implicitly} encodes \emph{distance information} (see \cref{thm:proof_dsswl_key}(e) and \cref{thm:proof_dsswl_cut_edge_1}). For example, two nodes $u\in\gV_G,v\in\gV_H$ will have different DSS-WL colors if the distance set $\ldblbrace \dis_G(u,w):w\in\gV_G\rdblbrace$ differs from $\ldblbrace \dis_H(v,w):w\in\gV_H\rdblbrace$. Our proof highlights that distance information plays a vital role in distinguishing edge-biconnectivity when combining with color refinement algorithms (detailed in \cref{sec:gdwl}), and it also helps distinguish vertex-biconnectivity (see the proof of \cref{thm:proof_dsswl_cut_vertex_3}). Consequently, our analysis provides a novel understanding and a strong justification for the success of DSS-WL in \emph{two} aspects: the graph representation computed by DSS-WL intrinsically encodes distance and biconnectivity information, both of which are fundamental structural properties of graphs but are lacking in 1-WL.

\looseness=-1 \textbf{Discussions on graph generation policies}. Note that \cref{thm:dsswl} holds for node marking policy. In fact, the ability of DSS-WL to encode distance information heavily relies on node marking as shown in the proof of \cref{thm:proof_dsswl_key}. In contrast, we prove that the ego-network policy $\pi_{\mathrm{EGO}(k)}$ cannot distinguish cut vertices (\cref{thm:ego_policy}), using the counterexample given in \cref{fig:counterexamples}(c). Therefore, our result shows an inherent advantage of node marking than the ego-network policy in distinguishing a class of non-isomorphic graphs, which is raised as an open question in \citet[Section 5]{bevilacqua2022equivariant}. It also highlights a theoretical limitation of $\pi_{\mathrm{EGO}(k)}$ compared with its node marking version $\pi_{\mathrm{EGOM}(k)}$, a subtle difference that may not have received sufficient attention yet. For example, both the GNN-AK and GNN-AK-ctx architecture \citep{zhao2022stars} cannot solve vertex-biconnectivity problems since it is similar to $\pi_{\mathrm{EGO}(k)}$ (see \cref{thm:gnnak}). On the other hand, the GNN-AK+ does not suffer from such a drawback although it also uses $\pi_{\mathrm{EGO}(k)}$, because it further adds distance encoding in each subgraph (which is more expressive than node marking).

\textbf{Discussions on DS-WL}. \citet{bevilacqua2022equivariant,cotta2021reconstruction} also considered a weaker version of DSS-WL, called the DS-WL, which aggregates the node color in each subgraph without interaction across different subgraphs (see formula (\ref{eq:dswl_update})). We show in \cref{thm:dswl_adaptation} that unfortunately, DS-WL with common node-based policies \emph{cannot} identify cut vertices when the color of each node $v$ is defined as its associated subgraph representation $G_v$. This theoretically reveals the importance of cross-graph aggregation and justifies the design of DSS-WL. Finally, we point out that \citet{qian2022ordered} very recently proposed an extension of DS-WL that adds a final cross-graph aggregation procedure, for which our negative result may not hold. It may be an interesting direction to theoretically analyze the expressiveness of this type of DS-WL in future work.

\vspace{-2pt}
\section{Generalized Distance Weisfeiler-Lehman Test}
\vspace{-2pt}
\label{sec:gdwl}
After an extensive review of prior GNN architectures, in this section we would like to formally study the following problem: can we design a principled and efficient GNN framework with provable expressiveness for biconnectivity? In fact, while in \cref{sec:esan} we have proved that DSS-WL can solve biconnectivity problems, it is still far from enough. Firstly, the corresponding GNNs based on DSS-WL is usually sophisticated due to the complex aggregation formula (\ref{eq:dsswl_aggregation}), which inspires us to study whether simpler architectures exist. More importantly, DSS-WL suffers from high computational costs in both time and memory. Indeed, it requires $\Theta(n^2)$ space and $\Theta(nm)$ time per iteration (using policy $\pi_\mathrm{NM}$) to compute node colors for a graph with $n$ nodes and $m$ edges, which is $n$ times costly than 1-WL. Given the theoretical \emph{linear} lower bound in \cref{thm:tarjan}, one may naturally raise the question of how to close the gap by developing more efficient color refinement algorithms.
% Answering this question is not only important in theory, but is also of great benefit in guiding the design of practically better GNNs that suit \emph{large-scale} tasks in application.

\vspace{-1pt}

We approach the problem by rethinking the classic 1-WL test. We argue that a major weakness of 1-WL is that it is agnostic to \emph{distance information} between nodes, partly because each node can only ``see'' its \emph{neighbors} in aggregation. On the other hand, the DSS-WL color mapping implicitly encodes distance information as shown in \cref{sec:esan}, which inspires us to formally study whether incorporating distance in the aggregation procedure is crucial for solving biconnectivity problems. To this end, we introduce a novel color refinement framework which we call Generalized Distance Weisfeiler-Lehman (GD-WL). The update rule of GD-WL is very simple and can be written as:
\begin{equation}
    \label{eq:gdwl}
    \chi_G^t(v):= \operatorname{hash}\left(\ldblbrace (d_G(v,u), \chi_G^{t-1}(u)):u\in \gV\rdblbrace\right),
\end{equation}
where $d_G$ can be an arbitrary distance metric. The full algorithm is described in \cref{alg:gdwl}.

\vspace{-1pt}

\textbf{SPD-WL for edge-biconnectivity}. As a special case, when choosing the \emph{shortest path distance} $d_G=\dis_G$, we obtain an algorithm which we call SPD-WL. It can be equivalently written as
\begin{equation}
\label{eq:spdwl}
% \setlength{\abovedisplayskip}{3pt}
% \setlength{\belowdisplayskip}{3pt}
\begin{aligned}
    \chi_G^t(v):= \operatorname{hash}&\left(\chi_G^{t-1}(v),\ldblbrace\chi_G^{t-1}(u):u\in\gN_G(v)\rdblbrace,\ldblbrace\chi_G^{t-1}(u):\dis_G(v,u)=2\rdblbrace,\right.\\
    &\left.\cdots,\ldblbrace\chi_G^{t-1}(u):\dis_G(v,u)=n-1\rdblbrace,\ldblbrace\chi_G^{t-1}(u):\dis_G(v,u)=\infty\rdblbrace\right).
\end{aligned}
\end{equation}
From (\ref{eq:spdwl}) it is clear that SPD-WL is strictly more powerful than 1-WL since it additionally aggregates the $k$-hop neighbors for all $k>1$. There have been several prior works related to SPD-WL, including using distance encoding as node features \citep{li2020distance} or performing $k$-hop aggregation for some small $k$ (see \cref{sec:related_work_distance} for more related works and discussions). Yet, these works are either purely empirical or provide limited theoretical analysis (e.g., by focusing only on regular graphs). Instead, we introduce the general and more expressive SPD-WL framework with a rather different motivation and perform a systematic study on its expressive power. Our key result confirms that SPD-WL is fully expressive for all edge-biconnectivity problems listed in \cref{sec:preliminary}.

\begin{theorem}
\label{thm:spdwl}
Let $G=(\gV_G,\gE_G)$ and $H=(\gV_H,\gE_H)$ be two graphs, and let $\chi_G$ and $\chi_H$ be the corresponding SPD-WL color mapping. Then the following holds:
\begin{itemize}[topsep=0pt,leftmargin=30pt]
\setlength{\itemsep}{0pt}
    \vspace{-3pt}
    \item For any two edges $\{w_1,w_2\}\in\gE_G$ and $\{x_1,x_2\}\in\gE_H$, if $\ldblbrace \chi_G(w_1),\chi_G(w_2)\rdblbrace=\ldblbrace \chi_H(x_1),\chi_H(x_2)\rdblbrace$, then $\{w_1,w_2\}$ is a cut edge if and only if $\{x_1,x_2\}$ is a cut edge.
    \vspace{-3pt}
    \item If $\ldblbrace \chi_G(w):w\in\gV_G\rdblbrace=\ldblbrace \chi_H(w):w\in\gV_H\rdblbrace$, then $\operatorname{BCETree}(G)\simeq\operatorname{BCETree}(H)$.
\end{itemize}
\end{theorem}

\cref{thm:spdwl} is highly non-trivial and perhaps surprising at first sight, as it combines three seemingly unrelated concepts (i.e., SPD, biconnectivity, and the WL test) into a unified conclusion. We give a proof in \cref{sec:proof_spdwl}, which separately considers two cases: $\chi_G(w_1)\neq\chi_G(w_2)$ and $\chi_G(w_1)=\chi_G(w_2)$ (see \cref{fig:counterexamples}(b,d) for examples). For each case, the key technique in the proof is to construct an auxiliary graph (\cref{def:color_graph,def:aux_graph}) that precisely characterizes the structural relationship between nodes that have specific colors (see \cref{thm:spdwl_case1_5,thm:spdwl_case2_6}). Finally, we highlight that the second item of \cref{thm:spdwl} may be particularly interesting: while distinguishing general non-isomorphic graphs are known to be hard \citep{cai1992optimal,babai2016graph}, we show distinguishing non-isomorphic graphs with different block cut-edge trees can be much easily solved by SPD-WL.

\textbf{RD-WL for vertex-biconnectivity}. Unfortunately, while SPD-WL is fully expressive for edge-biconnectivity, it is not expressive for vertex-biconnectivity. We give a simple counterexample in \cref{fig:counterexamples}(c), where SPD-WL cannot distinguish the two graphs. Nevertheless, we find that by using a different distance metric, problems related to vertex-biconnectivity can also be fully solved. We propose such a choice called the \emph{Resistance Distance} (RD) (denoted as $\disR_G$), which is also a basic metric in graph theory  \citep{doyle1984random,klein1993resistance,sanmartin2022algebraic}. Formally, the value of $\disR_G(u,v)$ is defined to be the effective resistance between nodes $u$ and $v$ when treating $G$ as an electrical network where each edge corresponds to a resistance of one ohm. We note that other generalized distances can also be considered \citep{li2020distance,velingker2022affinity}.

\looseness=-1 RD has many elegant properties. First, it is a valid \emph{metric}: indeed, RD is non-negative, semidefinite, symmetric, and satisfies the triangular inequality (see \cref{sec:detail_of_rdwl}). Moreover, similar to SPD, we also have $0\le\disR_G(u,v)\le n-1$, and $\disR_G(u,v)=\dis_G(u,v)$ if $G$ is a tree. In \cref{sec:detail_of_rdwl}, we further show that RD is highly related to the graph Laplacian and can be efficiently calculated.

\begin{theorem}
\label{thm:rdwl}
Let $G=(\gV_G,\gE_G)$ and $H=(\gV_H,\gE_H)$ be two graphs, and let $\chi_G$ and $\chi_H$ be the corresponding RD-WL color mapping. Then the following holds:
\begin{itemize}[topsep=0pt,leftmargin=30pt]
\setlength{\itemsep}{0pt}
    \vspace{-3pt}
    \item For any two nodes $w\in\gV_G$ and $x\in\gV_H$, if $\chi_G(w)=\chi_H(x)$, then $w$ is a cut vertex if and only if $x$ is a cut vertex.
    \vspace{-3pt}
    \item If $\ldblbrace \chi_G(w):w\in\gV_G\rdblbrace=\ldblbrace \chi_H(w):w\in\gV_H\rdblbrace$, then $\operatorname{BCVTree}(G)\simeq\operatorname{BCVTree}(H)$.
\end{itemize}
\end{theorem}

The form of \cref{thm:rdwl} exactly parallels \cref{thm:spdwl}, which shows that RD-WL is fully expressive for vertex-biconnectivity. We give a proof of \cref{thm:spdwl} in \cref{sec:proof_rdwl}. In particular, the proof of the second item is highly technical due to the challenges in analyzing the (complex) structure of the block cut-vertex tree. It also highlights that distinguishing non-isomorphic graphs that have different BCVTrees is much easier than the general case.

Combining \cref{thm:spdwl,thm:rdwl} immediately yields the following corollary, showing that all biconnectivity problems can be solved within our proposed GD-WL framework.
\begin{corollary}
\label{thm:gdwl}
When using both SPD and RD (i.e., by setting $d_G(u,v):=(\dis_G(u,v),\disR_G(u,v))$), the corresponding GD-WL is fully expressive for both vertex-biconnectivity and edge-biconnectivity.
\end{corollary}

\textbf{Computational cost}. The GD-WL framework only needs a complexity of $\Theta(n)$ space and $\Theta(n^2)$ time per-iteration for a graph of $n$ nodes and $m$ edges, both of which are strictly less than DSS-WL. In particular, GD-WL has the same space complexity as 1-WL, which can be crucial for large-scale tasks. On the other hand, one may ask how much computational overhead there is in preprocessing pairwise distances between nodes. We show in \cref{sec:detail_of_gdwl} that the computational cost can be trivially upper bounded by $O(nm)$ for SPD and $O(n^3)$ for RD. Note that the preprocessing step only needs to be executed once, and we find that the cost is negligible compared to the GNN architecture.

\textbf{Practical implementation}. One of the main advantages of GD-WL is its high degree of parallelizability. In particular, we find GD-WL can be easily implemented using a Transformer-like architecture by injecting distance information into Multi-head Attention \citep{vaswani2017attention}, similar to the structural encoding in Graphormer \citep{ying2021transformers}. The attention layer can be written as:
\begin{equation}
\setlength{\abovedisplayskip}{5pt}
\setlength{\belowdisplayskip}{5pt}
\label{eq:attention}
    \mathbf Y^h=\left[\phi_1^h(\mathbf D)\odot \operatorname{softmax}\left(\mathbf X\mathbf W^h_Q(\mathbf X\mathbf W^h_K)^\top+\mathbf \phi_2^h(\mathbf D)\right)\right]\mathbf X\mathbf W^h_V,
\end{equation}
where $\mathbf X\in\mathbb R^{n\times d}$ is the input node features of the previous layer, $\mathbf D\in\mathbb R^{n\times n}$ is the distance matrix such that $D_{uv}=d_G(u,v)$, $\mathbf W^h_Q,\mathbf W^h_K,\mathbf W^h_V\in\mathbb R^{d\times d_H}$ are learnable weight matrices of the $h$-th head, $\phi_1^h$ and $\phi_2^h$ are elementwise functions applied to $\mathbf D$ (possibly parameterized), and $\odot$ denotes the elementwise multiplication. The results $\mathbf Y^h\in\mathbb R^{n\times d_H}$ across all heads $h$ are then combined and projected to obtain the final output $\mathbf Y=\sum_h \mathbf Y^h\mathbf W_O^h$ where $\mathbf W_O^h\in\mathbb R^{d_H\times d}$. We call the resulting architecture Graphormer-GD, and the full structure of Graphormer-GD is provided in \cref{sec:transformer}.

It is easy to see that the mapping from $\mathbf X$ to $\mathbf Y$ in (\ref{eq:attention}) is \emph{equivariant} and simulates the GD-WL aggregation. Importantly, we have the following expressivity result, which precisely characterizes the power and limits of Graphormer-GD. We give a proof in \cref{sec:transformer}.
\begin{theorem}
\label{thm:Graphormer-GD-gdwl}
Graphormer-GD is at most as powerful as GD-WL in distinguishing non-isomorphic graphs. Moreover, when choosing proper functions $\phi_1^h$ and $\phi_2^h$ and using a sufficiently large number of heads and layers, Graphormer-GD is as powerful as GD-WL.
\end{theorem}

\textbf{On the expressivity upper bound of GD-WL}. To complete the theoretical analysis, we finally provide an upper bound of the expressive power for our proposed SPD-WL and RD-WL, by studying the relationship with the standard 2-FWL (3-WL) algorithm.

\begin{theorem}
\label{thm:2fwl_powerful_than_gdwl}
The 2-FWL algorithm is more powerful than both SPD-WL and RD-WL. Formally, the 2-FWL color mapping induces a finer vertex partition than that of both SPD-WL and RD-WL.
\end{theorem}
\vspace{-2pt}

We give a proof in \cref{sec:proof_2fwl_powerful}. Using \cref{thm:2fwl_powerful_than_gdwl}, we arrive at the important corollary:
\begin{corollary}
\label{thm:2fwl_biconnectivity}
The 2-FWL is fully expressive for both vertex-biconnectivity and edge-biconnectivity.
\end{corollary}

\textbf{A worst-case analysis of GD-WL for distance-regular graphs}. Since GD-WL heavily relies on distance information, one may wonder about its expressiveness in the worst-case scenario where distance information may not help distinguish certain non-isomorphic graphs, in particular, the class of distance-regular graphs \citep{brouwer1989distance}. Due to space limit, we provide a comprehensive study of this question in \cref{sec:distance_regular}, where we give a precise and complete characterization of what types of distance-regular graphs SPD-WL/RD-WL/2-FWL can distinguish (with both theoretical results and counterexamples). The main result is present as follows:

\begin{figure}[t]
    \vspace{-25pt}
    \centering
    \small
    \setlength\tabcolsep{6pt}
    \begin{tabular}{cccc}
        \includegraphics[height=9.5em]{figure/distance_regular_1.pdf} & \includegraphics[height=9.5em]{figure/distance_regular_2.pdf} & \includegraphics[height=9.5em]{figure/distance_regular_3.pdf} & \includegraphics[height=9.5em]{figure/distance_regular_4.pdf}\\
        Dodecahedron & Desargues graph & 4x4 rooks graph & Shrikhande graph\\
        \multicolumn{2}{c}{(a) SPD-WL fails while RD-WL succeeds.} & \multicolumn{2}{c}{(b) Both SPD-WL and RD-WL fail.}
    \end{tabular}
    \vspace{-7pt}
    \caption{Illustration of non-isomorphic distance-regular graphs.}
    \label{fig:distance_regular_graphs}
    \vspace{-10pt}
\end{figure}

\begin{theorem}
\label{thm:distance_regular_maintext}
    RD-WL is strictly more powerful than SPD-WL in distinguishing non-isomorphic distance-regular graphs. Moreover, RD-WL is as powerful as 2-FWL in distinguishing non-isomorphic distance-regular graphs.
\end{theorem}

The above theorem strongly justifies the power of resistance distance and our proposed GD-WL. Importantly, to our knowledge, this is the first result showing that a \emph{more efficient} WL algorithm can \emph{match} the expressive power of 2-FWL in distinguishing distance-regular graphs.

\section{Experiments}
\label{sec:experiments}

In this section, we perform empirical evaluations of our proposed Graphormer-GD. We mainly consider the following two sets of experiments. \emph{Firstly}, we would like to verify whether Graphormer-GD can indeed learn biconnectivity-related metrics easily as our theory predicts. \emph{Secondly}, we would like to investigate whether GNNs with sufficient expressiveness for biconnectivity can also help real-world tasks and benefit the generalization performance as well. The code and models will be made publicly available at \url{https://github.com/lsj2408/Graphormer-GD}.

\begin{wraptable}{r}{7.8cm}
\vspace{-20pt}
\caption{Accuracy on cut vertex (articulation point) and cut edge (bridge) detection tasks.}
  \vspace{4px}
  \label{tab:syn}
  \centering
  \resizebox{0.57\textwidth}{!}{ \renewcommand{\arraystretch}{1.1}
  \small
    \begin{tabular}{lcc}
    \toprule
    \thead{Model} & \thead{Cut Vertex \\ Detection}  & \thead{Cut Edge \\ Detection}\\ \midrule
    GCN~\citep{kipf2017semisupervised} & $51.5\%$$\pm$$1.3\%$ & $62.4\%$$\pm$$1.8\%$ \\
    GAT~\citep{velivckovic2018graph} & $52.0\%$$\pm$$1.3\%$ & $62.8\%$$\pm$$1.9\%$ \\
    GIN~\citep{xu2019powerful} & $53.9\%$$\pm$$1.7\%$ &  $63.1\%$$\pm$$2.2\%$ \\
    GSN~\citep{bouritsas2022improving} & $60.1\%$$\pm$$1.9\%$ & $70.7\%$$\pm$$2.1\%$ \\
    Graphormer~\citep{ying2021transformers} & $76.4\%$$\pm$$2.8\%$ & $84.5\%$$\pm$$3.3\%$ \\
    \midrule
    Graphormer-GD (ours) & $100\%$ & $100\%$ \\
    - w/o. Resistance Distance & $83.3\%$$\pm$$2.7\%$ & $100\%$ \\
    \bottomrule
    \end{tabular}
    }
\vspace{-8pt}
\end{wraptable}

\textbf{Synthetic tasks}. To test the expressive power of GNNs for biconnectivity metrics, we separately consider two tasks: $(\mathrm{i})$~Cut Vertex Detection and $(\mathrm{ii})$~Cut Edge Detection. Given a GNN model that outputs node features, we add a learnable prediction head that takes each node feature (or two node features corresponding to each edge) as input and predicts whether it is a cut vertex (cut edge) or not. The evaluation metric for both tasks is the graph-level accuracy, i.e., given a graph, the model prediction is considered correct only when all the cut vertices/edges are correctly identified. To make the results convincing, we construct a challenging dataset that comprises various types of hard graphs, including the regular graphs with cut vertices/edges and also \cref{example:1,example:2} mentioned in Section~\ref{sec:biconnect}. We also choose several GNN baselines with different levels of expressive power: $(\mathrm{i})$~classic MPNNs~\citep{kipf2017semisupervised,velivckovic2018graph,xu2019powerful}; $(\mathrm{ii})$~Graph Substructure Network~\citep{bouritsas2022improving}; $(\mathrm{iii})$~Graphormer \citep{ying2021transformers}. The details of model configurations, dataset, and training procedure are provided in \cref{sec:synthetic_detail}.

The results are presented in Table \ref{tab:syn}. It can be seen that baseline GNNs cannot perfectly solve these synthetic tasks. In contrast, the Graphormer-GD achieves 100\% accuracy on both tasks, implying that it can easily learn biconnectivity metrics even in very difficult graphs. Moreover, while using only SPD suffices to identify cut edges, it is still necessary to further incorporate RD to identify cut vertices. This is consistent with our theoretical results in \cref{thm:spdwl,thm:rdwl,thm:Graphormer-GD-gdwl}.

\textbf{Real-world tasks}. We further study the empirical performance of our Graphormer-GD on the real-world benchmark: ZINC from Benchmarking-GNNs~\citep{dwivedi2020benchmarking}.

To show the scalability of Graphormer-GD, we train our models on both ZINC-Full (consisting of 250K molecular graphs) and ZINC-Subset (12K selected graphs).

We comprehensively compare our model with prior expressive GNNs that have been publicly released. For a fair comparison, we ensure that the parameter budget of both Graphormer-GD and other compared models are around 500K, following~\citet{dwivedi2020benchmarking}.

Details of baselines and settings are presented in \cref{sec:realworld_detail}. 

The results are shown in Table \ref{tab:zinc}, where our score is averaged over four experiments with different seeds. We also list the per-epoch training time of different models on ZINC-subset as well as their model parameters. It can be seen that Graphormer-GD surpasses or matches all competitive baselines on the test set of both ZINC-Subset and ZINC-Full. Furthermore, we find that the empirical performance of compared models align with their expressive power measured by graph biconnectivity. For example, Subgraph GNNs that are expressive for biconnectivity also consistently outperform classic MPNNs by a large margin.


Compared with Subgraph GNNs, the main advantage of Graphormer-GD is that it is simpler to implement, has stronger parallelizability, while still achieving better performance. Therefore, we believe our proposed architecture is both effective and efficient and can be well extended to more practical scenarios like drug discovery.

\textbf{Other tasks}. We also perform node-level experiments on two popular datasets: the Brazil-Airports and the Europe-Airports. Due to space limit, the results are shown in \cref{sec:node_task}.

\begin{table}[t]
\vspace{-28pt}
\caption{Mean Absolute Error (MAE) on ZINC test set. Following~\citet{dwivedi2020benchmarking}, the parameter budget of compared models is set to 500k. We use $^{*}$ to indicate the best performance.}
  \vspace{2px}
  \label{tab:zinc}
  \small
  % \fontsize{8}{9}\selectfont
  % \scriptsize
  % \setlength\tabcolsep{1pt}
  \centering
  \resizebox{0.98\textwidth}{!}{ \renewcommand{\arraystretch}{1.0}
    \begin{tabular}{clcccc}
    \toprule
    \multirow{2}{*}{Method} & \multirow{2}{*}{Model} & \multirow{2}{*}{Time (s)} & \multirow{2}{*}{Params} & \multicolumn{2}{c}{Test MAE}\\
    & & & & ZINC-Subset & ZINC-Full \\ \midrule
    
    \multirow{7}{*}{MPNNs}
    & GIN~\citep{xu2019powerful} & 8.05 & 509,549 & 0.526$\pm$0.051 &  0.088$\pm$0.002 \\
    & GraphSAGE~\citep{hamilton2017inductive} & 6.02 & 505,341 & 0.398$\pm$0.002 & 0.126$\pm$0.003 \\
    & GAT~\citep{velivckovic2018graph} & 8.28 & 531,345 & 0.384$\pm$0.007 & 0.111$\pm$0.002 \\
    & GCN~\citep{kipf2017semisupervised} & 5.85 & 505,079 & 0.367$\pm$0.011 & 0.113$\pm$0.002 \\

    & MoNet~\citep{monti2017geometric} & 7.19 & 504,013 & 0.292$\pm$0.006 & 0.090$\pm$0.002 \\
    & \textls[-25]{GatedGCN-PE\citep{bresson2017residual}} & 10.74 & 505,011 & 0.214$\pm$0.006 & - \\
    & MPNN(sum)~\citep{gilmer2017neural} & - &  480,805 & 0.145$\pm$0.007 & - \\
    & PNA~\citep{corso2020principal} & - & 387,155 & 0.142$\pm$0.010 & - \\ 
    \midrule
    \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Higher-order\\GNNs\end{tabular}} & RingGNN~\citep{chen2019equivalence} & 178.03 & 527,283 &  0.353$\pm$0.019 & - \\
    & 3WLGNN~\citep{maron2019provably} & 179.35 & 507,603 &  0.303$\pm$0.068 & - \\
    \midrule
    \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Substructure-\\based GNNs\end{tabular}}
    & GSN~\citep{bouritsas2022improving} & - & $\sim$500k & 0.101$\pm$0.010 & - \\
    & CIN-Small~\citep{bodnar2021cellular} & - & $\sim$100k & 0.094$\pm$0.004 & 0.044$\pm$0.003 \\ \midrule
    
    \multirow{5}{*}{\begin{tabular}[c]{@{}c@{}}Subgraph\\GNNs\end{tabular}}
    & NGNN~\citep{zhang2021nested} & - & $\sim$500k & 0.111$\pm$0.003 & 0.029$\pm$0.001 \\
    % & DS-GNN (EGO)~\citep{bevilacqua2022equivariant} & 0.115$\pm$0.004 & - \\
    % & DS-GNN (EGO+)~\citep{bevilacqua2022equivariant} & 0.105$\pm$0.003 & - \\
    % & DSS-GNN (EGO)~\citep{bevilacqua2022equivariant} & 0.099$\pm$0.003 & - \\
    & DSS-GNN~\citep{bevilacqua2022equivariant} & - & 445,709 & 0.097$\pm$0.006 & - \\
    & GNN-AK~\citep{zhao2022stars} & - & $\sim$500k & 0.105$\pm$0.010 & - \\
    % & GNN-AK-CTX~\citep{zhao2022stars} & 0.093$\pm$0.002 & - \\
    & GNN-AK+~\citep{zhao2022stars} & - & $\sim$500k & 0.091$\pm$0.011 & - \\
    & SUN~\citep{frasca2022Understanding} & 15.04 & 526,489 & 0.083$\pm$0.003 & - \\ \midrule
    
    \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Graph\\Transformers\end{tabular}}
    & GT~\citep{dwivedi2021generalization} & - & 588,929 & 0.226$\pm$0.014 & - \\
    & SAN~\citep{kreuzer2021rethinking} & - & 508,577 & 0.139$\pm$0.006 & - \\
    & Graphormer~\citep{ying2021transformers} & 12.26 & 489,321 & 0.122$\pm$0.006 & 0.052$\pm$0.005 \\ 
    & URPE~\citep{luo2022your} & 12.40 & 491,737 & 0.086$\pm$0.007 & 0.028$\pm$0.002 \\
    \midrule
    

    GD-WL & Graphormer-GD (ours) & 12.52 & 502,793 & ~~0.081$\pm$0.009$^{*}$ & ~~0.025$\pm$0.004$^{*}$ \\\bottomrule
    \end{tabular}
    }
    \vspace{-8pt}
\end{table}


% \vspace{-2pt}
% \subsection{Real-world tasks}
% \label{exp:real}
% \vspace{-2pt}
% \begin{table}[t]
% \vspace{-15pt}
% \caption{Mean Absolute Error (MAE) on ZINC test set. Following~\citet{dwivedi2020benchmarking}, the parameter budget of compared models is set to 500k. We use $^{*}$ to indicate the best performance.}
%   \vspace{2px}
%   \label{tab:zinc}
%   \small
%   \centering
%   \resizebox{0.9\textwidth}{!}{ \renewcommand{\arraystretch}{1.08}
%     \begin{tabular}{c|lccc}
%     \toprule
%     \multirow{2}{*}{Method} & \multirow{2}{*}{Model} & \multirow{2}{*}{\#Param} & \multicolumn{2}{c}{Test MAE}\\
%     & & & ZINC-Subset & ZINC-Full \\ \midrule
    
%     \multirow{9}{*}{MPNNs}
%     & GIN~\citep{xu2019powerful} & 509,549 & 0.526$\pm$0.051 &  0.088$\pm$0.002 \\
%     & GraphSAGE~\citep{hamilton2017inductive} & 505,341 & 0.398$\pm$0.002 & 0.126$\pm$0.003 \\
%     & GAT~\citep{velivckovic2018graph} & 531,345 & 0.384$\pm$0.007 & 0.111$\pm$0.002 \\
%     & GCN~\citep{kipf2017semisupervised} & 505,079 & 0.367$\pm$0.011 & 0.113$\pm$0.002 \\
%     & MoNet~\citep{monti2017geometric} & 504,013 & 0.292$\pm$0.006 & 0.090$\pm$0.002 \\
%     % & GatedGCN-PE~\citep{bresson2017residual} & 0.214$\pm$0.006 & - \\
%     & MPNN(sum)~\citep{gilmer2017neural} & 480,805 & 0.145$\pm$0.007 & - \\
%     % & HIMP~\citep{fey2020hierarchical} & 614,516 & 0.151$\pm$0.006 & 0.036$\pm$0.002 \\
%     & PNA~\citep{corso2020principal} & 387,155 & 0.142$\pm$0.010 & - \\ \midrule
    
%     \multirow{2}{*}{}
%     Substructure Count & GSN~\citep{bouritsas2022improving} & - & 0.101$\pm$0.010 & - \\ \midrule
%     Cellular Complex & CIN-Small~\citep{bodnar2021cellular} & - & 0.094$\pm$0.004 & - \\ \midrule
    
%     \multirow{3}{*}{Subgraph GNNs}
%     & NGNN~\citep{zhang2021nested} & - & 0.111$\pm$0.003 & - \\
%     % & DS-GNN (EGO)~\citep{bevilacqua2022equivariant} & - & 0.115$\pm$0.004 & - \\
%     % & DS-GNN (EGO+)~\citep{bevilacqua2022equivariant} & - & 0.105$\pm$0.003 & - \\
%     % & DSS-GNN (EGO)~\citep{bevilacqua2022equivariant} & - & 0.099$\pm$0.003 & - \\
%     & DSS-GNN (EGO+)~\citep{bevilacqua2022equivariant} & - & 0.097$\pm$0.006 & - \\
%     & GNN-AK~\citep{zhao2022stars} & - & 0.105$\pm$0.010 & - \\
%     % & GNN-AK-CTX~\citep{zhao2022stars} & - & 0.093$\pm$0.002 & - \\
%     & GNN-AK+~\citep{zhao2022stars} & - & 0.091$\pm$0.011 & - \\ \midrule
    
%     \multirow{3}{*}{Graph Transformers}
%     & GT~\citep{dwivedi2021generalization} & 588,929 & 0.226$\pm$0.014 & - \\
%     & SAN~\citep{kreuzer2021rethinking} & 508,577 & 0.139$\pm$0.006 & - \\
%     & Graphormer~\citep{ying2021transformers} & 489,321 & 0.122$\pm$0.006 & 0.052$\pm$0.005 \\ 
%     % & URPE~\citep{luo2022your} & - & 0.086$\pm$0.007 & 0.028$\pm$0.002 \\
%     \midrule
    

%     GD-WL & Graphormer-GD (ours) & 502,793 &   ~~0.081$\pm$0.009$^{*}$ & ~~0.025$\pm$0.004$^{*}$ \\\bottomrule
%     \end{tabular}
%     }
%     \vspace{-10pt}
% \end{table}




% \vspace{-2pt}
\section{conclusion}
% \vspace{-2pt}
In this paper, we systematically investigate the expressive power of GNNs via the perspective of graph biconnectivity.
Through the novel lens, we gain strong theoretical insights into the power and limits of existing popular GNNs. We then introduce the principled GD-WL framework that is fully expressive for all biconnectivity metrics.
% Through the novel lens, we gain strong theoretical insights into a variety of popular GNNs such as Subgraph GNNs and Graph Transformers.
% Motivated by these theoretical results, w
We further design the Graphormer-GD architecture that is provably powerful while enjoying practical efficiency and parallelizability. Experiments on both synthetic and real-world datasets demonstrate the effectiveness of Graphormer-GD.

There are still many promising directions that have not yet been explored. \emph{Firstly}, it remains an important open problem whether biconnectivity can be solved more efficiently in $o(n^2)$ time using \emph{equivariant} GNNs. \emph{Secondly}, a deep understanding of GD-WL is generally lacking. For example, we conjecture that RD-WL can encode graph spectral \citep{lim2022sign} and is strictly more powerful than SPD-WL in distinguishing general graphs. \emph{Thirdly}, it may be interesting to further investigate more expressive distance (structural) encoding schemes beyond RD-WL and explore how to encode them in Graph Transformers. \emph{Finally}, one can extend biconnectivity to a hierarchy of higher-order variants (e.g., tri-connectivity), which provides a completely different view parallel to the WL hierarchy to study the expressive power and guide designing provably powerful GNNs architectures.


% \subsubsection*{Author Contributions}
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

\subsubsection*{Acknowledgments}
Bohang Zhang is grateful to Ruichen Li for his great help in discussing and checking several of the main results in this paper, including \cref{thm:scwl,thm:dsswl,thm:spdwl,thm:distance_regular_maintext}. In particular, after the initial submission, Ruichen Li discovered a simpler proof of \cref{thm:spdwl_case1_2} and helped complete the proof of \cref{thm:rd_distance_regular}. Bohang Zhang would also thank Yiheng Du, Kai Yang amd Ruichen Li for correcting some small mistakes in the proof of \cref{thm:proof_dsswl_cut_vertex_1,thm:proof_rdwl_part1_0}.

% \begin{spacing}{0.985}
\bibliography{iclr2023_conference}
% \end{spacing}
\bibliographystyle{iclr2023_conference}

\newpage
\appendix
% \tableofcontents
% \addtocontents{toc}{\protect\setcounter{tocdepth}{4}}
% \addtocontents{toc}{\protect\setcounter{secnumdepth}{4}}
\renewcommand \thepart{} % make "Part" text invisible
\renewcommand \partname{}
% \part{\Large\centering{Appendix}}
% \addcontentsline{toc}{section}{Appendix} % Add the appendix text to the document TOC
\part{Appendix} % Start the appendix part
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}
\parttoc % Insert the appendix TOC
% \setcounter{secnumdepth}{4}
% \setcounter{tocdepth}{0}
\newpage

\section{Recent advances in expressive GNNs}
\label{sec:related_work_expressive_gnn}
\vspace{-2pt}
Since the seminal works of \citet{xu2019powerful,morris2019weisfeiler}, extensive studies have devoted to developing new GNN architectures with better expressiveness beyond the 1-WL test. These works can be broadly classified into the following categories.

\textbf{Higher-order GNNs}. One straightforward way to design provably more expressive GNNs is inspired by the higher-order WL tests (see \cref{sec:kfwl}). Instead of performing node feature aggregation, these higher-order GNNs calculate a feature vector for each $k$-tuple of nodes ($k\ge 2$) and perform aggregation between features of different tuples using tensor operations \citep{morris2019weisfeiler,maron2019invariant,maron2019universality,maron2019provably,keriven2019universal,azizian2021expressive,geerts2022expressiveness}. In particular, \citet{maron2019provably} leveraged equivariant matrix multiplication to design network layers that mimic the 2-FWL aggregation. Due to the huge computational cost of higher-order GNNs, several recent works considered improving efficiency by leveraging the sparse and local nature of graphs and designing a ``local'' version of the $k$-WL aggregation, which comes at the cost of some expressiveness \citep{morris2020weisfeiler,morris2022speqnets}. The work of \citet{vignac2020building} can also be seen as a local 2-order GNN and its expressive power is bounded by 3-IGN \citep{maron2019universality}.

\textbf{Substructure-based GNNs}. Another way to design more expressive GNNs is inspired by studying the failure cases of 1-WL test. In particular, \citet{chen2020can} pointed out that standard MPNNs cannot detect/count common substructures such as cycles, cliques, and paths. Based on this finding, \citet{bouritsas2022improving} designed the Graph Substructure Network (GSN) by incorporating substructure counting into node features using a preprocessing step. Such an approach was later extended by \citet{barcelo2021graph} based on homomorphism counting. \citet{bodnar2021topological,bodnar2021cellular,thiede2021autobahn,horn2022topological} further developed novel WL aggregation schemes that take into account these substructures (e.g., cycles or cliques). \citet{toenshoff2021graph} considered using random walk techniques to generate small substructures.

\textbf{Subgraph GNNs}. In fact, the graphs indistinguishable by 1-WL tend to possess a high degree of symmetry (e.g., see \cref{fig:counterexamples}). Based on this observation, a variety of recent approaches sought to break the symmetry by feeding \emph{subgraphs} into an MPNN. To maintain equivariance, a set of subgraphs is generated \emph{symmetrically} from the original graph using predefined policies, and the final output is aggregated across all subgraphs. There have been several subgraph generation policies in prior works, such as node deletion \citep{cotta2021reconstruction}, edge deletion \citep{bevilacqua2022equivariant}, node marking \citep{papp2022theoretical}, and ego-networks \citep{zhao2022stars,zhang2021nested,you2021identity}. These works also slightly differ in the aggregation schemes. In particular, \citet{bevilacqua2022equivariant} developed a unified framework, called ESAN, which includes per-layer aggregation across subgraphs and thus enjoys better expressiveness. Very recently, \cite{frasca2022Understanding} further extended the framework based on a more relaxed symmetry analysis and proved an upper bound of its expressiveness to be 3-WL. \cite{qian2022ordered} provided a theoretical analysis of how subgraph GNNs relate to $k$-FWL and also designed an approach to learn policies.

\textbf{Non-equivariant GNNs}. Perhaps one of the simplest way to break the intrinsic symmetry of 1-WL aggregation is to use non-equivariant GNNs. Indeed, \citet{loukas2020graph} proved that if each node in a GNN is equipped with a unique identifier, then standard MPNNs can already be Turing universal. There have been several works that exploit this idea to build powerful GNNs, such as using port numbering \citep{sato2019approximation}, relational pooling \citep{murphy2019relational}, random features \citep{sato2021random,abboud2020surprising}, or dropout techniques \citep{papp2021dropgnn}. However, since the resulting architectures cannot fully preserve equivariance, the sample complexity required for training and generalization may not be guaranteed \citep{garg2020generalization}. Therefore, in this paper we only focus on analyzing and designing equivariant GNNs.

\textbf{Other approaches}. \citet{wijesinghe2022new,de2020natural} designed novel variants of MPNNs based on more powerful neighborhood aggregation schemes that are aware of the local graph structure, rather than simply treating neighboring nodes as a set. \citet{li2020distance,velingker2022affinity} incorporated distance encoding into node/edge features to enhance the expressive power of MPNNs. \citet{balcilar2021breaking,feldman2022weisfeiler} utilized spectral information of graphs to achieve better expressiveness beyond 1-WL. \citet{talak2021neural} proposed the Neural Tree Network that performs message passing between higher-order subgraphs instead of node-level aggregation.

Finally, for a comprehensive survey on expressive GNNs, we refer readers to \citet{sato2020survey} and \citet{morris2021weisfeiler}.

\section{The Weisfeiler-Lehman Algorithms and Recently Proposed Variants}
\label{sec:algorithms}
In this section, we give a precise description on the family of Weisfeiler-Lehman algorithms and several recently proposed variants that are studied in this paper. We first present the classic 1-WL algorithm \citep{weisfeiler1968reduction} and the more advanced $k$-FWL \citep{cai1992optimal,morris2019weisfeiler}. Then we present several recently proposed WL variants, including WL with Substructure Counting (SC-WL) \citep{bouritsas2022improving}, Overlap Subgraph WL (OS-WL) \citep{wijesinghe2022new}, Equivariant Subgraph Aggregation WL (DSS-WL) \citep{bevilacqua2022equivariant} and Generalized Distance WL (GD-WL).

Throughout this section, we assume $\operatorname{hash}:\gX\to\gC$ is an \emph{injective} hash function that can map ``arbitrary objects'' to a color in $\gC$ where $\gC$ is an abstract set called the \emph{color set}. Formally, the domain $\gX$ comprises all the objects we are interested in:
\begin{itemize}[topsep=0pt,leftmargin=30pt]
\setlength{\itemsep}{0pt}
    \item $\mathbb R\subset\gX$ and $\gC\subset \gX$;
    \item For any finite multiset $\gM$ with elements in $\gX$, $\gM\in\gX$;
    \item For any tuple $\vc\in\gX^k$ of finite dimension $k\in\mathbb N_+$, $\vc\in\gX$.
\end{itemize}

\subsection{1-WL Test}
\label{sec:1wl}
Given a graph $G=(\gV,\gE)$, the 1-dimensional Weisfeiler-Lehman algorithm (1-WL), also called the \emph{color refinement} algorithm, iteratively calculates a color mapping $\chi_G$ from each vertex $v\in \gV$ to a color $\chi_G(v)\in \gC$. The pseudo code of 1-WL is presented in \cref{alg:1wl}. Intuitively, at the beginning the color of each vertex is initialized to be the same. Then in each iteration, 1-WL algorithm updates each vertex color by combining its own color with the neighborhood color multiset using a hash function. This procedure is repeated for a sufficiently large number of iterations $T$, e.g. $T=|\gV|$.

\vspace{-5pt}
\begin{algorithm}[h]
\SetKwInOut{IN}{Input}
\SetKwInOut{OUT}{Output}
\caption{The 1-dimensional Weisfeiler-Lehman Algorithm}
\label{alg:1wl}
\IN{Graph $G=(\gV,\gE)$ and the number of iterations $T$}
\OUT{Color mapping $\chi_G: \gV\to \gC$}
\textbf{Initialize:} Pick a fixed (arbitrary) element $c_0\in \gC$, and let $\chi_G^0(v):=c_0$ for all $v\in\gV$\\
\For{$t \gets 1$ {\normalfont\textbf{to}} $T$}{
    \For{{\normalfont\textbf{each}} $v\in \gV$}{
        $\chi_G^t(v):= \operatorname{hash}\left(\chi_G^{t-1}(v),\ldblbrace \chi_G^{t-1}(u):u\in \mathcal N_G(v) \rdblbrace\right)$\label{alg:1wl_update}
    }
}
\textbf{Return:} $\chi_G^T$
\end{algorithm}
\vspace{-5pt}

At each iteration, the color mapping $\chi_G^t$ induces a \emph{partition} of the vertex set $\gV$ with an equivalence relation $\sim_{\chi_G^t}$ defined to be $u \sim_{\chi_G^t} v \iff \chi_G^t(u)=\chi_G^t(v)$ for $u,v\in\gV$. We call each equivalence class a \emph{color class} with an associated color $c\in \gC$, denoted as $(\chi_G^t)^{-1}(c):=\{v\in\gV:\chi_G^t(v)=c\}$. The corresponding partition is then denoted as $\gP_G^t=\{(\chi_G^t)^{-1}(c):c\in\gC_G^t\}$ where $\gC_G^t:=\{\chi_G^t(v):v\in\gV\}$ is the color set containing all the presented colors of vertices in $G$.

An important observation is that each 1-WL iteration \emph{refines} the partition $\gP_G^t$ to a finer partition $\gP_G^{t+1}$, because for any $u,v\in \gV$, $u \sim_{\chi_G^{t+1}} v$ implies $u \sim_{\chi_G^{t}} v$. Since the number of vertices $|\gV|$ is finite, there must exist an iteration $T_{\text{stable}}<|\gV|$ such that $\gP_G^{T_{\text{stable}}}=\gP_G^{T_{\text{stable}}+1}$. It follows that $\gP_G^{t}=\gP_G^{T_{\text{stable}}}$ for all $t\ge T_{\text{stable}}$, i.e. the partition stabilizes. We thus denote $\gP_G:=\gP_G^{T_{\text{stable}}}$ as the stable partition induced by the 1-WL algorithm, and denote $\chi_G$ as any stable color mapping (i.e. by picking any $\chi_G^t$ with $t\ge T_{\text{stable}}$). We can similarly define the inverse mapping $\chi_G^{-1}$. The mapping $\chi_G$ serves as a node feature extractor so that $\chi_G(v)$ is the representation of node $v\in \gV$. Correspondingly, the multiset $\ldblbrace \chi_G(v):v\in\gV\rdblbrace$ can serve as the representation of graph $G$.

The 1-WL algorithm can be used to distinguish whether two graphs $G$ and $H$ are isomorphic, by comparing their graph representations $\ldblbrace \chi_G(v):v\in\gV\rdblbrace$ and $\ldblbrace \chi_H(v):v\in\gV\rdblbrace$. If the two multisets are not equivalent, then $G$ and $H$ are clearly non-isomorphic. Thus 1-WL is a necessary condition to test graph isomorphism. Nevertheless, the 1-WL test fails when $\ldblbrace \chi_G(v):v\in\gV\rdblbrace=\ldblbrace \chi_H(v):v\in\gV\rdblbrace$ but $G$ and $H$ are still non-isomorphic (see \cref{fig:counterexamples} for a counterexample). This motivates the more powerful higher-order WL tests, which are illustrated in the next subsection.

\subsection{\texorpdfstring{$k$}{k}-FWL Test}
\label{sec:kfwl}
In this section, we present a family of algorithms called the $k$-dimensional Folklore  Weisfeiler-Lehman algorithms ($k$-FWL). Instead of calculating a node color mapping, $k$-FWL computes a color mapping on each $k$-tuple of nodes. The pseudo code of $k$-FWL ($k\ge 2$) is presented in \cref{alg:kfwl}.

\begin{algorithm}[h]
\SetKwInOut{IN}{Input}
\SetKwInOut{OUT}{Output}
\caption{The $k$-dimensional Folklore Weisfeiler-Lehman Algorithm}
\label{alg:kfwl}
\IN{Graph $G=(\gV,\gE)$ and the number of iterations $T$}
\OUT{Color mapping $\chi_G: \gV^k\to \gC$}
\textbf{Initialize:} Pick three fixed different elements $c_0,c_1, c_\mathrm{node}\in \gC$, let $\chi_G^0(\vv):= \operatorname{hash}(\operatorname{vec}(\mathbf A^\vv))$ for each $\vv\in \gV^k$ where $\mathbf A^\vv\in \gC^{k\times k}$ is a matrix with elements
\begin{equation}
\label{eq:kfwl_init}
    A^\vv_{ij}=\left\{\begin{array}{ll}
        c_\mathrm{node} & \text{if } v_i=v_j\\
        c_0 & \text{if } v_i\neq v_j \text{ and } \{v_i,v_j\}\notin \gE\\
        c_1 & \text{if } v_i\neq v_j \text{ and } \{v_i,v_j\}\in \gE
    \end{array}\right.
\end{equation}\\
\For{$t \gets 1$ {\normalfont\textbf{to}} $T$}{
    \For{{\normalfont\textbf{each}} $\vv\in \gV^k$}{
        $
            \chi_G^t(\vv):=\operatorname{hash}\left(\chi_G^{t-1}(\vv), \ldblbrace (\chi_G^{t-1}(N_1(\vv,u)), \cdots, \chi_G^{t-1}(N_k(\vv,u))):u\in \gV \rdblbrace\right)
        $\label{alg:kfwl_update}\\
        where $N_i(\vv,u)=(v_1,\cdots,v_{i-1}, u,v_{i+1}, \cdots, v_k)$
        
    }
}
\textbf{Return:} $\chi_G^T$
\end{algorithm}

Intuitively, at the beginning, the color of each vertex tuple $\vv$ encodes the full structure (i.e. isomophism type) of the subgraph induced by the \emph{ordered} vertex set $\{v_i:i\in[k]\}$, by hashing the ``adjacency'' matrix $\mathbf A^\vv$ defined in (\ref{eq:kfwl_init}). Then in each iteration, $k$-FWL algorithm updates the color of each vertex tuple by combining its own color with the ``neighborhood'' color using a hash function. Here, the neighborhood of a tuple $\vv$ is all the tuples that differ $\vv$ by exactly one element. These $k\times |\gV|$ neighborhood colors are grouped into a multiset of size $|\gV|$ where each element is a $k$-tuple. Finally, the update procedure is repeated for a sufficiently large number of iterations $T$, e.g. $T=|\gV|^k$.

Simiar to 1-WL, the $k$-FWL color mapping $\chi_G^t$ induces a partition of the set of \emph{vertex $k$-tuples} $\gV^k$, and each $k$-FWL iteration \emph{refines} the partition of the previous iteration. Since the number of vertex $k$-tuples $|\gV|^k$ is finite, there must exist an iteration $T_{\text{stable}}<|\gV|^k$ such that the partition no longer changes after $t\ge T_{\text{stable}}$. We denote the stable color mapping as $\chi_G$ by picking any $\chi_G^t$ with $t\ge T_{\text{stable}}$.

The $k$-FWL algorithm can be used to distinguish whether two graphs $G$ and $H$ are isomorphic, by comparing their graph representations $\ldblbrace \chi_G(\vv):\vv\in\gV^k\rdblbrace$ and $\ldblbrace \chi_H(\vv):\vv\in\gV^k\rdblbrace$. It has been proved that $k$-FWL is strictly more powerful than 1-WL in distinguishing non-isomorphic graphs, and $(k+1)$-FWL is strictly more powerful than $k$-FWL for all $k\ge 2$ \citep{cai1992optimal}.

Moreover, the $k$-FWL algorithm can also be used to extract \emph{node} representations as with 1-WL. To do this, we can simply define $\chi_G(v):=\chi_G(v,\cdots,v)$ as the vertex color of the $k$-FWL algorithm (without abuse of notation), which induces a partition $\gP_G$ over vertex set $\gV$. It has been shown that this partition is \emph{finer} than the partition induces by 1-WL, and also the vertex partition induced by $(k+1)$-FWL is finer than that of $k$-FWL \citep{kiefer2020power}.

\subsection{WL with Substructure Counting (SC-WL)}
\label{sec:scwl}
Recently, \citet{bouritsas2022improving} proposed a variant of the 1-WL algorithm by incorporating the so-called \emph{substructure counting} into WL aggregation procedure. This yields a algorithm that is provably powerful than the original 1-WL test.

% \looseness=-1
To describe the algorithm, we first need the notation of \emph{automorphism group}. Given a graph $H=(\gV_H,\gE_H)$, an automorphism of $H$ is a bijective mapping $f:\gV_H\to\gV_H$ such that for any two vertices $u,v\in\gV_H$, $\{u,v\}\in\gE_H\iff \{f(u),f(v)\}\in\gE_H$. It follows that all automorphisms of $H$ form a group under function composition, which is called the \emph{automorphism group} and denoted as $\operatorname{Aut}(H)$.

The automorphism group $\operatorname{Aut}(H)$ yields a partition of the vertex set $\gV$, called \emph{orbits}. Formally, given a vertex $v\in\gV_H$, define its orbit $\operatorname{Orb}_H(v)=\{u\in\gV_H:\exists f\in\operatorname{Aut}(H), f(u)=v\}$. The set of all orbits $H \backslash \operatorname{Aut}(H) := \{\operatorname{Orb}_H(v):v\in\gV_H\}$ is called the \emph{quotient} of the automorphism. Denote $d_H=|H \backslash \operatorname{Aut}(H)|$ and denote the elements in $H \backslash \operatorname{Aut}(H)$ as $\{\gO_{H,i}^\mathrm{V}\}_{i=1}^{d_H}$. We are now ready to describe the procedure of SC-WL.

\textbf{Pre-processing}. Depending on the tasks, one first specify a set of (small) connected graphs $\gH=\{H_1,\cdots,H_k\}$, which will be used for sub-structure counting in the input graph $G$. Popular choices of these small graphs are cycles of different lengths (e.g., triangle or square) and cliques. Given a graph $G=(\gV_G,\gE_G)$, for each vertex $v\in\gV_G$ and each graph $H\in\gH$, the following quantities are calculated:
\begin{equation}
\label{eq:scwl_1}
    x^\mathrm{V}_{H,i}(v):=\left\{G[\gS]:\gS\subset \gV,G[\gS]\simeq H,v\in\gS,f_{G[\gS]\to\gV_H}(v)\in\gO^\mathrm{V}_{H,i}\right\},\quad i\in[d_H]
\end{equation}
where $f_{G[\gS]\to \gV_H}$ is any isomorphism that maps the vertices of graph $G[\gS]$ to those of graph $H$. Intuitively, $x^\mathrm{V}_{H,i}(v)$ counts the number of induced subgraphs of $G$ that is isomorphic to $H$ and contains node $v$, such that the orbit of $v$ is similar to the orbit $\gO^\mathrm{V}_{H,i}$. The counts corresponding to different orbits $\gO^\mathrm{V}_{H,i}$ and different graphs $H$ are finally combined and concatenated into a vector:
\begin{equation}
\label{eq:scwl_2}
    \vx^\mathrm{V}(v)=[\vx^\mathrm{V}_{H_1}(v)^\top,\cdots,\vx^\mathrm{V}_{H_k}(v)^\top]^\top\in\mathbb N_+^{D}
\end{equation}
where the dimension of $\vx^\mathrm{V}(v)$ is $D=\sum_{i\in [k]}d_i$.

\textbf{Message Passing}. The message passing procedure is similar to \cref{alg:1wl}, except that the aggregation formula (\cref{alg:1wl_update}) is replaced by the following update rule:
\begin{equation}
\label{eq:scwl_3}
    \chi_G^t(v):= \operatorname{hash}\left(\chi_G^{t-1}(v),\vx^\mathrm{V}(v),\ldblbrace (\chi_G^{t-1}(u),\vx^\mathrm{V}(u)):u\in \mathcal N_G(v) \rdblbrace\right)
\end{equation}
which incorporates the substructure counts (\ref{eq:scwl_1}, \ref{eq:scwl_2}). Note that the update rule (\ref{eq:scwl_3}) is slightly simpler than the original paper \citep[Section 3.2]{bouritsas2022improving}, but the expressive power of the two formulations are the same.

Finally, we note that the above procedure counts substructures and calculates features $\vx^\mathrm{V}$ for \emph{each vertex} of $G$. One can similarly consider calculating substructure counts for \emph{each edge} of $G$, and the conclusion in this paper (\cref{thm:scwl}) still holds. Please refer to \citet{bouritsas2022improving} for more details on how to calculate edge features.

% \subsection{Overlap Subgraph WL (OS-WL)}
% \label{sec:oswl}
% Recently, \citet{wijesinghe2022new} proposed a generalized message-passing framework, called the Overlap Subgraph WL (OS-WL), which includes the classic 1-WL algorithm as a special case. The key motivation is that in 1-WL, the color of each node is updated by using only the neighborhood color multiset, which does not fully exploit the structural information of the neighborhood subgraph, e.g., the connections between two neighboring nodes. The authors thus developed a new aggregation scheme that takes into account the structure of the \emph{overlap subgraph} between each node and its neighbor (see below for details).

% Given a graph $G=(\gV,\gE)$, define the \emph{neighborhood subgraph} of a vertex $v\in\gV$ as the subgraph induced by the vertex set $\tilde\gN_G(v):=\gN_G(v)\cup\{v\}$, which contains
% all edges in $\gE$ that have both endpoints in $\tilde\gN_G(v)$. We adopt the notation $S_v:=G[\tilde\gN_G(v)]$. For two adjacent vertices $u$ and $v$, the \emph{overlap subgraph} $S_{uv}$ is defined as $S_{uv} := G[\tilde\gN_G(u)\cap \tilde\gN_G(v)]$. We say two neighborhood subgraphs $S_u$ and $S_v$ are isomorphic, if there is a bijective mapping $g:\tilde\gN_G(u)\to \tilde\gN_G(v)$ such that $g(u)=v$ and for any two nodes $w_1,w_2\in\tilde\gN_G(u)$, $\{w_1,w_2\}\in\gE$ if and only if $\{g(w_1),g(w_2)\}\in\gE$. In other words, the center node $u$ (or $v$) is considered different from its neighbors when talking about neighborhood subgraph isomorphism. Similarly, when we say two overlap graphs $S_{uv}$ and $S_{wx}$ are isomorphic, it further satisfies that the isomorphism maps $u$ to $w$.

% Let $\omega$ be a function that takes an overlap subgraph as input and outputs a \emph{structural coefficient}. It must satisfy that $\omega(S_{uv})=\omega(S_{wx})$ if graphs $S_{uv}$ and $S_{wx}$ are isomorphic. The OS-WL update rule is similar to 1-WL, except that the aggregation formula (\cref{alg:1wl_update} in \cref{alg:1wl}) is replaced by the following update rule:
% \begin{equation}
% \label{eq:oswl_1}
%     \chi_G^t(v):= \operatorname{hash}\left(\chi_G^{t-1}(v),\ldblbrace (\omega(S_{uv}),\chi_G^{t-1}(u)):u\in \mathcal N_G(v) \rdblbrace\right)
% \end{equation}
% Note that the update rule (\ref{eq:oswl_1}) is slightly simpler
% than the original paper \citep[Section 3]{wijesinghe2022new}, but the expressive power of the two formulations are the same. It is easy to see that when $\omega$ is chosen as a constant function, the resulting update recovers the classic 1-WL algorithm. For general (carefully designed) functions $\omega$, OS-WL has been proved to be strictly powerful than 1-WL.


\subsection{Equivariant Subgraph Aggregation WL (DSS-WL)}
\label{sec:dsswl}
Recently, \citet{bevilacqua2022equivariant} developd a new type of graph neural networks, called Equivariant Subgraph Aggregation Networks, as well as a new WL variant named DSS-WL. Given a graph $G=(\gV,\gE)$, DSS-WL first generates a bag of graphs $\gB^\pi_G=\ldblbrace G_1,\cdots,G_m\rdblbrace$ which share the vertices, i.e. $G_i=(\gV,\gE_i)$, but differ in the edge sets $\gE_i$. Here $\pi$ denotes the graph generation policy which determines the edge set $\gE_i$ for each graph $G_i$. The initial coloring $\chi_{G_i}^0(v)$ for each node $v\in\gV$ in graph $G_i$ is also determined by $\pi$ and can be different across different nodes and graphs. In each iteration, the algorithm refines the color of each node by jointly aggregating its neighboring colors in the own graph and across different graphs. This procedure is repeated for a sufficiently large iterations $T$ to obtain the stable color mappings $\chi_{G_i}$ and $\chi_G$. The pseudo code of DSS-WL is presented in \cref{alg:dsswl}. 

\begin{algorithm}[h]
\SetKwInOut{IN}{Input}
\SetKwInOut{OUT}{Output}
\caption{DSS Weisfeiler-Lehman Algorithm}
\label{alg:dsswl}
\IN{Graph $G=(\gV,\gE)$, the number of iterations $T$, and graph selection policy $\pi$}
\OUT{Color mapping $\chi_G: \gV\to \gC$}
\textbf{Initialize:} Generate a bag of graphs $\gB^\pi_G=\ldblbrace G_i\rdblbrace_{i=1}^m$, $G_i=(\gV,\gE_i)$ and initial coloring $\chi_{G_i}^0$ for $i\in[m]$ according to policy $\pi$ \\
Let $\chi_G^0(v):= \operatorname{hash}\left(\ldblbrace \chi_{G_i}^{t}(v):i\in [m]\rdblbrace\right)$ for each $v\in \gV$\\
\For{$t \gets 1$ {\normalfont\textbf{to}} $T$}{
    \For{{\normalfont\textbf{each}} $v\in \gV$}{
        \For{$i \gets 1$ {\normalfont\textbf{to}} $m$}{
            $\chi_{G_i}^t(v):= \operatorname{hash}\left(\chi_{G_i}^{t-1}(v),\ldblbrace \chi_{G_i}^{t-1}(u):u\in \mathcal N_{G_i}(v) \rdblbrace,\chi_{G}^{t-1}(v),\ldblbrace \chi_{G}^{t-1}(u):u\in \mathcal N_G(v) \rdblbrace\right)$\label{alg:dsswl_update}
        }
        $\chi_G^t(v):= \operatorname{hash}\left(\ldblbrace \chi_{G_i}^{t}(v):i\in [m]\rdblbrace\right)$ \label{alg:dsswl_all_graph_aggregation}
    }
}
\textbf{Return:} $\chi_G^T$
\end{algorithm}

The key component in the DSS-WL algorithm is the graph generation policy $\pi$ which must maintain \emph{symmetry}, i.e., be equivairant under permutation of the vertex set. We list several common choices below:
\begin{itemize}[topsep=0pt,leftmargin=30pt]
\setlength{\itemsep}{0pt}
    \item \textbf{Node marking policy} $\pi=\pi_{\mathrm{NM}}$. In this policy, we have $\gB^\pi_G=\ldblbrace G_v:v\in\gV\rdblbrace$ where $G_v=G$, i.e., there are $|\gV|$ graphs in $\gB^\pi_G$ whose structures are the completely the same. The difference, however, lies in the initial coloring which marks the special node $v$ in the following way: $\chi_{G_v}^0(v)=c_1$ and $\chi_{G_v}^0(u)=c_0$ for other nodes $u\neq v$, where $c_0,c_1\in\gC$ are two different colors.
    \item \textbf{Node deletion policy} $\pi=\pi_{\mathrm{ND}}$. The bag of graphs for this policy is also defined as $\gB^\pi_G=\ldblbrace G_v:v\in\gV\rdblbrace$, but each graph $G_v=(\gV,\gE_v)$ has a different edge set $\gE_v:=\gE\backslash\{\{v,w\}:w\in\gN_G(v)\}$. Intuitively, it removes all edges that connects to node $v$ and thus makes $v$ an isolated node. The initial coloring is chosen as a constant $\chi_{G_i}^0(v)=c_0$ for all $v\in\gV$ and $G_i\in \gB^\pi_G$ for some fixed color $c_0\in\gC$.
    
    \item \textbf{Ego network policy} $\pi=\pi_{\mathrm{EGO}(k)}$. In this policy, we also have $\gB^\pi_G=\ldblbrace G_v:v\in\gV\rdblbrace$, $G_v=(\gV,\gE_v)$. The edge set $\gE_v$ is defined as $\gE_v:=\{\{u,w\}\in\gE:\dis_G(u,v)\le k,\dis_G(w,v)\le k\}$, which corresponds to a subgraph containing all the $k$-hop neighbors of $v$ and isolating other nodes. The initial coloring is chosen as $\chi_{G_i}^0(v)=c_0$ for all $v\in\gV$ and $G_i\in \gB^\pi_G$ where $c_0\in\gC$ is a constant. One can also consider the \textbf{ego network policy with marking} $\pi=\pi_{\mathrm{EGOM}(k)}$, by marking the initial color of the special node $v$ for each $G_v$.
\end{itemize}

We note that for all the above policies, $|\gB^\pi_G|=|\gV|$. There are other choices such as the edge deletion policy \citep{bevilacqua2022equivariant}, but we do not discuss them in this paper. A straightforward analysis yields that DSS-WL with any above policy is strictly powerful than the classic 1-WL algorithm. Also, node marking policy has been shown to be not less powerful than the node deletion policy \citep{papp2022theoretical}.

Finally, we highlight that \citet{bevilacqua2022equivariant,cotta2021reconstruction} also proposed a weaker version of DSS-WL, called the DS-WL algorithm. The difference is that for DS-WL, \cref{alg:dsswl_update,alg:dsswl_all_graph_aggregation} in \cref{alg:dsswl} are replaced by a simple 1-WL aggregation: 
\begin{equation}
\label{eq:dswl_update}
    \chi_{G_i}^t(v):= \operatorname{hash}\left(\chi_{G_i}^{t-1}(v),\ldblbrace \chi_{G_i}^{t-1}(u):u\in \mathcal N_G(v) \rdblbrace\right).
\end{equation}
However, the original formulation of DS-WL \citep{bevilacqua2022equivariant} only outputs a graph representation $\ldblbrace\ldblbrace\chi_{G_i}(v):v\in\gV\rdblbrace:G_i\in\gB^\pi_G\rdblbrace$ rather than outputs each node color, which does not suit the node-level tasks (e.g., finding cut vertices). Nevertheless, there are simple adaptations that makes DS-WL output a color mapping $\chi_G$. We will study these adaptations in \cref{sec:counterexample_proof} (see the paragraph above \cref{thm:dswl_adaptation}) and discuss their limitations compared with DSS-WL.

\subsection{Generalized Distance WL (GD-WL)}
In this paper, we study a new variant of the color refinement algorithm, called the Generalized Distance WL (GD-WL). The complete algorithm is described below. As a special case, when choosing $d_G=\dis_G$, the resulting algorithm is called the Shortest Path Distance WL (SPD-WL), which is strictly powerful than the classic 1-WL.

\begin{algorithm}[h]
\SetKwInOut{IN}{Input}
\SetKwInOut{OUT}{Output}
\caption{The Genealized Distance Weisfeiler-Lehman Algorithm}
\label{alg:gdwl}
\IN{Graph $G=(\gV,\gE)$, distance metric $d_G:\gV\times \gV\to \mathbb R_+$, and the number of iterations $T$}
\OUT{Color mapping $\chi_G: \gV\to \gC$}
\textbf{Initialize:} Pick a fixed (arbitrary) element $c_0\in \gC$, and let $\chi_G^0(v):=c_0$ for all $v\in\gV$\\
\For{$t \gets 1$ {\normalfont\textbf{to}} $T$}{
    \For{{\normalfont\textbf{each}} $v\in \gV$}{
        $\chi_G^t(v):= \operatorname{hash}\left(\ldblbrace (d_G(v,u), \chi_G^{t-1}(u)):u\in \gV\rdblbrace\right)$\label{alg:gdwl_update}
    }
}
\textbf{Return:} $\chi_G^T$
\end{algorithm}


\section{Proof of Theorems}
This section provides all the missing proofs in this paper. For the convenience of reading, we will restate each theorem before giving a proof.

\subsection{Properties of color refinement algorithms}
\label{sec:property_of_wl}
In this subsection, we first derive several important properties that are shared by a general class of color refinement algorithms. They will serve as key lemmas in our subsequent proofs. Here, a general color refinement algorithm takes a graph $G=(\gV_G,\gE_G)$ as input and calculates a color mapping $\chi_G:\gV_G\to\gC$. We first define a concept called the \emph{WL-condition}.

\begin{definition}
\label{def:wl_condition}
\normalfont A color mapping $\chi_G:\gV_G\to\gC$ is said to satisfy the WL-condition if for any two vertices $u,v$ with the same color (i.e. $\chi_G(u)=\chi_G(v)$) and any color $c\in \gC$, $$|\gN_G(u)\cap \chi_G^{-1}(c)|=|\gN_G(v)\cap \chi_G^{-1}(c)|,$$
where $\chi_G^{-1}$ is the inverse mapping of $\chi_G$.
\end{definition}
\begin{remark}
\label{def:joint_wl_condition}
\normalfont The WL-condition can be further generalized to handle two graphs. Let $\chi_G:\gV_G\to\gC$ and $\chi_H:\gV_H\to\gC$ be two color mappings obtained by applying the same color refinement algorithm for graphs $G$ and $H$, respectively. $\chi_G$ and $\chi_H$ are said to \emph{jointly satisfy the WL-condition}, if for any two vertices $u\in\gV_G$ and $v\in\gV_H$ with the same color ($\chi_G(u)=\chi_H(v)$) and any color $c\in \gC$, $$|\gN_G(u)\cap \chi_G^{-1}(c)|=|\gN_H(v)\cap \chi_H^{-1}(c)|.$$
It clearly implies \cref{def:wl_condition} by choosing $G=H$.
\end{remark}

It is easy to see that the classic 1-WL algorithm (\cref{alg:1wl}) satisfies the WL-condition. In fact, many of the presented algorithms in this paper satisfy such a condition as we will show below, such as DSS-WL (\cref{alg:dsswl}), SPD-WL (\cref{alg:gdwl} with $d_G=\dis_G$), and $k$-FWL (\cref{alg:kfwl}).


\begin{proposition}
\label{thm:dsswl_wl_condition}
Consider the DSS-WL algorithm (\cref{alg:gdwl}) with arbitrary graph selection policy $\pi$. Let $\chi_G$ and $\chi_H$ be the color mappings for graphs $G$ and $H$, and let $\ldblbrace \chi_{G_i}:i\in[m_G]\rdblbrace$ and $\ldblbrace \chi_{H_i}:i\in[m_H]\rdblbrace$ be the color mapping for subgraphs generated by $\pi$. Then, 
\begin{itemize}[topsep=0pt,leftmargin=30pt]
\setlength{\itemsep}{0pt}
    \item $\chi_G$ and $\chi_H$ jointly satisfy the WL-condition;
    \item $\chi_{G_i}$ and $\chi_{H_j}$ jointly satisfy the WL-condition for any $i\in[m_G]$ and $j\in[m_H]$.
\end{itemize}
\end{proposition}
\begin{proof}
We first prove the second bullet of \cref{thm:dsswl_wl_condition}. By definition of the DSS-WL aggregation procedure (\cref{alg:dsswl_update} in \cref{alg:dsswl}), $\chi_{G_i}(u)=\chi_{H_i}(v)$ already implies $\ldblbrace \chi_{G_i}(w):w\in \mathcal N_{G_i}(u) \rdblbrace=\ldblbrace \chi_{H_j}(w):w\in \mathcal N_{H_j}(v) \rdblbrace$. Namely, $|\{w:w\in \gN_{G_i}(u)\cap \chi_{G_i}^{-1}(c)\}|=|\{w:w\in \gN_{H_j}(v)\cap \chi_{H_j}^{-1}(c)\}|$ holds for any $c\in\gC$.

We then turn to the first bullet. If $\chi_G(u)=\chi_H(v)$, then $\ldblbrace \chi_{G_i}(u):i\in [m_G]\rdblbrace=\ldblbrace \chi_{H_j}(v):j\in [m_H]\rdblbrace$ (\cref{alg:dsswl_all_graph_aggregation} in \cref{alg:dsswl}). Then there exists a pair of indices $i\in[m_G]$ and $j\in[m_H]$ such that $\chi_{G_i}(u)=\chi_{H_j}(v)$. By definition of the DSS-WL aggregation, it implies $\ldblbrace \chi_G(w):w\in \mathcal N_G(u) \rdblbrace=\ldblbrace \chi_H(w):w\in \mathcal N_H(v) \rdblbrace$ and concludes the proof.
\end{proof}

\begin{proposition}
\label{thm:spdwl_wl_condition}
Let $\chi_G$ and $\chi_H$ be two mappings returned by SPD-WL (\cref{alg:gdwl} with $d_G=\dis_G$) for graphs $G$ and $H$, respectively. Then $\chi_G$ and $\chi_H$ jointly satisfy the WL-condition.
\end{proposition}
\begin{proof}
If $\chi_G(u)=\chi_H(v)$ for some nodes $u,v$, then by the update rule (\cref{alg:gdwl_update} in \cref{alg:gdwl})
$$\ldblbrace (\dis_G(u,w), \chi_G(w)):w\in \gV\rdblbrace=\ldblbrace (\dis_G(v,w), \chi_G(w)):w\in \gV\rdblbrace.$$
Since $w\in\gN_G(u)$ if and only if $\dis_G(u,w)=1$, we have $$\ldblbrace \chi_G(w):w\in\gN_G(u)\rdblbrace=\ldblbrace \chi_G(w):w\in \gN_G(v)\rdblbrace.$$
Therefore, for any $c\in\gC$, $|\{w:w\in \gN_G(u)\cap \chi_G^{-1}(c)\}|=|\{w:w\in \gN_G(v)\cap \chi_G^{-1}(c)\}|$.
\end{proof}

\begin{proposition}
\label{thm:kfwl_wl_condition}
Let $\chi_G$ and $\chi_H$ be two vertex color mappings returned by the $k$-FWL algorithm ($k\ge 2$). Then $\chi_G$ and $\chi_H$ jointly satisfy the WL-condition.
\end{proposition}
\begin{proof}
Let $\chi_G(u)=\chi_H(v)$ for some $u\in\gV_G$ and $v\in\gV_H$. By the update formula (\cref{alg:kfwl_update} in \cref{alg:kfwl}), $\ldblbrace\chi_G(u,\cdots,u,w):w\in\gV_G\rdblbrace=\ldblbrace\chi_H(v,\cdots,v,w):w\in\gV_H\rdblbrace$. Note that for any nodes $w_1\in\gV_G,w_2\in\gV_H$ and any $x_1\in\gN_G(w_1)$, $x_2\notin\gN_H(w_2)$, one has $\chi_G(w_1,\cdots,w_1,x_1)\neq \chi_H(w_2,\cdots,w_2,x_2)$. This is obtained by the definition of the initialization mapping $\chi_G^0$ and the fact that $\chi_G$ refines $\chi_G^0$. Consequently, $\ldblbrace\chi_G(u,\cdots,u,w):w\in\gN_G(u)\rdblbrace=\ldblbrace\chi_G(v,\cdots,v,w):w\in\gN_H(v)\rdblbrace$. Next, we can use the fact that if $\chi_G(u,\cdots,u,w_1)=\chi_G(v,\cdots,v,w_2)$ for some $w_1,w_2\in\gV$, then $\chi_G(w_1)=\chi_G(w_2)$ (see \cref{thm:kfwl_lemma}). Therefore, $\ldblbrace\chi_G(w):w\in\gN_G(u)\rdblbrace=\ldblbrace\chi_G(w):w\in\gN_H(v)\rdblbrace$, which concludes the proof.
\end{proof}

To complete the proof of \cref{thm:kfwl_wl_condition}, it remains to prove the following lemma:
\begin{lemma}
\label{thm:kfwl_lemma}
Let $\chi_G$ and $\chi_H$ be color mappings for graphs $G$ and $H$ in the $k$-FWL algorithm ($k\ge 2$). Denote $$\operatorname{cat}_{i,j}(w,x):=(\underbrace{w,\cdots,w}_{i\text{ times}},\underbrace{x,\cdots,x}_{j\text{ times}}).$$
Then for any $i\in[k-1]$ and any nodes $u,w\in\gV_G$, $v,x\in\gV_H$, if $\chi_G(\operatorname{cat}_{k-i,i}(u,w))=\chi_H(\operatorname{cat}_{k-i,i}(v,x))$, then $\chi_G(\operatorname{cat}_{k-i-1,i+1}(u,w))=\chi_H(\operatorname{cat}_{k-i-1,i+1}(v,x))$. Consequently, $\chi_G(w)=\chi_H(x)$.
\end{lemma}
\begin{proof}
By the update formula (\cref{alg:kfwl_update} in \cref{alg:kfwl}), $\chi_G(\operatorname{cat}_{k-i,i}(u,w))=\chi_H(\operatorname{cat}_{k-i,i}(v,x))$ implies that $\ldblbrace\chi_G(\operatorname{cat}_{k-i-1,1,i}(u,y,w)):y\in\gV_G\rdblbrace=\ldblbrace\chi_H(\operatorname{cat}_{k-i-1,1,i}(v,y,x)):y\in\gV_H\rdblbrace$. Note that for any $j\in[k-1]$ and any $\vz\in\gV_G^k$, $\vz'\in\gV_H^k$ with $z_j=z_{j+1}$ but $z_j'\neq z_{j+1}'$, one has $\chi_G(\vz)\neq \chi_H(\vz')$. This is obtained by the definition of the initialization mapping $\chi_G^0$ and the fact that $\chi_G$ refines $\chi_G^0$. Therefore, we have $\chi_G(\operatorname{cat}_{k-i-1,i+1}(u,w))=\chi_H(\operatorname{cat}_{k-i-1,i+1}(v,x))$, as desired. 
\end{proof}

Equipped with the concept of WL-condition, we now present several key results. In the following, let $\chi_G:\gV_G\to\gC$ and $\chi_H:\gV_H\to\gC$ be two color mappings jointly satisfying the WL-condition.

\begin{lemma}
\label{thm:wl_condition}
Let $(v_0,\cdots, v_d)$ be any path (not necessarily simple) of length $d$ in graph $G$. Then for any node $u_0\in\chi_H^{-1}(\chi_G(v_0))$ in graph $H$, there exists a path $(u_0,\cdots, u_d)$ of the same length $d$ starting at $u_0$, such that $\chi_H(u_i)=\chi_G(v_i)$ holds for all $i\in[d]$.
\end{lemma}
\begin{proof}
The proof is based on induction over the path length $d$. For the base case of $d=1$, if the conclusion does not hold, then there exists two vertices $u\in\gV_G$, $v\in\gV_H$ with the same color (i.e. $\chi_G(u)=\chi_H(v)$) and a color $c=\chi_G(v_1)$ such that $\gN_G(u)\cap \chi_G^{-1}(c)\neq\emptyset$ but $\gN_H(v)\cap \chi_H^{-1}(c)=\emptyset$. This obviously contradicts the WL-condition. For the induction step on the path length $d$, one can just split it by two parts $(v_0,\cdots,v_{d-1})$ and $(v_{d-1},v_d)$. Separately using induction yields two paths $(u_0,\cdots,u_{d-1})$ and $(u_{d-1},u_d)$ such that $\chi_H(u_i)=\chi_G(v_i)$ for all $i\in[d]$. By linking the two paths we have completed the proof.
\end{proof}
Finally, let us define the shortest path distance between node $u$ and vertex set $\gS$ as $\dis_G(u,\gS):=\min_{v\in\gS}\dis_G(u,v)$. The above lemma directly yields the following corollary:
\begin{corollary}
\label{thm:wl_condition_corollary}
For any color $c\in\{\chi_G(w):w\in\gV_G\}$ and any two vertices $u\in\gV_G$, $v\in\gV_H$ with the same color (i.e. $\chi_G(u)=\chi_H(v)$), $\dis_G(u,\chi_G^{-1}(c))=\dis_H(v,\chi_H^{-1}(c))$.
\end{corollary}

\subsection{Counterexamples}
\label{sec:counterexample_proof}
We provides the following two families of counterexamples, which most prior works cannot distinguish.
\begin{example}
\label{example:1}
\normalfont Let $G_1=(\gV,\gE_1)$ and $G_2=(\gV,\gE_2)$ be a pair of graphs with $n=2km+1$ nodes where $m,k$ are two positive integers satisfying $mk\ge 3$. Denote $\gV=[n]$ and define the edge sets as follows:
\begin{align*}
    \gE_1=&\left\{\{i,(i\operatorname{mod} 2km)+1\}:i\in[2km]\right\}\cup \left\{\{n,i\}:i\in[2km],i\operatorname{mod} m=0\right\},\\
    \gE_2=&\left\{\{i,(i\operatorname{mod} km)+1\}:i\in[km]\right\}\cup \left\{\{i+km,(i\operatorname{mod} km)+km+1\}:i\in[km]\right\}\cup\\
    &\left\{\{n,i\}:i\in[2km],i\operatorname{mod} m=0\right\}.
\end{align*}
See \cref{fig:counterexamples}(a-c) for an illustration of three cases: $(\mathrm{i})$ $m=2,k=2$; $(\mathrm{ii})$ $m=4,k=1$; $(\mathrm{iii})$ $m=1,k=4$. It is easy to see that regardless of the chosen of $m$ and $k$, $G_1$ always has no cut vertex but $G_2$ do always have a cut vertex with node number $n$. The case of $k=1$ is more special, for which $G_2$ actually has three cut vertices with node number $m$, $2m$, and $n$, respectively, and it even has two cut edges $\{m,n\}$ and $\{2m,n\}$ (\cref{fig:counterexamples}(b)).
\end{example}

\begin{example}
\label{example:2}
\normalfont Let $G_1=(\gV,\gE_1)$ and $G_2=(\gV,\gE_2)$ be a pair of graphs with $n=2m$ nodes where $m\ge 3$ is an arbitrary integer. Denote $\gV=[n]$ and define the edge sets as follows:
\begin{align*}
    \gE_1=&\left\{\{i,(i\operatorname{mod} n)+1\}:i\in[n]\right\}\cup \left\{\{m,2m\}\right\},\\
    \gE_2=&\left\{\{i,(i\operatorname{mod} m)+1\}:i\in[m]\right\}\cup \left\{\{i+m,(i\operatorname{mod} m)+m+1\}:i\in[m]\right\}\cup\left\{\{m,2m\}\right\}.
\end{align*}
See \cref{fig:counterexamples}(d) for an illustration of the case $n=8$. It is easy to see that $G_1$ does not have any cut vertex or cut edge, but $G_2$ do have two cut vertices with node number $m$ and $2m$, and has a cut edge $\{m,2m\}$.
\end{example}

\begin{theorem}
\label{thm:scwl_proof}
Let $\gH=\{H_1,\cdots,H_k\}$, $H_i=(\gV_i,\gE_i)$ be any set of connected graphs and denote $n_\mathrm{V}=\max_{i\in[k]}|\gV_i|$. Then SC-WL (\cref{sec:scwl}) using the substructure set $\gH$ can neither distinguish whether a given graph has cut vertices nor distinguish whether it has cut edges. Moreover, there exist counterexample graphs whose size (both in terms of vertices and edges) is $O(n_\mathrm{V})$.
\end{theorem}
\begin{proof}
We would like to prove that SC-WL cannot distinguish both \cref{example:1,example:2} when $n_\mathrm{V}< m$ ($m$ is defined in these examples). First note that for both examples, any cycle in both $G_1$ and $G_2$ has a length of at least $m$. Since the number of nodes in $H_i$ is $O(n_\mathrm{V})$, if $H_i$ contains cycles, it will not occur in both $G_1$ and $G_2$, thus taking no effect in distinguishing the two graphs. As a result, we can simply assume all graphs in $\gH$ are trees (connected graphs with no cycles). Below, we provide a complete proof for \cref{example:1}, which already yields the conclusion that SC-WL can neither distinguish cut vertices nor cut edges. We omit the proof for \cref{example:2} since the proof technique is similar.

\emph{Proof for \cref{example:1}}. Let $H_i$ be a tree with less than $m$ vertices where $m$ is defined in \cref{example:1}. By symmetry of the two graphs $G_1$ and $G_2$, it suffices to prove the following two types of equations: $\vx_{G_1}^\mathrm{V}(n)=\vx_{G_2}^\mathrm{V}(n)$ and $\vx_{G_1}^\mathrm{V}(i)=\vx_{G_2}^\mathrm{V}(i)$ for all $m<i\le 2m$, where $\vx^\mathrm{V}$ is defined in (\ref{eq:scwl_2}). We first aim to prove that $\vx_{G_1}^\mathrm{V}(v)=\vx_{G_2}^\mathrm{V}(v)$ for $v\in\{m+1,\cdots,2m\}$. Consider an induced subgraph $G_1[\gS]$ which is isomorphic to $H_i$ and contains node $v$. Define the set $\gT:=\{jm:j\in[k]\}\cap \gS$. For ease of presentation, we define an operation $\operatorname{cir}(x,a,b)$ that outputs an integer $y$ in the range of $(a,b]$ such that $y$ has the same remainder as $x$ (mod $b-a$). Formally, $\operatorname{cir}(x,a,b)=y$ if $a<y\le b$ and $x\equiv y\pmod{ b-a}$.
\begin{itemize}[topsep=0pt,leftmargin=30pt]
\setlength{\itemsep}{0pt}
    \item If $n\notin\gS$, then it is easy to see that $G_1[\gS]$ is a chain, i.e., no vertices have a degree larger than 2. We define the following mapping $g_\gS:\gS\to [n]$, such that
    \begin{equation*}
        g_\gS(u)=\left\{\begin{array}{ll}
            \operatorname{cir}(u,m,2m) & \text{if }k=1, \\
            \operatorname{cir}(u,0,km) & \text{if }k\ge 2.
        \end{array}\right.
    \end{equation*}
    In this way, the chain $G_1[\gS]$ is mapped to a chain of $G_2$ that contains $v$. Concretely, denote $g_\gS(\gS)=\{g_\gS(u):u\in\gS\}$, then $G_2[g_\gS(\gS)]\simeq G_1[\gS]\simeq H_i$, and obviously the orbit of $v$ in $G_2[g_\gS(\gS)]$ matches the orbit of $v$ in $G_1[\gS]$. See \cref{fig:proof_scwl}(a,b) for an illustration of this case.
    \item If $n\in\gS$, then it is easy to see that the set $\gT\neq \emptyset$. We will similarly construct a mapping $g_\gS:\gS\to [n]$ that maps $\gS$ to $g_\gS(\gS)$ satisfying $g_\gS(v)=v$, which is defined as follows. For each $u\in\gS\backslash\{n\}$, we find a unique vertex $w_u$ in $\gT$ such that $\dis_{G_1[\gS]}(u,w_u)$ is the minimum. Note that the node $w_u$ is well-defined since $\gT\neq\emptyset$ and any path in $G_1[\gS]$ from $u$ to a node in $\gT$ goes through $w_u$. Define
    \begin{equation*}
        g_\gS(u)=\left\{\begin{array}{ll}
            \operatorname{cir}(u,m,2m) & \text{if }k=1\text{ and }w_u=w_v, \\
            \operatorname{cir}(u,0,m) & \text{if }k=1\text{ and }w_u\neq w_v, \\
            \operatorname{cir}(u,0,km) & \text{if }k>1\text{ and }w_u\le km,\\
            \operatorname{cir}(u,km,2km) & \text{if }k>1\text{ and }w_u> km.
        \end{array}\right.
    \end{equation*}
    We also define $g_\gS(n)=n$. Such a definition guarantees that for any $x_1,x_2\in\gS$, $\{x_1,x_2\}\in\gE_{G_1}\iff \{g_\gS(x_1),g_\gS(x_2)\}\in\gE_{G_2}$. Therefore, $G_2[g_\gS(\gS)]\simeq G_1[\gS]\simeq H_i$. Moreover, observe that $g_\gS(u)\equiv u\pmod{m}$ always holds, and thus it is easy to see that the orbit of $v$ in $G_2[g_\gS(\gS)]$ matches the orbit of $v$ in $G_1[\gS]$. See \cref{fig:proof_scwl}(c,d) for an illustration of this case.
\end{itemize}
Finally, note that for any two different sets $\gS_1$ and $\gS_2$ such that $G_1[\gS_1]\simeq G_1[\gS_2]\simeq H_i$, we have $g_{\gS_1}(\gS_1)\neq g_{\gS_2}(\gS_2)$, which guarantees that the mapping $g:\{\gS\subset[n]:G_1[\gS]\simeq H_i,v\in\gS\}\to\{\gS\subset[n]:G_2[\gS]\simeq H_i,v\in\gS\}$ defined to be $g(\gS)=g_\gS(\gS)$ is injective. One can further check that the mapping $g$ is also surjective, and thus it is bijective. This means $\vx_{G_1}^\mathrm{V}(v)=\vx_{G_2}^\mathrm{V}(v)$ for $v\in\{m,\cdots,2m-1\}$. The proof for $\vx_{G_1}^\mathrm{V}(n)=\vx_{G_2}^\mathrm{V}(n)$ is almost the same, so we omit it here. Noting that under classic 1-WL, the colors $\chi_{G_1}(v)=\chi_{G_2}(v)$ are also the same. Therefore, adding the features $\vx^\mathrm{V}(v)$ does not help distinguish the two graphs. We have finished the proof for \cref{example:1}.
\end{proof}

\begin{figure}[t]
    \centering
    \small
    % \setlength\tabcolsep{12pt}
    \begin{tabular}{cccc}
        \hspace{-6pt}\includegraphics[height=18em]{figure/proof_scwl_1.pdf}\hspace{-6pt} & \hspace{-6pt}\includegraphics[height=18em]{figure/proof_scwl_2.pdf}\hspace{-3pt} & \hspace{-3pt}\includegraphics[height=18em]{figure/proof_scwl_3.pdf}\hspace{-6pt} & \hspace{-6pt}\includegraphics[height=18em]{figure/proof_scwl_4.pdf}\hspace{-6pt}\\
        \hspace{-6pt}(a) $n\notin\gS,k=1$\hspace{-6pt} & (b) $n\notin\gS,k>1$ & \hspace{-6pt}(c) $n\in\gS,k=1$\hspace{-6pt} & (d) $n\in\gS,k>1$
    \end{tabular}
    \vspace{-7pt}
    \caption{Illustration of the proof of \cref{thm:scwl}. The trees $G_1[\gS], G_2[g(\gS)]$ are outlined by orange.}
    \label{fig:proof_scwl}
    \vspace{-12pt}
\end{figure}

Using a similar cycle analysis as the above proof, we have the following negative result for Simplicial WL \citep{bodnar2021topological} and Cellular WL \citep{bodnar2021cellular}:

\begin{proposition}
\label{thm:swl_cwl}
Consider the SWL algorithm \citep{bodnar2021topological}, or more generally, the CWL algorithms with either $k$-CL, $k$-IC, or $k$-C as lifting maps ($k\ge 3$ is an integer) \citep[Definition 14]{bodnar2021cellular}. These algorithms can neither distinguish whether a given graph has cut vertices nor distinguish whether it has cut edges.
\end{proposition}
\begin{proof}
Observe that the counterexample graphs in both \cref{example:1,example:2} do not have cliques. Therefore, SWL (or CWL with $k$-CL) reduces to the classic 1-WL and thus fails to distinguish them. Since the lengths of any cycles in these counterexample graphs are at least $m$ ($m$ is defined in \cref{example:1,example:2}), we have that CWL with $k$-IC or $k$-C also reduces to 1-WL when $m>k$. Therefore, there exists graphs whose size is $O(k)$ such that CWL can neither distinguish cut vertices nor cut edges.

Finally, we point out that even if $k$ is not a constant (i.e., can scale to the graph size), CWL with $k$-IC still fails to distinguish whether a given graph has cut vertices. This is because for \cref{example:1} with $k\ge 2$ (e.g. \cref{fig:counterexamples}(b,c)), CWL with IC still outputs the same graph representation for both $G_1$ and $G_2$. This happens because all the 2-dimensional \emph{cells} in these examples are cycles of an equal length of $m+2$ and one can easily check that they have the same CWL color.
\end{proof}

We finally turn to the case of subgraph-based WL variants.

\begin{proposition}
\label{thm:oswl}
The Overlap Subgraph WL \citep{wijesinghe2022new} using any subgraph mapping $\omega$ can neither distinguish whether a given graph has cut vertices nor distinguish whether it has cut edges.
\end{proposition}
\begin{proof}
 An important limitation of OS-WL is that if a graph does not contain triangles, then any overlap subgraph $S_{uv}$ between two adjacent nodes $u,v$ will only have one edge $\{u,v\}$. Consequently, the subgraph mapping $\omega$ does not take effect can OS-WL reduces to the classic 1-WL. Therefore, \cref{example:1} with $m>1$ and \cref{example:2} with $m>3$ still apply here since the graphs $G_1$ and $G_2$ do not contain triangles (see \cref{fig:counterexamples}(a,b,d)). Moreover, \cref{example:1} with $m=1$ (see \cref{fig:counterexamples}(c)) is also a counterexample as discussed in \citet[Figure 2(a)]{wijesinghe2022new}.
\end{proof}

\begin{proposition}
\label{thm:ego_policy}
The DSS-WL with ego network policy without marking cannot distinguish the graphs in \cref{example:1} with $m=1$ (\cref{fig:counterexamples}(c)).
\end{proposition}
\vspace{-5pt}
\begin{proof}
First note that for any two vertices $u,v$ in either $G_1$ or $G_2$ defined in \cref{example:1}, their shortest path distance does not exceed 2. Thus we only need to consider the ego network policy $\pi_\mathrm{EGO(1)}$ and $\pi_\mathrm{EGO(2)}$.
\begin{itemize}[topsep=0pt,leftmargin=30pt]
\setlength{\itemsep}{0pt}
    \item For $\pi_\mathrm{EGO(2)}$, the ego graphs of all nodes are simply the original graph and thus all graphs in the bag $\gB^\pi$ and equal. Thus DSS-WL reduces to the classic 1-WL and cannot distinguish $G_1$ and $G_2$.
    \item For $\pi_\mathrm{EGO(1)}$, the ego graph of each node $v\neq n$ is a graph with 5 edges, having a shape of two triangles sharing one edge. These ego graphs are clearly isomorphic. The ego graph of the special node $n$ is the original graph containing all edges. It is easy to see that the vertex partition of DSS-WL becomes stable only after one iteration, and the color mapping of $G_1$ and $G_2$ is the same. Therefore, DSS-WL cannot distinguish $G_1$ and $G_2$.
\end{itemize}
We thus conclude the proof.
\end{proof}

\begin{proposition}
\label{thm:gnnak}
The GNN-AK architecture proposed in \citet{zhao2022stars} cannot distinguish whether a given graph has cut vertices.
\end{proposition}
\vspace{-5pt}
\begin{proof}
The GNN-AK architecture is quite similar to DSS-WL using the ego network policy but is weaker. There is also a subtle difference: GNN-AK adds the so-called centroid encoding. However, unlike node marking that is performed before the WL procedure, centroid encoding is performed after the WL procedure. The subtle difference causes GNN-AK to be unable to distinguish between the two graphs $G_1$ and $G_2$.
\end{proof}

We finally consider the DS-WL algorithm proposed in \citet{cotta2021reconstruction,bevilacqua2022equivariant}. As discussed in \cref{sec:dsswl}, the original DS-WL formulation only outputs a graph representation rather than node colors. There are two simple ways to define nodes colors for DS-WL:
\begin{itemize}[topsep=0pt,leftmargin=30pt]
\setlength{\itemsep}{0pt}
    \item If the graph generation policy $\pi$ is node-based, then each subgraph in $\gB^\pi_G=\ldblbrace G_i\rdblbrace_{i=1}^{|\gV|}$ is uniquely associated to a specific node $v\in\gV$. We can thus use the graph representation of each subgraph $G_i$ as the color of each node. This strategy has appeared in prior works, e.g. \citet{zhao2022stars}.
    \item For a general graph generation policy $\pi$, there no longer exists an explicit bijective mapping between nodes and subgraphs. In this case, another possible way is to define $\chi_G(v):=\ldblbrace\chi_{G_i}(v):G_i\in\gB^\pi_G\rdblbrace$, similar to DSS-WL. This approach is recently introduced by \citet{qian2022ordered}. However, such a strategy loses the memory advantage of DS-WL (i.e., needing $\Theta(|\gV||\gB^\pi_G|)$ memory complexity rather than $\Theta(|\gV|+|\gB^\pi_G|)$), and is less expressive than DSS-WL. We thus do not study this variant in the present work.
\end{itemize}
\begin{proposition}
\label{thm:dswl_adaptation}
The DS-WL algorithm with node marking/deletion policy cannot distinguish cut vertices when each node's color is defined as its associated subgraph representation.
\end{proposition}
\vspace{-5pt}
\begin{proof}
One can similarly check that for \cref{example:1} with $m=1$ (see \cref{fig:counterexamples}(c)), the color of node $n$ will be the same for both graphs $G_1$ and $G_2$. Therefore, DS-WL cannot identify cut vertices.
\end{proof}
Finally, using a similar proof technique, the NGNN architecture proposed in \citet{zhang2021nested} (with shortest path distance encoding) cannot identify cut vertices.

\subsection{Proof of \cref{thm:dsswl}}
\label{sec:proof_dsswl}
\begin{theorem}
Let $G=(\gV,\gE_G)$ and $H=(\gV,\gE_H)$ be two graphs, and let $\chi_G$ and $\chi_H$ be the corresponding DSS-WL color mapping with node marking policy. Then the following holds:
\begin{itemize}[topsep=0pt,leftmargin=30pt]
\setlength{\itemsep}{0pt}
    \item For any two nodes $w\in\gV$ in $G$ and $x\in\gV$ in $H$, if $\chi_G(w)=\chi_H(x)$, then $w$ is a cut vertex in graph $G$ if and only if $x$ is a cut vertex in graph $H$.
    \item For any two edges $\{w_1,w_2\}\in\gE_G$ and $\{x_1,x_2\}\in\gE_H$, if $\ldblbrace \chi_G(w_1),\chi_G(w_2)\rdblbrace=\ldblbrace \chi_H(x_1),\chi_H(x_2)\rdblbrace$, then $\{w_1,w_2\}$ is a cut edge if and only if $\{x_1,x_2\}$ is a cut edge.
\end{itemize}
\end{theorem}

\emph{Proof}. We divide the proof into two parts in \cref{sec:proof_dsswl_cut_vertex,sec:proof_dsswl_cut_edge}, separately focusing on proving each bullet of \cref{thm:dsswl}. Before going into the proof, we first define several notations. Denote $\chi_G^u(v)$ as the color of node $v$ under the DSS-WL algorithm when marking $u$ as a special node. By definition of DSS-WL (\cref{alg:dsswl_all_graph_aggregation} in \cref{alg:dsswl}), $\chi_G(v)=\operatorname{hash}\left(\ldblbrace \chi_G^u(v):u\in\gV\rdblbrace\right)$. We can similarly define the inverse mappings $(\chi_G^u)^{-1}$.

We first present a lemma which can help us exclude the case of disconnected graphs. 
\begin{lemma}
\label{thm:proof_dsswl_0}
Given a node $w$, let $\gS_G(w)\subset \gV$ be the connected component in graph $G$ that comprises node $w$. For any two nodes $w\in\gV$ in $G$ and $x\in\gV$ in $H$, if $\chi_G(w)=\chi_H(x)$, then $\chi_{G[\gS_G(w)]}(w)=\chi_{H[\gS_H(x)]}(x)$.
\end{lemma}
\begin{proof}
We first prove that if $\chi_G(w)=\chi_H(x)$, then $\ldblbrace \chi_G^u(w):u\in\gS_G(w)\rdblbrace=\ldblbrace \chi_H^u(x):u\in\gS_H(x)\rdblbrace$. First note that for any nodes $u,w$ in $G$ and $v,x$ in $H$, if $u\in\gS_G(w)$ but $v\notin\gS_H(x)$, then $\chi_G^{u}(w)\neq \chi_H^{v}(x)$. This is because DSS-WL only performs neighborhood aggregation, and the marking $v$ cannot propagate to node $x$ while the marking $u$ can propagate to node $w$. By definition we have $$\chi_G(w)=\operatorname{hash}\left(\ldblbrace \chi_G^u(w):u\in\gS_G(w)\rdblbrace \cup \ldblbrace \chi_G^v(w):v\notin\gS_G(w)\rdblbrace\right).$$
Similarly,$$\chi_H(x)=\operatorname{hash}\left(\ldblbrace \chi_H^u(x):u\in\gS_H(x)\rdblbrace \cup \ldblbrace \chi_H^v(x):v\notin\gS_H(x)\rdblbrace\right).$$
Since $\chi_G(w)=\chi_H(x)$, we have $\ldblbrace \chi_G^u:u\in\gS_G(w)\rdblbrace=\ldblbrace \chi_H^u:u\in\gS_H(x)\rdblbrace$. This clearly implies $\ldblbrace \chi_{G[\gS_G(w)]}^u:u\in\gS_G(w)\rdblbrace=\ldblbrace \chi_{H[\gS_H(x)]}^u:u\in\gS_H(x)\rdblbrace$, and thus $\chi_{G[\gS_G(w)]}(w)=\chi_{H[\gS_H(x)]}(x)$.
\end{proof}

Note that $w$ is a cut vertex in $G$ implies $w$ is a cut vertex in $G[\gS_G(w)]$. Therefore, based on \cref{thm:proof_dsswl_0}, we can restrict our attention to subgraphs $G[\gS_G(w)]$ and $H[\gS_H(x)]$ instead of the original (potentially disconnected) graphs. In other words, in the subsequent proof we can simply assume that \emph{both graphs $G$ and $H$ are connected}.

We next present several simple but important properties regrading the DSS-WL color mapping as well as the subgraph color mappings.
\begin{lemma}
\label{thm:proof_dsswl_key}
Let $u,w$ be two nodes in connected graph $G$ and $v,x$ be two nodes in connected graph $H$. Then the following holds:
\begin{enumerate}[label=(\alph*),topsep=0pt,leftmargin=30pt]
\setlength{\itemsep}{0pt}
    \item If $w=u$ and $x\neq v$, then $\chi_G^u(w)\neq \chi_H^v(x)$;
    \item If $\chi_G^u(w)=\chi_H^v(x)$, then $\chi_G(w)=\chi_H(x)$;
    \item If $\chi_G^u(w)=\chi_H^v(x)$, then $\chi_G(u)=\chi_H(v)$;
    \item $\chi_G(w)=\chi_H(x)$ if and only if $\chi_G^w(w)=\chi_H^x(x)$;
    \item If $\chi_G^u(w)=\chi_H^v(x)$, then $\dis_G(u,w)=\dis_H(v,x)$.
\end{enumerate}
\end{lemma}
\begin{proof}
Item (a) holds because in DSS-WL, the node with marking cannot have the same color as a node without marking. This can be rigorously proved by induction over the iteration $t$ in the DSS-WL algorithm (\cref{alg:dsswl_update} in \cref{alg:dsswl}).

Item (b) simply follows by definition of the DSS-WL aggregation procedure since the color $\chi_G^u(w)$ encodes the color of $\chi_G(w)$.

We next prove item (c), which follows by using the WL-condition of DSS-WL algorithm (\cref{thm:dsswl_wl_condition}). Since $G$ is connected, there is a path from $w$ to $u$. Therefore, in graph $H$ there is also a path from $x$ to some node $v'$ satisfying $\chi_G^u(u)=\chi_H^v(v')$ (\cref{thm:wl_condition}). Now using item (a), it can only be the case $v'=v$ and thus $\chi_G^u(u)=\chi_H^v(v)$. Finally, by item (b) we obtain the desired result.

We next prove item (d). On the one hand, item (b) already shows that  $\chi_G^w(w)=\chi_G^x(x)\implies \chi_G(w)=\chi_H(x)$. On the other hand, by definition of the DSS-WL algorithm,
\begin{align*}
    \chi_G(w)&= \operatorname{hash}\left(\ldblbrace \chi_G^w(w)\rdblbrace \cup \ldblbrace \chi_G^u(w):u\in\gV\backslash\{w\}\rdblbrace\right),\\
    \chi_H(x)&= \operatorname{hash}\left(\ldblbrace \chi_H^x(x)\rdblbrace \cup \ldblbrace \chi_H^v(x):v\in\gV\backslash\{x\}\rdblbrace\right).
\end{align*}
Since $\chi_G(w)=\chi_H(x)$ and $\chi_G^w(w)\neq \chi_H^v(x)$ holds for all $v\in\gV\backslash\{x\}$ (by item (a)), we obtain $\chi_G^w(w)=\chi_G^x(x)$.

We finally prove item (e), which again can be derived from the WL-condition of DSS-WL algorithm. If $\chi_G^u(w)=\chi_H^v(x)$, then by \cref{thm:wl_condition_corollary} we have $\dis_G(w,(\chi_G^u)^{-1}(\chi_G^u(u)))=\dis_H(x,(\chi_H^v)^{-1}(\chi_G^u(u)))$. Using item (a), we have $(\chi_G^u)^{-1}(\chi_G^u(u))=\{u\}$ and for any $v'\neq v$, $\chi_H^v(v')\neq\chi_H^v(v)$. Therefore, it can only be the case that $(\chi_H^v)^{-1}(\chi_G^u(u))=\{v\}$ and $\chi_H^v(v)=\chi_G^u(u)$. This yields $\dis_G(u,w)=\dis_G(v,x)$ and concludes the proof.
\end{proof}


\subsubsection{Proof for the first part of \cref{thm:dsswl}}
\label{sec:proof_dsswl_cut_vertex}
The following technical lemma is useful in the subsequent proof:
\begin{lemma}
\label{thm:proof_dsswl_cut_vertex_1}
Let $u,v\in\gV$ be two nodes in connected graphs $G$ and $H$, respectively. If $\chi_G^u(u)=\chi_H^v(v)$, then $\ldblbrace \chi_G^u(w):w\in\gV\rdblbrace=\ldblbrace \chi_H^v(w):w\in\gV\rdblbrace$.
\end{lemma}
\begin{proof}
Let $\gN_G^d(u):=\{w\in\gV:\dis_G(u,w)=d\}$ be the $d$-hop neighbors of node $u$ in graph $G$, and denote $\gC_G^d:=\ldblbrace \chi_G^u(w):w\in\gN_G^d(u)\rdblbrace$ as the multiset containing the color of all nodes $w$ with distance $d$ to node $u$. We can similarly denote $\gN_H^d(v):=\{w:\dis_H(v,w)=d\}$ and $\gC_H^d=\ldblbrace \chi_H^v(w):w\in\gN_H^d(v)\rdblbrace$. It suffices to prove that for all $d\in\mathbb N_+$, $\gC_G^d=\gC_H^d$.

We will prove the above result by induction. The case of $d=0$ is trivial. Now suppose the case of $d$ is true (i.e., $\gC_G^d=\gC_H^d$) and we want to prove $\gC_G^{d+1}=\gC_H^{d+1}$. Note that for any nodes $x_1,x_2$ satisfying $\chi_G^u(x_1)=\chi_H^v(x_2)$, $\ldblbrace\chi_G^u(w):w\in\gN_G(x_1)\rdblbrace=\ldblbrace\chi_H^v(w):w\in\gN_H(x_2)\rdblbrace$. Therefore, by the induction assumption $\gC_G^d=\gC_H^d$,
\begin{equation*}
    \bigcup_{x\in\gN_G^d(u)}\ldblbrace\chi_G^u(w):w\in\gN_G(x)\rdblbrace=\bigcup_{x\in\gN_H^d(v)}\ldblbrace\chi_H^v(w):w\in\gN_H(x)\rdblbrace.
\end{equation*}
We next claim that $\gC_G^d\cap\gC_G^{d'}=\emptyset$ for any $d\neq d'$. This is because for any nodes $w_1$ and $w_2$ with the same color $\chi_G^u(w_1)=\chi_G^u(w_2)$, by \cref{thm:proof_dsswl_key}(e) we have $\dis_G(w_1,u)=\dis_G(w_2,u)$. Using this property, we obtain
\begin{equation*}
    \bigcup_{x\in\gN_G^d(u)}\ldblbrace\chi_G^u(w):w\in\gN_G(x)\cap\gN_G^{d+1}(u)\rdblbrace=\bigcup_{x\in\gN_H^d(v)}\ldblbrace\chi_H^v(w):w\in\gN_H(x)\cap\gN_H^{d+1}(v)\rdblbrace.
\end{equation*}
It is equivalent to the following equation:
\begin{equation*}
    \bigcup_{w\in\gN_G^{d+1}(u)}\ldblbrace\chi_G^u(w)\rdblbrace\times |\gN_G(w)\cap\gN_G^{d}(u)|=\bigcup_{w\in\gN_H^{d+1}(v)}\ldblbrace\chi_H^v(w)\rdblbrace\times |\gN_H(w)\cap\gN_H^{d}(v)|.
\end{equation*}
where $\ldblbrace c\rdblbrace\times m$ is a multiset containing $m$ repeated elements $c$. Finally, observe that if $\chi_G^u(w_1)=\chi_H^v(w_2)$ for some nodes $w_1$ and $w_2$, then $|\gN_G(w_1)\cap\gN_G^{d}(u)|=|\gN_H(w_2)\cap\gN_H^{d}(v)|$ (because $\gC_G^d\cap\gC_G^{d'}=\emptyset$ for any $d\neq d'$). Consequently, $\ldblbrace\chi_G^u(w):w\in\gN_G^{d+1}(u)\rdblbrace=\ldblbrace\chi_H^v(w):w\in\gN_H^{d+1}(v)\rdblbrace$, namely  $\gC_G^{d+1}=\gC_H^{d+1}$. We have thus completed the proof of the induction step.
\end{proof}

We now present the following key result, which shows an important property of the color mapping for DSS-WL:
\begin{corollary}
\label{thm:proof_dsswl_cut_vertex_2}
Let $u,v\in\gV$ be two nodes in connected graph $G$ with the same DSS-WL color, i.e. $\chi_G(u)=\chi_G(v)$. Then for any color $c\in\gC$, $\ldblbrace\chi_G^u(w):w\in\chi_G^{-1}(c)\rdblbrace=\ldblbrace\chi_G^v(w):w\in\chi_G^{-1}(c)\rdblbrace$.
\end{corollary}
\begin{proof}
First observe that if $\chi_G(u)=\chi_G(v)$, then $\chi_G^u(u)=\chi_G^v(v)$ (by \cref{thm:proof_dsswl_key}(d)). Consequently, $\ldblbrace\chi_G^u(w):w\in\gV\rdblbrace=\ldblbrace\chi_G^v(w):w\in\gV\rdblbrace$ holds by \cref{thm:proof_dsswl_cut_vertex_1}. If $\ldblbrace\chi_G^u(w):w\in\chi_G^{-1}(c)\rdblbrace\neq\ldblbrace\chi_G^v(w):w\in\chi_G^{-1}(c)\rdblbrace$, then there must exist two nodes $w_1\in\chi_G^{-1}(c)$ and $w_2\notin\chi_G^{-1}(c)$, such that $\chi_G^u(w_1)=\chi_G^v(w_2)$. Therefore, by \cref{thm:proof_dsswl_key}(b) we have $\chi_G(w_1)=\chi_G(w_2)$, yielding a contradiction.
\end{proof}

In the subsequent proof, we assume the connected graph $G$ is not vertex-biconnected and let $u\in\gV$ be a cut vertex in $G$. Let $\{\gS_i\}_{i=1}^m$ ($m\ge 2$) be the partition of the vertex set $\gV\backslash\{u\}$, representing each connected component after removing node $u$.

\begin{lemma}
\label{thm:proof_dsswl_cut_vertex_3}
There is at most one set $\gS_i$ satisfying $\gS_i\cap\chi_G^{-1}(\chi_G(u))\neq \emptyset$. In other words, if $\gS_i\cap\chi_G^{-1}(\chi_G(u))\neq \emptyset$ for some $i\in[m]$, then for any $j\in[m]$ and $j\neq i$, $\gS_j\cap\chi_G^{-1}(\chi_G(u))=\emptyset$.
\end{lemma}
\begin{proof}
When $|\chi_G^{-1}(\chi_G(u))|=1$, the conclusion clearly holds. If $|\chi_G^{-1}(\chi_G(u))|>1$, then we can pick a node $u_1\in\chi_G^{-1}(\chi_G(u))$ that maximizes the shortest path distance $\dis_G(u_1,u)$. Let $u_1\in\gS_i$ for some $i\in[m]$. If the lemma does not hold, then we can pick another node $u_2\in\chi_G^{-1}(\chi_G(u))$ and $u_2\notin\gS_i$. Since $u_1$ and $u_2$ are in different connected component after removing $u$, $\dis_G(u_1,u_2)=\dis_G(u_1,u)+\dis_G(u_2,u)$. See \cref{fig:proof_dsswl}(a) for an illustration of this paragraph.

By \cref{thm:proof_dsswl_cut_vertex_2}, $\ldblbrace\chi_G^{u_1}(w):w\in\chi_G^{-1}(\chi_G(u))\rdblbrace=\ldblbrace\chi_G^{u}(w):w\in\chi_G^{-1}(\chi_G(u))\rdblbrace$. Therefore, there must exist a node $u_3\in\chi_G^{-1}(\chi_G(u))$ satisfying $\chi_G^{u_1}(u_2)=\chi_G^u(u_3)$. We thus have $\dis_G(u_2,u_1)=\dis_G(u_3,u)$ by \cref{thm:proof_dsswl_key}(e). On the other hand, by definition of the node $u_1$, $\dis_G(u_1,u)\ge \dis_G(u_3,u)$. Therefore, $\dis_G(u_2,u_1)=\dis_G(u_1,u)+\dis_G(u_2,u)> \dis_G(u_3,u)$. This yields a contradiction and concludes the proof.
\end{proof}

\begin{figure}[t]
    \centering
    \small
    \setlength\tabcolsep{20pt}
    \begin{tabular}{cc}
        \includegraphics[height=11em]{figure/proof_dsswl_1.pdf} & \includegraphics[height=11em]{figure/proof_dsswl_2.pdf} \\
        (a) Proof of \cref{thm:proof_dsswl_cut_vertex_3} & (b) Proof of  \cref{thm:proof_dsswl_cut_vertex_4}
    \end{tabular}
    \caption{Several illustrations to help understand the lemmas.}
    \label{fig:proof_dsswl}
\end{figure}

\begin{lemma}
\label{thm:proof_dsswl_cut_vertex_4}
For all $u'\in\chi_G^{-1}(\chi_G(u))$, $u'$ it is a cut vertex of $G$.
\end{lemma}
\begin{proof}
When $|\chi_G^{-1}(\chi_G(u))|=1$, the conclusion clearly holds. Now assume $|\chi_G^{-1}(\chi_G(u))|>1$. Since $u$ is a cut vertex in $G$, by \cref{thm:proof_dsswl_cut_vertex_3}, there exists a set $\gS_j$ such that $\gS_j\cap\chi_G^{-1}(\chi_G(u))=\emptyset$. Pick any node $w\in\gS_j$, then $\chi_G(w)\neq \chi_G(u)$. Let $u'\neq u$ be any node with color $\chi_G(u)=\chi_G(u')$. It follows that $\chi_G^u(u)=\chi_G^{u'}(u')$ by \cref{thm:proof_dsswl_key}(d). Based on the WL-condition of the mappings $\chi_G^u$ and $\chi_G^{u'}$, by \cref{thm:wl_condition} there exists a node $w'$ with color $\chi_G^{u'}(w')=\chi_G^u(w)$ (because there is a path from node $u$ to $w$). See \cref{fig:proof_dsswl}(b) for an illustration of this paragraph.

Suppose $u'$ is not a cut vertex. Then there is a path $P$ from $w'$ to $u$ without going through node $u'$. Denote $P=(x_0,\cdots,x_d)$ where $x_0=w'$ and $x_d=u$. It follows that $\chi_G^{u'}(x_i)\neq\chi_G^{u'}(u')$ for all $i\in[d]$ (by \cref{thm:proof_dsswl_key}(a)). Again by using the WL-condition, there exists a path $Q=(y_0,\cdots,y_d)$ satisfying $y_0=w$ and $\chi_G^u(y_i)=\chi_G^{u'}(x_i)$ for all $i\in[d]$. In particular, $\chi_G^u(y_d)=\chi_G^{u'}(u)$, which implies $\chi_G(y_d)=\chi_G(u)$ by using \cref{thm:proof_dsswl_key}(b). By the definition of $w$ and \cref{thm:proof_dsswl_cut_vertex_3}, any path from $w$ to $y_d\in\chi_G^{-1}(\chi_G(u))$ must go through node $u$, implying that $\chi_G^u(y_i)=\chi_G^u(u)$ for some $i\in[d]$. However, we have proved that $\chi_G^u(y_i)=\chi_G^{u'}(x_i)\neq\chi_G^{u'}(u')=\chi_G^u(u)$, yielding a contradiction. Therefore, $u'$ is a cut vertex.
\end{proof}

Using a similar proof technique as the one in \cref{thm:proof_dsswl_cut_vertex_4}, we can prove the first part of \cref{thm:dsswl}. Suppose $u'\in\chi_H^{-1}(\chi_G(u))$ and we want to prove that $u'$ is a cut vertex of graph $H$. Observe that $|\chi_G^{-1}(\chi_G(u))|=|\chi_H^{-1}(\chi_H(u))|$. (A simple proof is as follows: $\chi_G(u)=\chi_H(u')$ implies $\chi_G^u(u)=\chi_H^{u'}(u')$ by \cref{thm:proof_dsswl_key}(d), and thus using \cref{thm:proof_dsswl_cut_vertex_1} we have $\ldblbrace \chi_G^u(w):w\in\gV\rdblbrace=\ldblbrace \chi_H^{u'}(w):w\in\gV\rdblbrace$ and finally obtain $\ldblbrace \chi_G(w):w\in\gV\rdblbrace=\ldblbrace \chi_H(w):w\in\gV\rdblbrace$ by \cref{thm:proof_dsswl_key}(b).)

\begin{figure}[t]
    \centering
    \small
    \setlength\tabcolsep{5pt}
    \begin{tabular}{cc}
        \includegraphics[height=10em]{figure/proof_dsswl_3.pdf} & \includegraphics[height=10em]{figure/proof_dsswl_4.pdf} \\
        (a) Proof of the main theorem ($|\chi_G^{-1}(\chi_G(u))|>1$) & (b) Proof of the main theorem ($|\chi_G^{-1}(\chi_G(u))|=1$)
    \end{tabular}
    \caption{Several illustrations to help understand the main proof of \cref{thm:dsswl}.}
    \label{fig:proof_dsswl_main}
\end{figure}

We first consider the case when $|\chi_G^{-1}(\chi_G(u))|=|\chi_H^{-1}(\chi_H(u))|>1$. Following the above proof, we can similarly pick $w\in\gS_j$ in $G$ and $w'$ in $H$ satisfying $\chi_G(w)\neq\chi_G(u)$ and $\chi_H^{u'}(w')=\chi_G^u(w)$. Since $|\chi_G^{-1}(\chi_G(u))|>1$, we can pick a node $u_H\in\chi_H^{-1}(\chi_G(u))$ in $H$ such that $u_H\neq u'$. If $u'$ is not a cut vertex, then there is a path $P=(x_0,\cdots,x_d)$ in $H$ where $x_0=w'$ and $x_d=u_H$, such that $\chi_H^{u'}(x_i)\neq\chi_H^{u'}(u')$ for all $i\in[d]$ (by \cref{thm:proof_dsswl_key}(a)). Using the WL-condition, there exists a path $Q=(y_0,\cdots,y_d)$ in $G$ satisfying $y_0=w$ and $\chi_G^u(y_i)=\chi_H^{u'}(x_i)$ for all $i\in[d]$. In particular, $\chi_G^u(y_d)=\chi_H^{u'}(u_H)$, which implies $\chi_G(y_d)=\chi_G(u_H)$ by using \cref{thm:proof_dsswl_key}(b). However, any path from $w$ to $y_d\in\chi_G^{-1}(\chi_G(u))$ must go through node $u$, implying that $\chi_G^u(y_i)=\chi_G^u(u)$ for some $i\in[d]$. This yields a contradiction because $\chi_G^u(y_i)=\chi_H^{u'}(x_i)\neq\chi_H^{u'}(u')=\chi_G^u(u)$. See \cref{fig:proof_dsswl_main}(a) for an illustration of this paragraph.

We finally consider the case when $|\chi_G^{-1}(\chi_G(u))|=|\chi_H^{-1}(\chi_H(u))|=1$. Let $w\in \gS_1$ and $x\in \gS_2$ be two nodes in $G$ that belongs to different connected components when removing node $u$, then $\chi_G(w)\neq\chi_G(u)$ and $\chi_G(x)\neq \chi_G(u)$. Since $\chi_G(u)=\chi_H(u')$, by the WL-condition (\cref{thm:wl_condition}) there is a node $w'\in\chi_H^{-1}(\chi_G(w))$ in $H$. Consequently, $\chi_G^{w}(w)=\chi_H^{w'}(w')$ (\cref{thm:proof_dsswl_key}(d)). Again by the WL-condition, there is a node $x'\in(\chi_H^{w'})^{-1}(\chi_G^w(x))$ in $H$. Clearly, $w'\neq u'$ and $x'\neq u'$ (because they have different colors). If $u'$ is not a cut vertex, then there is path $P=(y_0,\cdots,y_d)$ in $H$ such that $y_0=x'$, $y_d=w'$ and $y_i\neq u'$ for all $i\in[d]$. It follows that for all $i\in[d]$, $\chi_H(y_i)\neq\chi_H(u')$ by our assumption $|\chi_H^{-1}(\chi_H(u))|=1$, and thus $\chi_H^{w'}(y_i)\neq\chi_H^{w'}(u')$ (by \cref{thm:proof_dsswl_key}(b)). Since $\chi_G^w(x)=\chi_H^{w'}(x')$, by the WL-condition (\cref{thm:wl_condition}), there is a path $Q=(z_0,\cdots,z_d)$ in $G$ satisfying $z_0=x$ and $z_i\in(\chi_G^{w})^{-1}(\chi_H^{w'}(y_i))$ for $i\in[d]$. See \cref{fig:proof_dsswl_main}(b) for an illustration of this paragraph.

Clearly, we have $z_d=w$ using $\chi_G^w(z_d)=\chi_H^{w'}(w')$ and \cref{thm:proof_dsswl_key}(a). On the other hand, by \cref{thm:proof_dsswl_key}(b), $\chi_G^w(z_i)=\chi_H^{w'}(y_i)$ implies $\chi_G(z_i)=\chi_H(y_i)$ and thus $\chi_G(z_i)\neq \chi_H(u')=\chi_G(u)$ holds for all $i\in[d]$ and thus $z_i\neq u$. In other words, we have found a path from $x$ to $w$ without going through node $u$, which yields a contradiction as $u$ is a cut vertex. We have thus finished the proof.

\subsubsection{Proof for the second part of \cref{thm:dsswl}}
\label{sec:proof_dsswl_cut_edge}
The proof is based on the following key result:
\begin{corollary}
\label{thm:proof_dsswl_cut_edge_1}
Let $w$ and $x$ be two nodes in connected graph $G$ with the same DSS-WL color, i.e. $\chi_G(w)=\chi_G(x)$. Then for any color $c\in\gC$, $$\ldblbrace\dis_G(w,v):v\in\chi_G^{-1}(c)\rdblbrace=\ldblbrace\dis_G(x,v):v\in\chi_G^{-1}(c)\rdblbrace.$$
\end{corollary}
\begin{proof}
By \cref{thm:proof_dsswl_cut_vertex_2}, we have $\ldblbrace\chi_G^w(v):v\in\chi_G^{-1}(c)\rdblbrace=\ldblbrace\chi_G^x(v):v\in\chi_G^{-1}(c)\rdblbrace$. Since for any nodes $u,v$, $\chi_G^w(u)=\chi_G^x(v)$ implies $\dis_G(u,w)=\dis_G(v,x)$ (by \cref{thm:proof_dsswl_key}(e)), we have obtained the desired conclusion.
\end{proof}
Equivalently, the above corollary says that if $\chi_G(w)=\chi_G(x)$, then the following two multisets are equivalent:
$$\ldblbrace(\dis_G(w,v),\chi_G(v)):v\in\gV\rdblbrace=\ldblbrace(\dis_G(x,v),\chi_G(v)):v\in\gV\rdblbrace.$$
Therefore, it guarantees that the vertex partition induced by the DSS-WL color mapping is \emph{finer} than that of the SPD-WL (\cref{alg:gdwl} with $d_G=\dis_G$). We can thus invoke \cref{thm:spdwl}, which directly concludes the proof (due to \cref{thm:finer_partition}).

\subsection{Proof of \cref{thm:spdwl}}
\label{sec:proof_spdwl}
\begin{theorem}
Let $G=(\gV,\gE_G)$ and $H=(\gV,\gE_H)$ be two graphs, and let $\chi_G$ and $\chi_H$ be the corresponding SPD-WL color mapping. Then the following holds:
\begin{itemize}[topsep=0pt,leftmargin=30pt]
\setlength{\itemsep}{0pt}
    \item For any two edges $\{w_1,w_2\}\in\gE_G$ and $\{x_1,x_2\}\in\gE_H$, if $\ldblbrace \chi_G(w_1),\chi_G(w_2)\rdblbrace=\ldblbrace \chi_H(x_1),\chi_H(x_2)\rdblbrace$, then $\{w_1,w_2\}$ is a cut edge if and only if $\{x_1,x_2\}$ is a cut edge.
    \item If the graph representations of $G$ and $H$ are the same under SPD-WL, then their block cut-edge trees (\cref{def:bcetree}) are isomorphic. Mathematically, $\ldblbrace \chi_G(w):w\in\gV\rdblbrace=\ldblbrace \chi_H(w):w\in\gV\rdblbrace$ implies that $\operatorname{BCETree}(G)\simeq\operatorname{BCETree}(H)$.
\end{itemize}
\end{theorem}
\emph{Proof Sketch}.
The proof of \cref{thm:spdwl} is highly non-trivial and is divided into three parts (presented in \cref{sec:proof_spdwl_case1,sec:proof_spdwl_case2,sec:proof_spdwl_general}, respectively). We first consider the special setting when both $G$ and $H$ are connected and $\ldblbrace \chi_G(w):w\in\gV\rdblbrace=\ldblbrace \chi_H(w):w\in\gV\rdblbrace$. Assume $G$ is not edge-biconnected, and let $\{u,v\}\in\gE_G $ be a cut edge in $G$. We separately consider two cases: $\chi_G(u)\neq \chi_G(v)$ (\cref{sec:proof_spdwl_case1}) and $\chi_G(u)= \chi_G(v)$ (\cref{sec:proof_spdwl_case2}), and prove that any edge $\{u',v'\}\in\gE_H$ satisfying $\ldblbrace\chi_G(u),\chi_G(v)\rdblbrace=\ldblbrace\chi_H(u'),\chi_H(v')\rdblbrace$ is also a cut edge of $H$. This basically finishes the proof of the first bullet in the theorem. Finally, we consider the general setting where graphs $G,H$ can be disconnected and their representation is not the same in \cref{sec:proof_spdwl_general}, and complete the proof of \cref{thm:spdwl}.

Without abuse of notation, throughout \cref{sec:proof_spdwl_case1,sec:proof_spdwl_case2} we redefine the color set $\gC:=\{ \chi_G(w):w\in\gV\}=\{ \chi_H(w):w\in\gV\}$ to focus only on colors that are present in $G$ (or $H$), rather than all (irrelevant) colors in the range of a hash function. 

\subsubsection{The case of \texorpdfstring{$\chi_G(u)\neq \chi_G(v)$}{G(u)=G(v)} for connected graphs}
\label{sec:proof_spdwl_case1}
We first define several notations. Throughout this case, denote $\{\gS_u,\gS_v\}$ as the partition of $\gV$, representing the two connected components after removing the edge $\{u,v\}$ such that $u\in \gS_u$, $v\in \gS_v$, $\gS_u\cap\gS_v=\emptyset$ and $\gS_u\cup\gS_v=\gV$. We then define an important concept called the color graph.

\begin{definition}
\label{def:color_graph}
\normalfont (Color graph) Define the auxiliary color graph $G^\mathrm{C}=(\gC,\gE_{G^\mathrm{C}})$ where $\gE_{G^\mathrm{C}}=\{\ldblbrace \chi_G(w),\chi_G(x)\rdblbrace:\{w,x\}\in E_G\}$. Note that $G^\mathrm{C}$ can have self loops, so each edge is denoted as a multiset with two elements.
\end{definition}

\begin{lemma}
\label{thm:spdwl_case1_1}
Let $\gS=\chi_G^{-1}(\chi_G(u))\cup \chi_G^{-1}(\chi_G(v))$ be the set containing vertices with color $\chi_G(u)$ or $\chi_G(v)$. Then either $\gS\cap \gS_u=\{u\}$ or $\gS\cap \gS_v=\{v\}$.
\end{lemma}
\begin{proof}
Assume the lemma does not hold, i.e. $|\gS\cap \gS_u|>1$ and $|\gS\cap \gS_v|>1$. We first prove that $\chi_G^{-1}(\chi_G(u))\cap \gS_v\neq \emptyset$ and $\chi_G^{-1}(\chi_G(v))\cap \gS_u\neq \emptyset$. By symmetry, we only need to prove the former. Suppose $\chi_G^{-1}(\chi_G(u))\cap\gS_v=\emptyset$, then $(\chi_G^{-1}(\chi_G(v))\cap \gS_v)\backslash \{v\}\neq\emptyset$ (because $|\gS\cap \gS_v|>1$), and thus there exists $v'\in \gS_v$, $v'\neq v$ such that $\chi_G(v')=\chi_G(v)$. Note that $v'$ must connect to a node $u'$ with $\chi_G(u')=\chi_G(u)$. Since $\{u,v\}$ is a cut edge in $G$, $u'\in \gS_v$. Therefore, $\chi_G^{-1}(\chi_G(u))\cap \gS_v\neq \emptyset$, yielding a contradiction. This paragraph is illustrated in \cref{fig:proof_spdwl_1}(a).

We next prove that at least one of the following two conditions holds (which are symmetric): $(\mathrm{i})$ $(\chi_G^{-1}(\chi_G(u))\cap \gS_u)\backslash \{u\}\neq\emptyset$; $(\mathrm{ii})$ $(\chi_G^{-1}(\chi_G(v))\cap \gS_v)\backslash \{v\}\neq\emptyset$. Based on the above paragraph, there exists $v'\in \gS_u$ satisfying $\chi_G(v')=\chi_G(v)$. Note that $v'$ must connect to a node with color $\chi_G(u)$. If condition $(\mathrm{i})$ does not hold, i.e. $\chi_G^{-1}(\chi_G(u))\cap \gS_u=\{u\}$, then $v'$ must connect to $u$. This means $|\gN_G(u)\cap \chi_G^{-1}(\chi_G(v))|\ge 2$. Again using $\chi_G^{-1}(\chi_G(u))\cap\gS_v\neq \emptyset$ (the above paragraph), we can pick such a node $u'\in\chi_G^{-1}(\chi_G(u))\cap\gS_v$. By the WL-condition (\cref{thm:spdwl_wl_condition}), $|\gN_G(u')\cap \chi_G^{-1}(\chi_G(v))|\ge 2$, which implies $|\gS_v\cap \chi_G^{-1}(\chi_G(v))|\ge 2$. Thus $(\chi_G^{-1}(\chi_G(v))\cap \gS_v)\backslash \{v\}\neq\emptyset$ holds, which is exactly the condition $(\mathrm{ii})$. This paragraph is illustrated in \cref{fig:proof_spdwl_1}(b).

Based on the above two paragraphs, by symmetry we can without loss of generality assume $\chi_G^{-1}(\chi_G(u))\cap \gS_v\neq \emptyset$ and $(\chi_G^{-1}(\chi_G(u))\cap \gS_u)\backslash\{u\}\neq \emptyset$. We are now ready to derive a contradiction. To do this, pick $\tilde u=\arg\max_{w\in\chi_G^{-1}(\chi_G(u))} \dis_G(u,w)$ and separately consider the following two cases:
\begin{itemize}[topsep=0pt,leftmargin=30pt]
\setlength{\itemsep}{0pt}
    \item $\tilde u\in \gS_u$. Then by picking a node $x\in \gS_v\cap \chi_G^{-1}(\chi_G(u))$, it follows that $\dis_G(x,\tilde u)= \dis_G(x,v)+\dis_G(u,\tilde u)+1>\dis_G(u,\tilde u)$.
    \item $\tilde u\in \gS_v$. Then by picking a node $x\in (\gS_u\cap \chi_G^{-1}(\chi_G(u)))\backslash \{u\}$, it follows that $\dis_G(x,\tilde u)\ge \dis_G(x,u)+\dis_G(u,\tilde u)>\dis_G(u,\tilde u)$. 
\end{itemize}
In both cases, $x$ and $u$ cannot have the same color under SPD-WL because $$\max_{w\in\chi_G^{-1}(\chi_G(u))} \dis_G(u,w)=\dis_G(u,\tilde u)<\dis_G(x,\tilde u)\le \max_{w\in\chi_G^{-1}(\chi_G(u))} \dis_G(x,w).$$
This yields a contradiction and concludes the proof.
\end{proof}

Based on \cref{thm:spdwl_case1_1}, in the subsequent proof we can without loss of generality assume $\chi_G^{-1}(\chi_G(u))\cap \gS_u=\{u\}$ and $ \chi_G^{-1}(\chi_G(v))\cap \gS_u=\emptyset$. This leads to the following lemma:

\begin{figure}[t]
    \centering
    \begin{minipage}[t]{0.53\textwidth}
        % \centering
        \vspace{0pt}
        % \begin{figure}[t]
        % \centering
        \small
        \begin{tabular}{m{1em}m{1cm}}
        (a) & \includegraphics[height=8em]{figure/proof_spdwl_1_1.pdf}\\
        (b) & \includegraphics[height=8em]{figure/proof_spdwl_1_2.pdf}\\
        \end{tabular}
        \caption{Illustration of the proof of \cref{thm:spdwl_case1_1}.}
        \label{fig:proof_spdwl_1}
        % \end{figure}
    \end{minipage}
    \hspace{10pt}
    \begin{minipage}[t]{0.4\textwidth}
        % \centering
        \vspace{0pt}
        % \begin{figure}[t]
        % \centering
        \small
        \includegraphics[width=\textwidth]{figure/proof_spdwl_2.pdf}
        \caption{Illustration of the proof of \cref{thm:spdwl_case1_2}.}
        \label{fig:proof_spdwl_2}
        % \end{figure}
    \end{minipage}
\end{figure}

\begin{lemma}
\label{thm:spdwl_case1_2}
For any $u_1,u_2\in \chi_G^{-1}(\chi_G(u))$, $u_1\neq u_2$, any path from $u_1$ to $u_2$ goes through a node $v'\in \chi_G^{-1}(\chi_G(v))$.
\end{lemma}
\begin{proof}
Note that $\chi_G^{-1}(\chi_G(u))\cap \gS_u=\{u\}$. If $|\chi_G^{-1}(\chi_G(u))\cap \gS_v|\le 1$, the conclusion is clear since any path from $u_1$ to $u_2$ goes through $v$. Now suppose $|\chi_G^{-1}(\chi_G(u))\cap \gS_v|>1$ and the lemma does not hold. Then there exist two different nodes $u_1',u_2'\in \chi_G^{-1}(\chi_G(u))\cap \gS_v$ and a path $P$ from $u_1'$ to $u_2'$ without going through any node in the set $\chi_G^{-1}(\chi_G(v))$. Pick $u_1$, $u_2$ and $P$ such that the length $|P|$ is minimal. Split $P$ into two parts $P_1$ and $P_2$ with endpoints $\{u_1,w\}$ and $\{w,u_2\}$ such that $|P_1|\le |P_2|\le |P_1|+1$ and $|P_1|+|P_2|=|P|$. Note that $|P|\ge 2$ since $\{u_1,u_2\}\notin \gE_G$ (otherwise $u$ cannot have the same color as $u_1$ because $\chi_G^{-1}(u)\cap \gS_u=\{u\}$). Therefore, $w\neq u_1$ and $w\neq u_2$. Also note that $\chi_G(w)\neq \chi_G(u)$ since $|P|$ is minimal. Since SPD-WL satisfies the WL-condition (\cref{thm:spdwl_wl_condition}), there is a path (not necessarily simple) from $u$ to some $w'\in \chi_G^{-1}(\chi_G(w))$ of length $|P_1|$ without going through nodes in the set $\chi_G^{-1}(\chi_G(v))$ (according to \cref{thm:wl_condition}). Therefore, $w'\in \gS_u$. See \cref{fig:proof_spdwl_2} for an illustration of this paragraph.

We next prove that $\dis_G(u,w')=|P_1|$. First, we obviously have $\dis_G(u,w')\le |P_1|$. Moreover, since $w',u\in\gS_u$ and $\chi_G^{-1}(\chi_G(v))\cap \gS_u=\emptyset$ (\cref{thm:spdwl_case1_1}), any shortest path from $w'$ to $u$ does not go through nodes in the set $\chi_G^{-1}(\chi_G(v))$. Again using the WL-condition, there exists a path $P_3$ (not necessarily simple) from $w$ to some $u_3\in \chi_G^{-1}(\chi_G(u))$ of length $|P_3|=\dis_G(u,w')$ without going through nodes in the set $\chi_G^{-1}(\chi_G(v))$ (according to \cref{thm:wl_condition}). It follows that $u_3\in\gS_v$. Consider the following two cases:
\begin{itemize}[topsep=0pt,leftmargin=30pt]
\setlength{\itemsep}{0pt}
    \item If $u_3= u_1$, by the minimal length of $P$ we have $ |P_1|\le |P_3|=\dis_G(u,w')\le|P_1|$ and thus $\dis_G(u,w')=|P_1|$.
    \item If $u_3\neq u_1$, by linking the path $P_1$ and $P_3$, there will be a path of length $|P_1|+|P_3|$ from $u_1$ to $u_3$ without going through nodes in $\chi_G^{-1}(\chi_G(v))$. Since $P$ has the minimal length, $|P_1|+|P_2|\le |P_1|+|P_3|$. Therefore, $|P_2|\le |P_3|=\dis_G(u,w')$ and thus by definition $|P_1|\le |P_2|\le \dis_G(u,w')\le |P_1|$. Therefore, $|P_1|=|P_2|=\dis_G(u,w')$.
\end{itemize}
Now define the set $\gD(x):=\{u':u'\in\chi_G^{-1}(\chi_G(u)),\dis_G(x,u')\le |P_2|\}$. Let us focus on the cardinality of the sets $\gD(w)$ and $\gD(w')$. It follows that $\gD(w')=\{u\}$, because for any other node $u'\in\chi_G^{-1}(\chi_G(u))$, $u'\neq u$, we have $u'\in\gS_v$ and thus $$\dis_G(w',u')>\dis_G(w',v)=\dis_G(w',u)+1=|P_1|+1\ge |P_2|.$$
Therefore, $|\gD(w')|=1$. On the other hand, we clearly have $|\gD(w)|\ge 2$ since both $u_1,u_2\in \gD(w)$. Consequently, $w$ and $w'$ cannot have the same color under the SPD-WL algorithm because $|\gD(w')|\neq |\gD(w')|$. This yields a contradiction and completes the proof.
\end{proof}

The next lemma presents an important property of the color graph $G^\mathrm{C}$ (defined in \cref{def:color_graph}).

\begin{lemma}
\label{thm:spdwl_case1_3}
$G^\mathrm{C}$ has a cut edge $\ldblbrace\chi_G(u),\chi_G(v)\rdblbrace$.
\end{lemma}
\begin{proof}
Suppose $\ldblbrace \chi_G(u),\chi_G(v)\rdblbrace$ is not a cut edge of $G^\mathrm{C}$. Then there is a simple cycle $(c_1,\cdots,c_m)$ where $c_1=\chi_G(u)$, $c_m=\chi_G(v)$ and $m>2$. Namely, there exists a simple path from $c_1$ to $c_m$ with length $\ge 2$. By the definition of $G^\mathrm{C}$ and the WL-condition, there exists a sequence of nodes of $G$ $\{w_i\}_{i=1}^m$ where $w_1=u$ and $\chi(w_i)=c_i$ such that $\{w_i,w_{i+1}\}\in\gE_G$, $i\in[m-1]$. Note that $w_i\neq u$ for $i=\{2,\cdots,m\}$ and $w_2\neq v$ because $(c_1,\cdots,c_m)$ is a simple path. Therefore, $w_i\in \gS_u$ for all $i\in[m]$. However, it contradicts $|\gS\cap \gS_u|=1$ (\cref{thm:spdwl_case1_1}) since $\chi_G(w_m)=\chi_G(v)$.
\end{proof}

Combining \cref{thm:spdwl_case1_1,thm:spdwl_case1_2,thm:spdwl_case1_3}, we arrived at the following corollary:

\begin{corollary}
\label{thm:spdwl_case1_4}
For all $u'\in\chi_G^{-1}(\chi_G(u))$ and $v'\in\chi_G^{-1}(\chi_G(v))$, if $\{u',v'\}\in\gE_G$, then it is a cut edge of $G$.
\end{corollary}
\begin{proof}
If $\{u',v'\}$ is not a cut edge, there is a simple cycle going through $\{u',v'\}$. Denote it as $(w_1,\cdots,w_m)$ where $w_1=u',w_m=v'$, $m>2$. By \cref{thm:spdwl_case1_1}, $w_2\notin \chi_G(v)$, otherwise $u'$ will connect to at least two different nodes $w_2,w_m\in\chi_G^{-1}(\chi_G(v))$ and thus $u'$ and $u$ cannot have the same color under SPD-WL. Let $j$ be the index such that $j=\min\{j\in [m]:\chi_G(w_j)=\chi_G(v)\}$, then $j>2$. Consider the path $(w_1,\cdots,w_j)$. It follows that $\chi_G(w_k)\neq \chi_G(u)$ for all $k\in\{2,\cdots,j\}$ by \cref{thm:spdwl_case1_2} (otherwise there is a path from node $w_1$ to some node $w_i\in\chi_G^{-1}(\chi_G(u))$ ($i\in\{2,\cdots,j\}$) that does not go through nodes in the set $\chi_G^{-1}(\chi_G(v))$, a contradiction). Therefore, $(\chi_G(w_1),\cdots,\chi_G(w_j))$ is a path of length $\ge 2$ in $G^\mathrm{C}$ from $\chi_G(u)$ to $\chi_G(v)$ (not necessarily simple), without going through the edge $\ldblbrace \chi_G(u),\chi_G(v)\rdblbrace$. This contradicts \cref{thm:spdwl_case1_3}, which says that $\ldblbrace \chi_G(u),\chi_G(v)\rdblbrace$ is a cut edge in $G^\mathrm{C}$.
\end{proof}

Based on \cref{thm:spdwl_case1_3}, the cut edge $\ldblbrace \chi_G(u),\chi_G(v)\rdblbrace$ partitions the vertices $\gC$ of the color graph $G^\mathrm{C}$ into two classes. Denote them as $\{\gC_u,\gC_v\}$ where $\chi_G(u)\in \gC_u$ and $\chi_G(v)\in \gC_v$. The next corollary characterizes the structure of the node colors calculated in SPD-WL.

\begin{corollary}
\label{thm:spdwl_case1_5}
For any $w$ satisfying $\chi_G(w)\in \gC_u$, there exists a cut edge $\{u',v'\}$, $u'\in\chi_G^{-1}(\chi_G(u))$, $v'\in\chi_G^{-1}(\chi_G(v))$, that partitions $\gV$ into two classes $\gS_{u'}\cup \gS_{v'}$, $u',w\in \gS_{u'}$, $v'\in \gS_{v'}$, such that $\chi_G^{-1}(\chi_G(u'))\cup \gS_{u'}=\{u'\}$ and $\chi_G^{-1}(\chi_G(v'))\cup \gS_{u'}=\emptyset$.
\end{corollary}
\begin{remark}
\normalfont \cref{thm:spdwl_case1_5} can be seen as a generalized version of \cref{thm:spdwl_case1_1}. Indeed, when $w\in \gS_u$, one can pick $u'=u$ and $v'=v$. Then $\chi_G^{-1}(\chi_G(u'))\cup \gS_{u'}=\{u'\}$ and $\chi_G^{-1}(\chi_G(v'))\cup \gS_{u'}=\emptyset$ hold due to \cref{thm:spdwl_case1_1}. In general, \cref{thm:spdwl_case1_5} says that all the cut edges with color $\{\chi_G(u),\chi_G(v)\}$ play an equal role: \cref{thm:spdwl_case1_1} applies for any chosen cut edge $\{u',v'\}$. An illustration of \cref{thm:spdwl_case1_5} is given in \cref{fig:proof_spdwl_3}(a).
\end{remark}
\begin{proof}
By the definition of $\gC_u$, any node $c\in\gC_u$ in the \emph{color graph} can reach the node $\chi_G(u)$ without going through $\chi_G(v)$. Therefore, there exists some $u'\in\chi_G^{-1}(\chi_G(u))$ such that there exists a path $P_1$ from $w$ to $u'$ without going through nodes in the set $\chi_G^{-1}(\chi_G(v))$. Also, there exists a node $v'\in\gN_G(u')$ with $\chi_G(v')=\chi_G(v)$ due to the color of $u'$. By \cref{thm:spdwl_case1_4}, $\{u',v'\}$ is a cut edge of $G$. Clearly, $w\in \gS_{u'}$. 

We next prove the following fact: for any $x\in \gS_{u'}$, $\chi_G(x)\in \gC_u$. Otherwise, one can pick a node $x\in\gS_{u'}$ with color $\chi_G(x)\in\gC_v$. Consider the \emph{shortest} path between nodes $x$ and $u'$, denoted as $(y_1,\cdots,y_m)$ where $y_1=x$ and $y_m=u'$. It follows that $y_i\in \gS_u$ for all $i\in[m]$. Denote $c_i=\chi_G(y_i), i\in [m]$. Then $(c_1,\cdots,c_m)$ is a path (not necessarily simple) in the color graph $G^\mathrm{C}$. Now pick the index $j=\max\{j\in[m]:c_j\in \gC_v\}$ (which is well-defined because $c_1\in\gC_v$). It follows that $j<m$ (since $y_m\in\gC_u$), $c_j=\chi_G(v)$ and $c_{j+1}=\chi_G(u)$ (because $\ldblbrace\chi_G(u),\chi_G(v)\rdblbrace$ is a cut edge that partitions the color graph $G^\mathrm{C}$ into $\gC_u$ and $\gC_v$). Consider the following two cases (see \cref{fig:proof_spdwl_3}(b) for an illustration):
\begin{itemize}[topsep=0pt,leftmargin=30pt]
\setlength{\itemsep}{0pt}
    \item $j=m-1$. Then $u'$ connects to both nodes $y_j$ and $v'$ with color $\chi_G(y_j)=\chi_G(v')=\chi_G(v)$. This contradicts \cref{thm:spdwl_case1_1} since $u$ only connects to one node $v$ with color $\chi_G(v)$.
    \item $j<m-1$. Then $y_{j+1}\neq u'$ because the path $(y_1,\cdots,y_m)$ is simple. Howover, one has $\chi_G(y_i)\neq \chi_G(v)$ for all $i\in\{j+1,\cdots,m\}$ by definition of $j$. This contradicts \cref{thm:spdwl_case1_2}.
\end{itemize}
This completes the proof that for any $x\in \gS_{u'}$, $\chi_G(x)\in \gC_u$. Therefore, $\chi_G^{-1}(\chi_G(v'))\cup \gS_{u'}=\emptyset$.

We finally prove that $\chi_G^{-1}(\chi_G(u))\cup \gS_{u'}=\{u'\}$. If not, pick $u''\in \chi_G^{-1}(\chi_G(u))\cup \gS_{u'}$ and $u''\neq u'$. By \cref{thm:spdwl_case1_2}, the \emph{shortest} path between $u'$ and $u''$ goes through some node $v''$ with color $\chi_G(v)$. Clearly, $v''\in \gS_u$, which contradicts the above paragraph and concludes the proof.
\end{proof}

\begin{figure}[t]
    \centering
    \small
    \setlength\tabcolsep{15pt}
    \begin{tabular}{cc}
        \includegraphics[height=15em]{figure/proof_spdwl_3_1.pdf} & \includegraphics[height=15em]{figure/proof_spdwl_3_2.pdf} \\
        (a) & (b)
    \end{tabular}
    \vspace{-7pt}
    \caption{Illustration of \cref{thm:spdwl_case1_5} and its proof.}
    \label{fig:proof_spdwl_3}
    \vspace{-5pt}
\end{figure}

We have already fully characterized the properties of cut edges $\{u',v'\}$ with color $\{\chi_G(u),\chi_G(v)\}$. Now we switch our focus to the graph $H$. We first prove a general result that holds for arbitrary $H$.

\begin{lemma}
\label{thm:spdwl_case1_6}
Let $\{w_1,w_2\}\in \gE_H$ and $P$ is a path with the minimum length from $w_1$ to $w_2$ without going through edge $\{w_1,w_2\}$. In other words, linking path $P$ with the edge $\{w_1,w_2\}$ forms a simple cycle $Q$. Then for any two nodes $x_1,x_2$ in $Q$, $\dis_H(x_1,x_2)=\dis_Q(x_1,x_2)$.
\end{lemma}
\begin{proof}
Split the cycle $Q$ into two paths $Q_1$ and $Q_2$ with endpoints $\{x_1,x_2\}$ where $Q_1$ contains the edge $\{w_1,w_2\}$ and $Q_2$ does not contain $\{w_1,w_2\}$. Assume the above lemma does not hold and $\dis_H(w,x)<\dis_Q(w,x)$. It means that there exists a path $R$ in $H$ from $x_1$ to $x_2$ for which $|R|<\min(|Q_1|,|Q_2|)$. Note that the edge $\{u,v\}$ occurs at most once in $R$. Separately consider two cases:
\begin{itemize}[topsep=0pt,leftmargin=30pt]
\setlength{\itemsep}{0pt}
    \item $\{w_1,w_2\}$ occurs in $R$. Then linking $R$ with $Q_2$ forms a cycle that contains $\{w_1,w_2\}$ exactly once;
    \item $\{w_1,w_2\}$ does not occur in $R$. Then linking $R$ with $Q_1$ forms a cycle that contains $\{w_1,w_2\}$ exactly once.
\end{itemize}
In both cases, the cycle has a length less than $|Q|$. This contradicts the condition that $P$ is a path with minimum length from $w_1$ to $w_2$ without passing edge $\{w_1,w_2\}$. 
\end{proof}

We can similarly consider the color graph $H^\mathrm{C}=(\gC,\gE_{H^\mathrm{C}})$ defined in \cref{def:color_graph}. Note that we have assumed that the graph representations of $G$ and $H$ are the same, i.e. $\ldblbrace \chi_G(w):w\in\gV\rdblbrace=\ldblbrace \chi_H(w):w\in\gV\rdblbrace$. It follows that $H^\mathrm{C}$ is isomorphic to $G^\mathrm{C}$ and the identity vertex mapping is an isomorphism, i.e., $\ldblbrace c_1,c_2\rdblbrace\in\gE_{G^\mathrm{C}}\iff\ldblbrace c_1,c_2\rdblbrace\in\gE_{H^\mathrm{C}}$. Therefore, $\ldblbrace\chi_G(u),\chi_G(v)\rdblbrace$ is a cut edge of $H^\mathrm{C}$ (\cref{thm:spdwl_case1_3}) that splits the vertices $\gC$ into two classes $\gC_u,\gC_v$. Since the vertex labels of $H$ are not important, we can without abuse of notation let $u,v$ be two nodes such that $\{u,v\}\in\gE_H$, $\chi_H(u)=\chi_G(u)$, $\chi_H(v)=\chi_G(v)$, and $\chi_H(u)\in\gC_u$, $\chi_H(v)\in \gC_v$. We similarly define $\chi_H^{-1}(c)=\{w\in\gV:\chi_H(w)=c\}$. Define a mapping $h:\gC\to\{\chi_H(u),\chi_H(v)\}$ where
\begin{equation}
    h(c)=\left\{\begin{array}{ll}
        \chi_H(u) & \text{if } \dis_{H^\mathrm{C}}(c,\chi_H(u))<\dis_{H^\mathrm{C}}(c,\chi_H(v)), \\
        \chi_H(v) & \text{if } \dis_{H^\mathrm{C}}(c,\chi_H(u))>\dis_{H^\mathrm{C}}(c,\chi_H(v)).
    \end{array}\right.
\end{equation}
Note that it never happens that $\dis_{H^\mathrm{C}}(c,\chi_H(u))=\dis_{H^\mathrm{C}}(c,\chi_H(v))$ because $\ldblbrace\chi_H(u),\chi_H(v)\rdblbrace$ is a cut edge of $H^\mathrm{C}$.

Assume $\{u,v\}$ is not a cut edge in $H$. Then there exists a path $(w_1,\cdots,w_m)$ in $H$ with $w_1=u$ and $w_m=v$ without going through $\{u,v\}$. We pick such a path with the minimum length, then the path is simple. Since $h(\chi_H(u))\in\gC_u$ and $h(\chi_H(v))\in\gC_v$, there is a minimum index $j\in[m-1]$ such that $h(\chi_H(w_j))\in\gC_u$ and $h(\chi_H(w_{j+1}))\in\gC_v$. By definition of $\gC_u,\gC_v$ and the cut edge $\ldblbrace\chi_H(u),\chi_H(v)\rdblbrace$, it follows that $\chi_H(w_j)=\chi_H(u)$ and $\chi_H(w_{j+1})=\chi_H(v)$. Denote $u':=w_j$. Note that $j\neq 1$ and $j\neq 2$, otherwise $u$ either connects to two nodes $w_2$ and $w_m$ with color $\chi_H(w_2)=\chi_H(w_m)=\chi_H(v)$, or connects to the node $u'$ with color $\chi_H(u')=\chi_H(u)$, contradicting $\chi_H(u)=\chi_G(u)$. Pick $k=\lceil j/2\rceil$. By \cref{thm:spdwl_case1_6}, $(w_1,\cdots,w_k)$ is the shortest path between $u$ and $w_k$, and $(w_k,\cdots,w_j)$ is the shortest path between $w_k$ and $u'$. We give an illustration of the structure of $H$ in \cref{fig:proof_spdwl_4}(a) based on this paragraph.

\begin{figure}[t]
    \centering
    \small
    \setlength\tabcolsep{12pt}
    \begin{tabular}{cc}
        \includegraphics[height=13em]{figure/proof_spdwl_4_1.pdf} & \includegraphics[height=13em]{figure/proof_spdwl_4_2.pdf} \\
        (a) Graph $H$ & (b) Graph $G$
    \end{tabular}
    % \vspace{-7pt}
    \caption{Illustrations to help understand the proof of the main result.}
    \label{fig:proof_spdwl_4}
    \vspace{-5pt}
\end{figure}

Since the graph representations of $G$ and $H$ are the same under SPD-WL, there exists a node $w'$ with color $\chi_G(w')=\chi_H(w_k)$ and two different nodes $u_1',u_2'$ with color $\chi_G(u_1')=\chi_G(u_2')=\chi_G(u)$, such that $\dis_G(w',u_1')=\dis_H(w_k,w_1)$ and $\dis_G(w',u_2')=\dis_H(w_k,w_j)$. In particular, $|\dis_G(w',u_1')-\dis_G(w',u_2')|\le 1$. Note that by the definition of indices $j$ and $k$, in the color graph $H^\mathrm{C}$ there is a path from $\chi_H(w_k)$ to $\chi_H(u)$ without going through nodes in the set $\chi_H^{-1}(\chi_H(v))$, so $\chi_H(w_k)\in\gC_u$, namely $\chi_G(w')\in\gC_u$. By \cref{thm:spdwl_case1_5}, there is a cut edge $\{u_w',v_w'\}$ that partitions $G$ into two vertex sets $\gS_{u_w'}, \gS_{v_w'}$, with $w',u_w'\in \gS_{u_w'}$, $v_w'\in \gS_{v_w'}$. Note that $u_w'\neq u_1'$ and $u_w'\neq u_2'$ (otherwise by \cref{thm:spdwl_case1_5} any path from $w'$ to a node $u'\neq u_w'$ with color $\chi_G(u')=\chi_G(u)$ must first go through $u_w'$ and then go through $v_w'$, implying that $|\dis_G(w',u_1')-\dis_G(w',u_2')|\ge 2$ and yielding a contradiction). Therefore, $\dis_G(w',u_1')>\dis_G(w',u_w')$ and $\dis_G(w',u_2')>\dis_G(w',u_w')$. We give an illustration of the structure of $G$ in \cref{fig:proof_spdwl_4}(b) based on this paragraph.

Pick any $v_w\in \chi_H^{-1}(\chi_H(v))$ satisfying $\dis_H(v_w,w_k)=\dis_G(v_w',w')$. Denote the operation $\operatorname{dropmin}(\gS):=\gS\backslash\ldblbrace\min\gS\rdblbrace$ that takes a multiset $\gS$ and removes one of the minimum elements in $\gS$. We have
\begin{align}
    \notag
    &\operatorname{dropmin}(\ldblbrace \dis_G(w',u_G):u_G\in \textstyle\chi_G^{-1}(\chi_G(u))\rdblbrace)\\
    =&\operatorname{dropmin}(\ldblbrace \dis_G(w',v_w')+\dis_G(v_w',u_G):u_G\in\chi_G^{-1}(\chi_G(u))\rdblbrace) \tag{\text{by \cref{thm:spdwl_case1_5}}}\\
    \notag
    =&\operatorname{dropmin}(\ldblbrace \dis_H(w_k,v_w)+\dis_H(v_w,u_H):u_H\in\chi_H^{-1}(\chi_H(u))\rdblbrace)
\end{align}
and also
\begin{equation*}
    \operatorname{dropmin}(\ldblbrace \dis_G(w',u_G):u_G\in \textstyle\chi_G^{-1}(\chi_G(u)))= \operatorname{dropmin}(\ldblbrace\dis_H(w_k,u_H):u_H\in\chi_H^{-1}(\chi_H(u))\rdblbrace)
\end{equation*}
due to the same color $\chi_G(w')=\chi_H(w_k)$. Combining the above two equations and noting that $\dis_H(w_k,v_w)+\dis_H(v_w,u_H)\ge \dis_H(w_k,u_H)$, we obtain the following result: for any $u_H\in\chi_H^{-1}(\chi_H(u))$ for which $\dis_H(w_k,v_w)+\dis_H(v_w,u_H)>\dis_G(w',u_w')$, $\dis_H(w_k,v_w)+\dis_H(v_w,u_H)= \dis_H(w_k,u_H)$. In particular,
\begin{align*}
    \dis_H(w_k,w_1)&=\dis_H(w_k,v_w)+\dis_H(v_w,w_1),\\
    \dis_H(w_k,w_j)&=\dis_H(w_k,v_w)+\dis_H(v_w,w_j).
\end{align*}
Therefore,
\begin{equation*}
\begin{aligned}
    \dis_H(w_1,w_j)&=\dis_H(w_1,w_k)+\dis_H(w_k,w_j)\\
    &=2\dis_H(w_k,v_w)+\dis_H(v_w,w_1)+\dis_H(v_w,w_j)\\
    &\ge 2\dis_H(w_k,v_w)+\dis_H(w_1,w_j),
\end{aligned}
\end{equation*}
implying $w_k=v_w$. However, $\chi_H(w_k)\in\gC_u$ while $\chi_H(v_w)\in\gC_v$, yielding a contradiction.


\subsubsection{The case of \texorpdfstring{$\chi_G(u)= \chi_G(v)$}{G(u)=G(v)} for connected graphs}
\label{sec:proof_spdwl_case2}
We first define several notations. Define the mapping $f_G:\gV\to \{u,v\}\times \gC$ as follows: $f_G(w)=(h_G(w),\chi_G(w))$ where
\begin{equation}
\label{eq:proof_spdwl_case2}
    h_G(w)=\left\{\begin{array}{ll}
        u & \text{if } \dis_G(w,v)=\dis_G(w,u)+1,\\
        v & \text{if } \dis_G(w,u)=\dis_G(w,v)+1.
    \end{array}\right.
\end{equation}
It is easy to see that $h_G$ is well-defined for all $w\in\gV$ because $\{u,v\}$ is a cut edge of $G$. We further define the following auxiliary graph:
\begin{definition}
\label{def:aux_graph}
\normalfont (Auxiliary graph) Define the auxiliary graph $G^\mathrm{A}=(\gV_{G^\mathrm{A}},\gE_{G^\mathrm{A}})$ where $\gV_{G^\mathrm{A}}:=\{u,v\}\times \gC$ and $\gE_{G^\mathrm{A}}:=\{\ldblbrace f_G(w_1),f_G(w_2)\rdblbrace:\{w_1,w_2\}\in \gE_G\}$. Note that $G^\mathrm{A}$ can have self loops, so each edge is denoted as a multiset with two elements.
\end{definition}
It is straightforward to see that there is only one edge in $G^\mathrm{A}$ with the form $\ldblbrace(u,c_1),(v,c_2)\rdblbrace\in\gE_{G^\mathrm{A}}$ for some $c_1,c_2\in\gC$ since $\{u,v\}$ is a cut edge of $G$. Therefore, the only edge is $\ldblbrace(u,\chi_G(u)),(v,\chi_G(v))\rdblbrace$ and is a cut edge in $G^\mathrm{A}$.

We also define $f_G^{-1}$ as the inverse mapping of $f_G$, i.e. $f_G^{-1}(z,c)=\{w\in \gV:f_G(w)=(z,c)\}$. We first prove that $f_G^{-1}$ is well-defined on the domain $\gV_{G^\mathrm{A}}$.

\begin{lemma}
\label{thm:spdwl_case2_1}
$f_G$ is a surjection.
\end{lemma}
\begin{proof}
Suppose that $f_G$ is not a surjection. Then there exists a color $c\in \gC$ such that either $f_G^{-1}(u,c)$ or $f_G^{-1}(v,c)$ is an empty set. Without loss of generality, assume $f_G^{-1}(v,c)=\emptyset$, then $f_G^{-1}(u,c)\neq\emptyset$. Pick any $w\in f_G^{-1}(u,c)$. Obviously, $w\neq u$ (otherwise $f_G^{-1}(v,\chi_G(v))= \emptyset$, a contradiction). Then we claim that for any $x\in\gN_G(w)$, $f_G^{-1}(v,\chi_G(x))$ is empty. Note that $x\in f_G^{-1}(u,\chi_G(x))$. If the claim does not hold, take $x'\in f_G^{-1}(v,\chi_G(x))$. Since $x$ connects to a node with color $c$ and $\chi_G(x)=\chi_G(x')$, $x'$ must also connect to a node with color $c$. Denote the node that connects to $x'$ with color $c$ as $w'$. Then $w'\in f_G^{-1}(v,c)$, yielding a contradiction.

By induction, for any $x$ such that there exists a path from $x$ to $w$ without going through the edge $\{u,v\}$, we have $f_G^{-1}(v,\chi_G(x))=\emptyset$. This finally implies $f_G^{-1}(v,\chi_G(v))=\emptyset$, leading to a contradiction. Therefore, $f$ is a surjection.
\end{proof}

\begin{lemma}
\label{thm:spdwl_case2_2}
$|f_G^{-1}(u,\chi_G(u))|=|f_G^{-1}(v,\chi_G(v))|=1$.
\end{lemma}
\begin{proof}
Pick $u'=\arg\max_{u'\in f_G^{-1}(u,\chi(u))}\dis_G(u,u')$ and similarly pick $v'$. It follows that any path between $u'$ and $v'$ goes through edge $\{u,v\}$. Therefore, $\dis_G(u',v')=\dis_G(u,u')+\dis_G(v,v')+1$. Since all nodes $u,u',v,v'$ have the same color under SPD-WL, there exists a node $w\in\chi_G^{-1}(\chi_G(u))$ satisfying $\dis_G(u,w)=\dis_G(u',v')$ and thus $\dis_G(u,w)>\dis_G(u,u')$. By definition of the node $u'$, $f_G(w)\neq (u,\chi(u))$ and thus $f_G(w)= (v,\chi(u))$. Therefore, $\dis_G(u,w)=\dis_G(v,w)+1$, which implies that $$\dis_G(v,w)=\dis_G(v,v')+\dis_G(u,u').$$
Since $\dis_G(v,w)\le \dis_G(v,v')$, we have $\dis_G(v,w)=\dis_G(v,v')$ and $u=u'$. A similar argument yields $v=v'$, finishing the proof. 
\end{proof}

We can now prove some useful properties of the auxiliary graph $G^\mathrm{A}$ based on \cref{thm:spdwl_case2_1,thm:spdwl_case2_2}.
\begin{corollary}
\label{thm:spdwl_case2_3}
For any $c_1,c_2\in \gC$, $\ldblbrace (u,c_1),(u,c_2)\rdblbrace\in \gE_{G^\mathrm{A}}$ if and only if $\ldblbrace (v,c_1),(v,c_2)\rdblbrace\in \gE_{G^\mathrm{A}}$.
\end{corollary}
\begin{proof}
By definition of $\gE_G^\mathrm{A}$, if $\ldblbrace (u,c_1),(u,c_2)\rdblbrace\in \gE_{G^\mathrm{A}}$, then there exists two vertices $w_1\in f_G^{-1}(u,c_1)$ and $w_2\in f_G^{-1}(u,c_2)$ such that $\{w_1,w_2\}\in\gE_G$. By \cref{thm:spdwl_case2_2}, either $\chi_G(w_1)\neq \chi_G(u)$ or $\chi_G(w_2)\neq \chi_G(u)$. Without loss of generality, assume $c_1\neq \chi_G(u)$. By \cref{thm:spdwl_case2_1}, there exists $x_1\in f_G^{-1}(v,c_1)$. Since $\chi_G(x_1)=\chi_G(w_1)$, $x_1$ must also connect to a node $x_2$ with $\chi_G(x_2)=c_2$. The edge $\{x_1,x_2\}\neq \{u,v\}$ because $\chi_G(x_1)=c_1\neq \chi_G(u)$. Therefore, $f(x_2)=(v,c_2)$, namely $\ldblbrace(v,c_1),(v,c_2)\rdblbrace\in \gE_G^\mathrm{A}$.
\end{proof}

The following lemma establishes the distance relationship between graphs $G$ and $G^\mathrm{A}$.

\begin{lemma}
\label{thm:spdwl_case2_4}
The following holds:
\begin{itemize}[topsep=0pt,leftmargin=30pt]
\setlength{\itemsep}{0pt}
    \item For any $w,w'\in\gV$, $\dis_G(w,w')\ge\dis_{G^\mathrm{A}}(f(w),f(w'))$.
    \item For any $\xi,\xi'\in\gV_{G^\mathrm{A}}$ and any node $w\in f_G^{-1}(\xi)$, there exists a node $w'\in f_G^{-1}(\xi')$ such that $\dis_G(w,w')=\dis_{G^\mathrm{A}}(\xi,\xi')$.
\end{itemize}
\end{lemma}
\begin{proof}
The first bullet is trivial since for all $\{w,w'\}\in\gE_G$, $\ldblbrace f(w),f(w')\rdblbrace\in\gE_{G^\mathrm{A}}$ by \cref{def:aux_graph}. We prove the second bullet in the following. Note that $G^\mathrm{A}$ can have self-loops, but for any $\xi,\xi'\in\gV^\mathrm{A}$, the shortest path between $\xi$ and $\xi'$ will not go through self-loops. We only need to prove that for all $\ldblbrace\xi,\xi'\rdblbrace\in\gE^\mathrm{A}$, $\xi\neq \xi'$ and all $w\in f_G^{-1}(\xi)$, there exists $w'\in f_G^{-1}(\xi')$ such that $\{w,w'\}\in \gE_G$. This will imply that for any $\xi,\xi'\in\gV_{G^\mathrm{A}}$ and any node $w\in f_G^{-1}(\xi)$, there exists a node $w'\in f_G^{-1}(\xi')$ such that $\dis_G(w,w')\le\dis_{G^\mathrm{A}}(\xi,\xi')$, which completes the proof by combining the first bullet in \cref{thm:spdwl_case2_4}.

The case of $\ldblbrace\xi,\xi'\rdblbrace=\ldblbrace(u,\chi_G(u)),(v,\chi_G(v))\rdblbrace$ is trivial. Now assume that $\ldblbrace\xi,\xi'\rdblbrace\neq\ldblbrace(u,\chi_G(u)),(v,\chi_G(v))\rdblbrace$. By \cref{def:aux_graph}, there exists $x\in f_G^{-1}(\xi)$ and $x'\in f_G^{-1}(\xi')$, such that $\{x,x'\}\in\gE_G$. Note that $h_G(x)=h_G(x')$ because $\{x,x'\}\neq \{u,v\}$. Since $\chi_G(x)=\chi_G(w)$, there exists $w'\in \chi_G^{-1}(\chi_G(x'))$ such that $\{w,w'\}\in\gE_G$. It must hold that $h_G(w)=h_G(w')$ (otherwise $\{w,w'\}=\{u,v\}$ and thus $\ldblbrace\xi,\xi'\rdblbrace=\ldblbrace(u,\chi_G(u)),(v,\chi_G(v))$). Therefore, $h_G(w')=h_G(w)=h_G(x)=h_G(x')$ and thus $f_G(w')=f_G(x')$, namely $w'\in f_G^{-1}(\xi')$.
\end{proof}

\cref{thm:spdwl_case2_4} leads to the following corollary:
\begin{corollary}
\label{thm:spdwl_case2_5}
The following holds:
\begin{itemize}[topsep=0pt,leftmargin=30pt]
\setlength{\itemsep}{0pt}
    \item For any $w,w'\in\gV$ satisfying $\chi_G(w)=\chi_G(w')$ and $h_G(w)=h_G(w')$ (i.e. $f_G(w)=f_G(w')$), $\dis_G(u,w)=\dis_G(u,w')$ and $\dis_G(v,w)=\dis_G(v,w')$;
    \item For any $w,w'\in\gV$ satisfying $\chi_G(w)=\chi_G(w')$ and $h_G(w)\neq h_G(w')$, $\dis_G(u,w)=\dis_G(v,w')$ and $\dis_G(v,w)=\dis_G(u,w')$.
\end{itemize}
\end{corollary}
\begin{proof}
Proof of the first bullet: by \cref{thm:spdwl_case2_4}, there exists two nodes $u_1,u_2\in f_G^{-1}(f_G(u))$ such that $\dis_G(u_1,w)=\dis_{G^\mathrm{A}}(f_G(u),f_G(w))$ and $\dis_G(u_2,w')=\dis_{G^\mathrm{A}}(f_G(u),f_G(w'))$. Therefore, $\dis_G(u_1,w)=\dis_G(u_2,w')$. However, by \cref{thm:spdwl_case2_2} and the condition $h_G(w)=h_G(w')$, it must be $u_1=u_2=u$, namely $\dis_G(u,w)=\dis_G(u,w')$. The proof of $\dis_G(v,w)=\dis_G(v',w')$ is similar.

Proof of the second bullet: Let $\chi_G(w)=\chi_G(w')=c$. Without loss of generality, assume $f_G(w)=(u,c)$ and $f(w')=(v,c)$. By \cref{thm:spdwl_case2_4}, it suffices to prove that $\dis_{G^\mathrm{A}}((u,\chi_G(u)),(u,c))=\dis_{G^\mathrm{A}}((v,\chi_G(v)),(v,c))$. By the definition of $G^\mathrm{A}$ and its cut edge $\ldblbrace(u,\chi_G(u),(v,\chi_G(v))\rdblbrace$, the shortest path between $(u,\chi_G(u))$ and $(u,c)$ must only go through nodes in the set $\{(u,c_1):c_1\in \gC\}$, and similarly the shortest path between $(v,\chi_G(v))$ and $(v,c)$ must only go through nodes in $\{(v,c_2):c_2\in \gC\}$. Finally, \cref{thm:spdwl_case2_3} says that for $c_1,c_2\in\gC$, $\ldblbrace(u,c_1),(u,c_2)\rdblbrace\in G^\mathrm{A}$ if and only if $\ldblbrace(v,c_1),(v,c_2)\rdblbrace\in G^\mathrm{A}$. We thus conclude that $\dis_{G^\mathrm{A}}((u,\chi_G(u)),(u,c))=\dis_{G^\mathrm{A}}((v,\chi_G(v)),(v,c))$ and $\dis_G(u,w)=\dis_G(v,w')$.
\end{proof}

Finally, we can prove the following important corollary:
\begin{corollary}
\label{thm:spdwl_case2_6}
For any $c\in\gC$, $|f_G^{-1}(u,c)|=|f_G^{-1}(v,c)|$.
\end{corollary}
\begin{proof}
Pick any $w\in f_G^{-1}(u,c)$ and $x\in f_G^{-1}(v,c)$. By \cref{thm:spdwl_case2_5}, we have
\begin{align*}
    \dis_G(w,u)&=\dis_G(x,v):=d,\\
    \dis_G(w,v)&=\dis_G(x,u)=d+1.
\end{align*}
The multiset $\ldblbrace\dis_G(u,w'):\chi_G(w')=c\rdblbrace$ contains $|f_G^{-1}(u,c)|$ elements of value $d$ and $|f_G^{-1}(v,c)|$ elements of value $d+1$. The multiset $\ldblbrace\dis_G(v,w'):\chi_G(w')=c\rdblbrace$ has $|f_G^{-1}(v,c)|$ elements of value $d$ and $|f_G^{-1}(u,c)|$ elements of value $d+1$. Since $u$ and $v$ has the same color under SPD-WL, the two multiset must be equivalent. Therefore, $|f_G^{-1}(u,c)|=|f_G^{-1}(v,c)|$.
\end{proof}

Next, we switch our focus to the graph $H$. Since we have assumed that the graph representations of $G$ and $H$ are the same, i.e. $\ldblbrace\chi_G(w): w\in\gV\rdblbrace = \ldblbrace\chi_H(w): w\in\gV\rdblbrace$, the size of the set $\{w\in \gV:\chi_H(w)=\chi_G(u)\}$ must be 2. We may denote the elements as $u$ and $v$ without abuse of notation and thus $\{u,v\}\in\gE_H$. Also for any $w\in \gV$, we have $\dis_H(w,u)\neq\dis_H(w,v)$. Therefore, we can similarly define the mapping $f_H:\gV\to\{u,v\}\times\gV$ and the mapping $h_H:\gV\to\{u,v\}$ as in (\ref{eq:proof_spdwl_case2}). The auxiliary graph $H^\mathrm{A}$ is defined analogous to \cref{def:aux_graph}.

\begin{lemma}
\label{thm:spdwl_case2_7}
For any $c\in\gC$, $|f_H^{-1}(u,c)|=|f_H^{-1}(v,c)|=|f_G^{-1}(u,c)|=|f_G^{-1}(v,c)|$.
\end{lemma}
\begin{proof}
If $|f_H^{-1}(u,c)|\neq |f_H^{-1}(v,c)|$, we have $\ldblbrace \dis_H(u,w):\chi_H(w)=c\rdblbrace\neq \ldblbrace \dis_H(v,w):\chi_H(w)=c\rdblbrace$, implying that $u$ and $v$ cannot have the same color under SPD-WL. This already concludes the proof by using \cref{thm:spdwl_case2_6} as $$|f_H^{-1}(u,c)|+|f_H^{-1}(v,c)|=|f_G^{-1}(u,c)|+|f_G^{-1}(v,c)|.$$

\vspace{-25pt}
\end{proof}

We finally present a technical lemma which will be used in the subsequent proof.
\begin{lemma}
\label{thm:spdwl_case2_8}
Given node $w\in\gV$ and color $c\in\gC$, define multisets
\begin{align*}
    \gD_{G,=}(w,c)&:=\ldblbrace\dis_G(w,x):x\in\chi_G^{-1}(c),h_G(x)=h_G(w)\rdblbrace,\\
    \gD_{G,\neq}(w,c)&:=\ldblbrace\dis_G(w,x):x\in\chi_G^{-1}(c),h_G(x)\neq h_G(w)\rdblbrace.
\end{align*}
For any two nodes $w,w'\in{\gV}$ in graphs $G$ and $H$ satisfying $\chi_G(w)=\chi_H(w')$, pick any $d\in\gD_{G,\neq}(w,c)$ and $d'\in\gD_{H,=}(w',c)$. Then $d'<d$.
\end{lemma}
\begin{proof}
Without loss of generality, assume $h_G(w)=h_H(w')=u$ and let $f_G(w)=f_H(w')=(u,c_w)$. Pick $x\in f_G^{-1}(v,c)$ and $x'\in f_H^{-1}(u,c)$, then $\dis_H(x',u)=\min(\dis_G(x,u),\dis_G(x,v))$ and $\dis_H(w',u)=\min(\dis_G(w,u),\dis_G(w,v))$. Thus
\begin{align}
    \notag
    \dis_H(w',x')&\le \dis_H(w',u)+\dis_H(u,x')\\
    \notag
    &=\min(\dis_G(w,u),\dis_G(w,v))+\min(\dis_G(x,u),\dis_G(x,v))\\
    \notag
    &<\min(\dis_G(w,u)+\dis_G(x,v),\dis_G(w,v)+\dis_G(x,u))+1\\
    \notag
    &=\dis_G(w,x),
\end{align}
which concludes the proof.
\end{proof}

In the following, we will prove that $\{u,v\}$ is a cut edge in graph $H$. Consider an edge $\ldblbrace(u,c_1),(v,c_2)\rdblbrace\in \gE_{H^\mathrm{A}}$ (such an edge exists because $\ldblbrace(u,\chi_H(u)),(v,\chi_H(v))\rdblbrace\in \gE_H^\mathrm{A}$). We will prove that this is the only case, i.e. it must be $c_1=\chi_H(u)=\chi_H(v)=c_2$.

By \cref{def:aux_graph}, $\ldblbrace(u,c_1),(v,c_2)\rdblbrace\in \gE_{H^\mathrm{A}}$ implies that there exists two nodes $x'\in f_H^{-1}(u,c_1)$ and $w'\in f_H^{-1}(v,c_2)$, such that $\{w',x'\}\in\gE_H$. Pick $w\in \chi_G^{-1}(c_2)$. By \cref{thm:spdwl_case2_8}, $\gD_{H,=}(w',c_1)\cap \gD_{G,\neq}(w,c_1)=\emptyset$. Since $w'$ and $w$ have the same color under SPD-WL, $$\gD_{H,=}(w',c_1)\cup \gD_{H,\neq}(w',c_1)=\gD_{G,=}(w,c_1)\cup\gD_{G,\neq}(w,c_1).$$
By \cref{thm:spdwl_case2_7}, $|\gD_{H,=}(w',c_1)|=|\gD_{H,\neq}(w',c_1)|=|\gD_{G,=}(w,c_1)|=|\gD_{G,\neq}(w,c_1)|$. Therefore, $\gD_{G,\neq}(w,c_1)=\gD_{H,\neq}(w',c_1)$. We claim that all elements in the set $\gD_{G,\neq}(w,c_1)$ are the same. This is because for any $x\in \chi_G^{-1}(c_1)$, $h_G(x)\neq h_G(w)$, we have $$\dis_G(w,x)=\dis_G(w,h(w))+1+\dis_G(h(x),x),$$
and by \cref{thm:spdwl_case2_5} $\dis_G(h(x),x)$ has an equal value for different $x$. Since $\{w',x'\}\in\gE_H$, we have $1\in\gD_{H,\neq}(w',c_1)$ and thus all elements in $\gD_{G,\neq}(w,c_1)$ equals 1. Therefore, $c_1=\chi_G(u)$. Analogously, $c_2=\chi_G(u)$. Therefore, $c_1=\chi_H(u)=\chi_H(v)=c_2$.

Let $\gS_u=\{w\in\gV:h_H(w)=u\}$ and $\gS_v=\{w\in\gV:h_H(w)=v\}$. Then the above argument implies that if $w\in\gS_u$, $x\in\gS_v$ and $\{w,x\}\in\gE_G$, then $\{w,x\}=\{u,v\}$. Therefore $\{u,v\}$ is a cut edge of graph $H$.


\subsubsection{The general case}
\label{sec:proof_spdwl_general}
The above proof assumes that the graphs $G$ and $H$ are both connected, and their graph representations are euqal, i.e. $\ldblbrace \chi_G(w):w\in\gV\rdblbrace=\ldblbrace \chi_H(w):w\in\gV\rdblbrace$. In the subsequent proof we remove these assumptions and prove the general setting.

\begin{lemma}
\label{thm:spdwl_general_1}
Either of the following two properties holds:
\begin{itemize}[topsep=0pt,leftmargin=30pt]
\setlength{\itemsep}{0pt}
    \item $\ldblbrace \chi_G(w):w\in\gV\rdblbrace=\ldblbrace \chi_H(w):w\in\gV\rdblbrace$;
    \item $\ldblbrace \chi_G(w):w\in\gV\rdblbrace\cap\ldblbrace \chi_H(w):w\in\gV\rdblbrace=\emptyset$.
\end{itemize}
\end{lemma}
\begin{proof}
Consider the GD-WL procedure defined in \cref{alg:gdwl} with arbitrary distance function $d_G$. Suppose at iteration $t\ge T$, $\ldblbrace \chi_G^t(w):w\in\gV\rdblbrace\neq\ldblbrace \chi_H^t(w):w\in\gV\rdblbrace$. Then at iteration $t+1$, we have for each $v\in\gV$,
$$\chi_G^{t+1}(v)= \operatorname{hash}\left(\ldblbrace \operatorname{hash}(d_G(v,u), \chi_G^{t}(u)):u\in \gV\rdblbrace\right).$$
Therefore, $\chi_H^{t+1}(v)\neq \chi_G^{t+1}(u)$ for all $u\in\gV_G$ and $v\in\gV_H$, namely $$\ldblbrace \chi_G^{t+1}(w):w\in\gV\rdblbrace\cap\ldblbrace \chi_H^{t+1}(w):w\in\gV\rdblbrace=\emptyset.$$
Finally, by the injective property of the hash function, for any $t\ge T+1$, the above equation always holds. Therefore, the stable color mappings $\chi_G$ and $\chi_H$ satisfy \cref{thm:spdwl_general_1}.
\end{proof}

The above lemma implies that if there exists edges $\{w_1,w_2\}\in\gE_G$, $\{x_1,x_2\}\in\gE_H$ satisfying $\ldblbrace \chi_G(w_1),\chi_G(w_2)\rdblbrace=\ldblbrace \chi_H(x_1),\chi_H(x_2)\rdblbrace$, then $\ldblbrace \chi_G(w):w\in\gV\rdblbrace=\ldblbrace \chi_H(w):w\in\gV\rdblbrace$. Also, SPD-WL ensures that both graphs are either connected or disconnected. If they are both connected, the previous proof (\cref{sec:proof_spdwl_case1,sec:proof_spdwl_case2}) ensures that $\{w_1,w_2\}$ is a cut edge of $G$ if and only if $\{x_1,x_2\}$ is a cut edge of $H$. For the disconnected case, let $\gS_G\subset \gV$ be the largest connected component containing nodes $w_1,w_2$, and similarly denote $\gS_H\subset \gV$ as the largest connected component containing nodes $x_1,x_2$. Obviously, $|\gS_G|=|\gS_H|$ due to the facts that $\dis_G(w_1,y)=\infty\neq \dis_G(w_1,y')$ for all $y\notin \gS_G,y'\in\gS_G$ and that the two edges $\{w_1,w_2\}\in\gE_G$, $\{x_1,x_2\}$ have the same color under SPD-WL. Moreover, $\ldblbrace \chi_G(w):w\in\gS_G\rdblbrace=\ldblbrace \chi_H(w):w\in\gS_H\rdblbrace$. Now consider re-execute the SPD-WL algorithm on subgraphs $G[\gS_G]$ and $H[\gS_H]$ induced by the vertices in set $\gS_G$ and $\gS_H$, respectively. It follows that for any $u_G\in\gS_G$ and $u_H\in\gS_H$, $\chi_G(u_G)=\chi_H(u_H)$ implies that $\chi_{G[\gS_G]}(u_G)=\chi_{H[\gS_H]}(u_H)$. Therefore, $\{w_1,w_2\}$ is a cut edge of $G[\gS_G]$ if and only if $\{x_1,x_2\}$ is a cut edge of $H[\gS_H]$. By the dinifition of $\gS_G$ and $\gS_H$, $\{w_1,w_2\}$ is a cut edge of $G$ if and only if $\{x_1,x_2\}$ is a cut edge of $H$.

It remains to prove that $\ldblbrace \chi_G(w):w\in\gV\rdblbrace=\ldblbrace \chi_H(w):w\in\gV\rdblbrace$ implies $\operatorname{BCETree}(G)\simeq\operatorname{BCETree}(H)$. By definition of the block cut-edge tree, each cut edge of $G$ corresponds to a tree edge in $\operatorname{BCETree}(G)$ and each biconnected component of $G$ corresponds to a node of $\operatorname{BCETree}(G)$. We still only focus on the case of connected graphs $G,H$, and it is straightforward to extend the proof to the general (disconnected) case using a similar technique as the previous paragraph.

Given a fixed SPD-WL graph representation $\gR$, consider any graphs $G=(\gV,\gE_G)$ satisfying $\ldblbrace\chi_G(w):w\in\gV\rdblbrace=\gR$. Since we have proved that the SPD-WL node feature $\chi_G(v)$, $v\in\gV$ precisely locates all the cut edges, the multiset $$\gC^\mathrm{E}:=\ldblbrace\{\chi_G(u),\chi_G(v)\}:\{u,v\}\in\gE_G \text{ is a cut edge}\rdblbrace$$
is fixed (fully determined by $\gR$, not $G$). Denote $\gC^\mathrm{V}:=\bigcup_{\{c_1,c_2\}\in\gC^\mathrm{E}}\{c_1,c_2\}$
as the set that contains the color of endpoints of all cut edges. For each cut edge $\{u,v\}\in\gE_G$, denote $\gS_{G,u}$ and $\gS_{G,v}$ be the vertex partition corresponding to the two connected components after removing the edge $\{u,v\}$, satisfying $u\in\gS_{G,u}$, $v\in\gS_{G,v}$, $\gS_{G,u}\cap\gS_{G,v}=\emptyset$, $\gS_{G,u}\cup\gS_{G,v}=\gV$. It suffices to prove that given a cut edge $\{u,v\}\in\gE_G$ with color $\{\chi_G(u),\chi_G(v)\}$, the multiset $\ldblbrace \chi_G(w):w\in \gS_{G,u},\chi_G(w)\in\gC^\mathrm{V}\rdblbrace$ can be determined purely based on $\gR$ and the edge color $\ldblbrace\chi_G(u),\chi_G(v)\rdblbrace$, rather than the specific graph $G$ or edge $\{u,v\}$. This basically concludes the proof, since the BCETree can be uniquely constructed as follows: if $\ldblbrace \chi_G(w):w\in \gS_{G,u},\chi_G(w)\in\gC^\mathrm{V}\rdblbrace=\ldblbrace\chi_G(u)\rdblbrace$ (i.e. with only one element), then $\ldblbrace\chi_G(u),\chi_G(v)\rdblbrace$ is a leaf edge of the BCETree such that $\chi_G(u)$ connects to a biconnected component that is a leaf of the BCETree. After finding all the leaf edges, we can then find the BCETree edges that connect to leaf edges and determine which leaf edges they connect. The procedure can be recursively executed until the full BCETree is constructed. The whole procedure does not depend on the specific graph $G$  and only depends on $\gR$.

We now show how to determine $\ldblbrace \chi_G(w):w\in \gS_{G,u},\chi_G(w)\in\gC^\mathrm{V}\rdblbrace$ given a cut edge $\{u,v\}\in\gE_G$ with color $\{\chi_G(u),\chi_G(v)\}$. Define the multiset
\begin{equation}
    \gD(c_1,c_2):=\ldblbrace \dis_G(w,x):x\in\chi_G^{-1}(c_2) \rdblbrace\tag{$w\in\chi_G^{-1}(c_1)$ can be picked arbitrarily}
\end{equation}
Note that $\gD(c_1,c_2)$ is well-defined (does not depend on $w$) by definition of the SPD-WL color. For any $c_u,c_v\in\gC^\mathrm{E}$, pick arbitrary cut edge $\{u,v\}$ with color $\chi_G(u)=c_u,\chi_G(v)=c_v$. Define
\begin{equation}
    \gT(c_u,c_v)=\bigcup_{c\in\gC^\mathrm{V}}\ldblbrace c\rdblbrace\times |(\gD(c_u,c)+1)\cap\gD(c_v,c)|
\end{equation}
where $\ldblbrace c\rdblbrace\times m$ denotes a multiset with $m$ repeated elements $c$, and $\gD(c_u,c)+1:=\ldblbrace d+1:d\in\gD(c_u,c)\rdblbrace$. Intuitively speaking, $\gT(c_u,c_v)$ corresponds to the color of all nodes $w\in\gV$ such that $\dis_G(u,w)+1=\dis_G(v,w)$ and $\chi_G(w)\in\gC^\mathrm{V}$. Therefore, $\gT(c_u,c_v)$ is exactly the multiset $\ldblbrace \chi_G(w):w\in \gS_{G,u},\chi_G(w)\in\gC^\mathrm{V}\rdblbrace$ and we have completed the proof.

\subsection{Proof of \cref{thm:rdwl}}
\label{sec:proof_rdwl}
\begin{theorem}
Let $G=(\gV,\gE_G)$ and $H=(\gV,\gE_H)$ be two graphs, and let $\chi_G$ and $\chi_H$ be the corresponding RD-WL color mapping. Then the following holds:
\begin{itemize}[topsep=0pt,leftmargin=30pt]
\setlength{\itemsep}{0pt}
    \item For any two nodes $w\in\gV$ in $G$ and $x\in\gV$ in $H$, if $\chi_G(w)=\chi_H(x)$, then $w$ is a cut vertex of $G$ if and only if $x$ is a cut vertex of $H$.
    \item If the graph representations of $G$ and $H$ are the same under RD-WL, then their block cut-vertex trees (\cref{def:bcvtree}) are isomorphic. Mathematically, $\ldblbrace \chi_G(w):w\in\gV\rdblbrace=\ldblbrace \chi_H(w):w\in\gV\rdblbrace$ implies that $\operatorname{BCVTree}(G)\simeq\operatorname{BCVTree}(H)$.
\end{itemize}
\end{theorem}
\begin{proof}[Proof Sketch]
First observe that \cref{thm:spdwl_general_1} holds for general distances and thus applies here. Therefore, if $\chi_G(w)=\chi_H(x)$, the graph representations will be the same, i.e. $\ldblbrace \chi_G(w):w\in\gV\rdblbrace=\ldblbrace \chi_H(w):w\in\gV\rdblbrace$. By a similar analysis as SPD-WL (\cref{sec:proof_spdwl_general}), we can only focus on the case that both graphs are connected. We prove the first bullet of \cref{thm:rdwl} in \cref{sec:proof_rdwl_part1} and prove the second bullet in \cref{sec:proof_rdwl_part2}, both assuming that $G$ and $H$ are connected and their graph representations are the same.
\end{proof}

\subsubsection{Proof of the first part}
\label{sec:proof_rdwl_part1}

We first present a key property of the Resistance Distance, which surprisingly relates to the cut vertices in a graph.
\begin{lemma}
\label{thm:proof_rdwl_part1_0}
Let $G=(\gV,\gE)$ be a connected graph and $v\in \gV$. Then $v$ is a cut vertex of $G$ if and only if there exists two nodes $u,w\in\gV$, $u\neq v$, $w\neq v$, such that $\disR_G(u,v)+\disR_G(v,w)=\disR_G(u,w)$.
\end{lemma}
\begin{proof}
We use the key finding that the Resistance Distance is equivalent to the Commute Time Distance multiplied by a constant \citep[see also \cref{sec:detail_of_rdwl}]{chandra1996electrical}, i.e. $\disC_G(u,w)=2|\gE|\disR_G(u,w)$. Here, the Commute Time Distance is defined as $\disC_G(u,w):=h_G(u,w)+h_G(w,u)$ where $h_G(u,w)$ is the average hitting time from $u$ to $w$ in a random walk (\cref{sec:detail_of_rdwl}).
\begin{itemize}[topsep=0pt,leftmargin=30pt]
\setlength{\itemsep}{0pt}
    \item If $v$ is not a cut vertex, given any nodes $u,w$, $u\neq v$, $w\neq v$, we can partition the set of all \emph{hitting} paths $\gP_{uw}$ from $u$ to $w$ (not necessarily simple) into two sets $\gP_{uw}^v$ and $\overline\gP_{uw}^{v}$ such that all paths $P\in\gP_{uw}^v$ contain $v$ and no path $P\in\overline\gP_{uw}^v$ contains $v$. Clearly, $\overline\gP_{uw}^v\neq\emptyset$. Given a path $P=(x_0,\cdots,x_m)$, define the probability function $q(P):=1/\prod_{i=0}^{m-1} \deg_G(x_i)$. Then by definitions of the average hitting time $h$,
    \begin{align*}
        h_G(u,w)&=\sum_{P\in\gP_{uw}}q(P)\cdot |P|=\sum_{P\in\gP_{uw}^v}q(P)\cdot |P|+\sum_{P\in\overline\gP_{uw}^v}q(P)\cdot |P|\\
        &=\sum_{P_1\in\overline\gP_{uv}^w,P_2\in\gP_{vw}}q(P_1)q(P_2)(|P_1|+|P_2|)+\sum_{P\in\overline\gP_{uw}^v}q(P)\cdot |P|\\
        &=\sum_{P_1\in\overline\gP_{uv}^w}q(P_1)|P_1|\left(\sum_{P_2\in\gP_{vw}} q(P_2)\right)+\sum_{P_2\in\gP_{vw}}q(P_2)|P_2|\left(\sum_{P_1\in\overline\gP_{uv}^w} q(P_1)\right)\\
        &\quad+\sum_{P\in\overline\gP_{uw}^v}q(P)|P|\\
        &\le \sum_{P\in\overline\gP_{uv}^w}q(P)|P|+\sum_{P\in\overline\gP_{uw}^v}q(P)|P|+\sum_{P\in\gP_{vw}}q(P)|P|\\
        &< \sum_{P\in\overline\gP_{uv}^w}q(P)|P|+\sum_{P\in\gP_{uv}^w}q(P)|P|+\sum_{P\in\gP_{vw}}q(P)|P|\\
        &=h_G(u,v)+h_G(v,w).
    \end{align*}
    We can similarly prove that $h_G(w,u)< h_G(w,v)+h_G(v,u)$.
    \item If $v$ is a cut vertex, then there exist two different nodes $u,w\in\gV$, $u\neq v$, $w\neq v$, such that any path from $u$ to $w$ goes through $v$. A similar analysis yields the conclusion that $h_G(u,w)=h_G(u,v)+h_G(v,w)$ and $h_G(w,u)=h_G(w,v)+h_G(v,u)$.
\end{itemize}
This completes the proof of \cref{thm:proof_rdwl_part1_0}.
\end{proof}

In the subsequent proof, assume $u\in\gV$ is a cut vertex of $G$, and let $\{\gS_i\}_{i=1}^m$ ($m\ge 2$) be the partition of the vertex set $\gV\backslash\{u\}$, representing each connected component after removing node $u$. We have the following lemma (which has a similar form as \cref{thm:spdwl_case1_1}):

\begin{lemma}
\label{thm:proof_rdwl_part1_1}
There is at most one set $\gS_i$ satisfying $\gS_i\cap\chi_G^{-1}(\chi_G(u))\neq \emptyset$. In other words, if $\gS_i\cap\chi_G^{-1}(\chi_G(u))\neq \emptyset$ for some $i\in[m]$, then for any $j\in[m]$ and $j\neq i$, $\gS_j\cap\chi_G^{-1}(\chi_G(u))=\emptyset$.
\end{lemma}
\begin{proof}
Let $u_i=\arg\max_{u'\in\chi_G^{-1}(\chi_G(u))}\disR_G(u,u')$. If $u_i=u$, then $\gS_i\cap\chi_G^{-1}(\chi_G(u))= \emptyset$ for all $i\in[m]$ and thus \cref{thm:proof_rdwl_part1_1} clearly holds. Otherwise, $u_i\in \gS_i$ for some $i$. We will prove that for any $j\neq i$, $\gS_j\cap\chi_G^{-1}(\chi_G(u))=\emptyset$.

If the above conclusion does not holds, then we can pick a set $\gS_j$ and a vertex $u_j\in\gS_j\cap\chi_G^{-1}(\chi_G(u))$. Since $u$ is a cut vertex and $\gS_i$, $\gS_j$ are different connected components, by \cref{thm:proof_rdwl_part1_0} we have $\disR_G(u_i,u_j)=\disR_G(u_i,u)+\disR_G(u,u_j)>\disR_G(u_i,u)$. This yields a contradiction because $\max_{u'\in\chi_G^{-1}(\chi_G(u))}\disR_G(u,u')\neq \max_{u'\in\chi_G^{-1}(\chi_G(u_i))}\disR_G(u_i,u')$, which means that $u$ and $u_i$ cannot have the same RD-WL color.
\end{proof}

The next lemma presents a key result which is similar to \cref{thm:spdwl_case1_4}.

\begin{lemma}
\label{thm:proof_rdwl_part1_2}
For all $u'\in\chi_G^{-1}(\chi_G(u))$, $u'$ it is a cut vertex of $G$.
\end{lemma}
\begin{proof}
If $|\chi_G^{-1}(\chi_G(u))|=1$, then \cref{thm:proof_rdwl_part1_2} clearly holds. Otherwise, by \cref{thm:proof_rdwl_part1_1} there exists two sets $\gS_i$ and $\gS_j$ satisfying $\gS_i\cap\chi_G^{-1}(\chi_G(u))\neq \emptyset$, $\gS_j\cap\chi_G^{-1}(\chi_G(u))= \emptyset$. Since $\gS_j\neq \emptyset$, we can pick $w\in \gS_j$ with color $\chi_G(w)\neq\chi_G(u)$. Pick $u'\in\gS_i\cap\chi_G^{-1}(\chi_G(u))$. Since $\chi_G(u)=\chi_G(u')$, there exists a node $w'\in\chi_G^{-1}(\chi_G(w))$ such that $\disR_G(u,w)=\disR_G(
u',w')$. Then we have
\begin{align}
    \label{eq:proof_rdwl_part1_2_0}
    \ldblbrace\disR_G(w,u''):u''\in\chi_G^{-1}(\chi_G(u))\rdblbrace
    &=\ldblbrace\disR_G(w,u)+\disR_G(u,u''):u''\in\chi_G^{-1}(\chi_G(u))\rdblbrace\\
    \label{eq:proof_rdwl_part1_2_1}
    &=\ldblbrace\disR_G(w',u')+\disR_G(u',u''):u''\in\chi_G^{-1}(\chi_G(u))\rdblbrace
\end{align}
where (\ref{eq:proof_rdwl_part1_2_0}) holds because $u$ is a cut vertex and all $u''\neq u$ are in the set $\gS_i$ but $w\in\gS_j$ (\cref{thm:proof_rdwl_part1_1}), and (\ref{eq:proof_rdwl_part1_2_1}) holds because $\chi_G(u)=\chi_G(u')$. On the other hands, 
$$\ldblbrace\disR_G(w,u''):u''\in\chi_G^{-1}(\chi_G(u))\rdblbrace=\ldblbrace\disR_G(w',u''):u''\in\chi_G^{-1}(\chi_G(u))\rdblbrace.$$
Therefore, $\disR_G(w',u'')=\disR_G(w',u')+\disR_G(u',u'')$ for all $u''\in\chi_G^{-1}(\chi_G(u))$. Pick $u''=u$, then clearly $u''\neq u'$ and $u''\neq w$. \cref{thm:proof_rdwl_part1_0} shows that $u'$ is a cut vertex, which concludes the proof. See \cref{fig:proof_rdwl_1} for an illustration of the above proof.
\end{proof}

\begin{wrapfigure}{r}{0.32\textwidth}
  \vspace{-10pt}
  \begin{center}
   \includegraphics[width=0.3\textwidth]{figure/proof_rdwl_1.pdf}
  \end{center}
  \vspace{-10pt}
  \caption{Illustration of the proof of \cref{thm:proof_rdwl_part1_2}.}
  \label{fig:proof_rdwl_1}
  \vspace{-20pt}
\end{wrapfigure}

Using a similar proof technique as the one in \cref{thm:proof_rdwl_part1_2}, we can prove the first bullet of \cref{thm:rdwl}. Note that we have assumed $\ldblbrace \chi_G(w):w\in\gV\rdblbrace=\ldblbrace \chi_H(w):w\in\gV\rdblbrace$. First consider the case when $|\chi_G^{-1}(\chi_G(u))|>1$. Pick $w_H\in\chi_H^{-1}(\chi_G(w))$ where $w$ is defined in the proof of \cref{thm:proof_rdwl_part1_2}. Then, there exists $u_H\in\chi_H^{-1}(\chi_G(u))$ satisfying $\disR_H(w_H,u_H)=\disR_G(w_,u)$. Pick another node $u_H'\in\chi_H^{-1}(\chi_G(u))$, $u_H'\neq u_H$ (this is feasible as $|\chi_H^{-1}(\chi_G(u))|>1$). Following the procedure of the above proof, we can obtain that
$\disR_H(w_H,u'')=\disR_H(w_H,u_H)+\disR_H(u_H,u'')$ for all $u''\in\chi_H^{-1}(\chi_G(u))$. Therefore, $\disR_H(w_H,u_H')=\disR_H(w_H,u_H)+\disR_H(u_H,u_H')$, implying $u_H$ is a cut vertex of $H$ by \cref{thm:proof_rdwl_part1_0}.

Now consider the case when $|\chi_G^{-1}(\chi_G(u))|=1$. Then $|\chi_H^{-1}(\chi_G(u))|=1$ and we can denote the node in $\chi_H^{-1}(\chi_G(u))$ as $u$ without abuse of notation. Choose arbitrary two nodes $w_1\in\gS_1$ and $w_2\in\gS_2$, then $\disR_G(w_1,u)+\disR_G(u,w_2)=\disR_G(w_1,w_2)$ (\cref{thm:proof_rdwl_part1_0}). Pick any $w_1'\in\chi_H^{-1}(\chi_G(w_1))$ in $H$, then there exists a node $w_2'\in\chi_H^{-1}(\chi_G(w_2))$ satisfying $\disR_G(w_1,w_2)=\disR_H(w_1',w_2')$. We also have $\disR_H(w_1',u)=\disR_G(w_1,u)$ and $\disR_H(w_2',u)=\disR_G(w_2,u)$ because $u$ is the unique node with color $\chi_G(u)$ in $H$. Therefore, $\disR_H(w_1',u)+\disR_H(u,w_2')=\disR_H(w_1',w_2')$ and $u$ is a cut vertex in $H$ (\cref{thm:proof_rdwl_part1_0}).

\subsubsection{Proof of the second part}
\label{sec:proof_rdwl_part2}
We first introduce some notations. As before, we assume $G$ and $H$ are connected and $\ldblbrace \chi_G(w):w\in\gV\rdblbrace=\ldblbrace \chi_H(w):w\in\gV\rdblbrace$. As we will consider multiple cut vertices in the following proof, we adopt the notation $\{\gS_{G,i}(u)\}_{i=1}^{m_G(u)}$, which represents the set of connected components of graph $G$ after removing node $u$. Here, $m_G(u)$ is the number of connected components after removing node $u$, which is greater than 1 if $u$ is a cut vertex. It follows that $\bigcup_{i=1}^{m_G(u)} \gS_{G,i}(u)=\gV\backslash\{u\}$. We further define the index set $\gM_{G}(u):=\{i\in [m_G(u)]:\gS_{G,i}(u)\cap\chi_G^{-1}(\chi_G(u))=\emptyset\}$. By \cref{thm:proof_rdwl_part1_1}, either $|\gM_G(u)|=m_G(u)-1$ or $|\gM_G(u)|=m_G(u)$.

\begin{lemma}
\label{thm:proof_rdwl_part2_0}
Let $u\in\gV$ be a cut vertex of $G$. Let $u'\in\chi_H^{-1}(\chi_G(u))$, then $u'$ is also a cut vertex of $H$. Let $i\in [m_G(u)]$ and $j\in [m_H(u')]$ be two indices and pick nodes $w\in\gS_{G,i}(u)$ and $w'\in\gS_{H,j}(u')$. Assume $w$ and $w'$ have the same color, i.e. $\chi_G(w)=\chi_H(w')$. Then the following holds:
\begin{itemize}[topsep=0pt,leftmargin=30pt]
\setlength{\itemsep}{0pt}
    \item If $i\in\gM_G(u)$ and $j\in\gM_H(u')$, then $\disR_G(w,u)=\disR_H(w',u')$.
    \item If $i\in\gM_G(u)$ and $j\notin\gM_H(u')$, then $\disR_G(w,u)<\disR_H(w',u')$.
\end{itemize}
\end{lemma}
\begin{proof}
Proof of the first bullet: since $i\in\gM_G(u)$, any path from $w$ to a node $u_G\in\chi_G^{-1}(\chi_G(u))$ goes through the cut vertex $u$, implying $\min_{u_G\in\chi_G^{-1}(\chi_G(u))}\disR_G(w,u_G)=\disR_G(w,u)$. Similarly, since $j\in\gM_H(u')$,  $\min_{u_H\in\chi_H^{-1}(\chi_H(u'))}\disR_H(w',u_H)=\disR_H(w',u')$. Since the color of nodes $w$ and $w'$ are the same under RD-WL, we have 
$$\min_{u_H\in\chi_H^{-1}(\chi_H(u'))}\disR_H(w',u_H)=\min_{u_G\in\chi_G^{-1}(\chi_G(u))}\disR_G(w,u_G)$$ and thus $\disR_H(w',u')=\disR_G(w,u)$.

Proof of the second bullet: first note that $\disR_H(w',u')\ge \disR_G(w,u)$ because $$\disR_H(w',u')\ge \min_{u_H\in\chi_H^{-1}(\chi_H(u'))}\disR_H(w',u_H)=\min_{u_G\in\chi_G^{-1}(\chi_G(u))}\disR_G(w,u_G)=\disR_G(w,u).$$
If the lemma does not hold, then $\disR_H(w',u')= \disR_G(w,u)$. Consequently,
\begin{align*}
    \ldblbrace \disR_G(w,u_G):u_G\in\chi_G^{-1}(\chi_G(u))\rdblbrace 
    &=\ldblbrace \disR_G(w,u)+\disR_G(u,u_G):u_G\in\chi_G^{-1}(\chi_G(u))\rdblbrace \\
    &=\ldblbrace \disR_H(w',u')+\disR_H(u',u_H):u_H\in\chi_H^{-1}(\chi_H(u'))\rdblbrace.
\end{align*}
On the other hands, 
$$\ldblbrace\disR_G(w,u_G):u_G\in\chi_G^{-1}(\chi_G(u))\rdblbrace=\ldblbrace\disR_H(w',u_H):u_H\in\chi_H^{-1}(\chi_H(u'))\rdblbrace.$$
Therefore, $\disR_H(w',u_H)=\disR_H(w',u')+\disR_H(u',u_H)$ for all $u_H\in\chi_H^{-1}(\chi_H(u'))$. However, we can choose $u''\in\chi_H^{-1}(\chi_H(u'))\cap\gS_{H,j}(u')$ by definition of $j$, and clearly $\disR_H(w',u'')<\disR_H(w',u')+\disR_H(u',u'')$ because $w'$ and $u''$ are in the same connected component (\cref{thm:proof_rdwl_part1_0}). This yields a contradiction and concludes the proof.
\end{proof}

\begin{corollary}
\label{thm:proof_rdwl_part2_1}
Let $u\in\gV$ be a cut vertex of $G$. Let $u'\in\chi_H^{-1}(\chi_G(u))$, then $u'$ is also a cut vertex of $H$. Pick any $\gS_{G,i}(u)$ and $\gS_{H,j}(u')$ with indices $i\in\gM_G(u)$ and $j\in\gM_H(u')$. Then either of the following holds:
\begin{itemize}[topsep=0pt,leftmargin=30pt]
\setlength{\itemsep}{0pt}
    \item $\ldblbrace\chi_G(w):w\in\gS_{G,i}(u)\rdblbrace=\ldblbrace\chi_H(w):w\in\gS_{H,j}(u')\rdblbrace$.
    \item $\ldblbrace\chi_G(w):w\in\gS_{G,i}(u)\rdblbrace\cap\ldblbrace\chi_H(w):w\in\gS_{H,j}(u')\rdblbrace=\emptyset$.
\end{itemize}
\end{corollary}
\begin{proof}
Assume $\ldblbrace\chi_G(w):w\in\gS_{G,i}(u)\rdblbrace\cap\ldblbrace\chi_H(w):w\in\gS_{H,j}(u')\rdblbrace\neq\emptyset$. Then there exists nodes $w\in\gS_{G,i}(u)$ in $G$ and $w'\in\gS_{H,j}(u')$ in $H$, satisfying $\chi_G(w)=\chi_H(w')$. Our goal is to prove that $\ldblbrace\chi_G(w):w\in\gS_{G,i}(u)\rdblbrace=\ldblbrace\chi_H(w):w\in\gS_{H,j}(u')\rdblbrace$. It thus suffices to prove that for any color $c\in \gC$, $|\chi_G^{-1}(c)\cap\gS_{G,i}(u)|=|\chi_H^{-1}(c)\cap\gS_{H,j}(u')|$.

Define $\gD_G(w,c)=\ldblbrace\disR_G(w,x):x\in\chi_G^{-1}(c)\rdblbrace$ and define $\gD_G(w,c)+d:=\ldblbrace d+d':d'\in\gD_G(w,c)\rdblbrace$. We next claim that $$|\chi_G^{-1}(c)\cap\gS_{G,i}(u)|=|\chi_G^{-1}(c)|-|\gD_G(w,c)\cap(\gD_G(u,c)+\disR_G(w,u))|.$$
This is simply because for any $x\in\chi_G^{-1}(c)$, either $x\in\gS_{G,i}(u)$ or $x\notin\gS_{G,i}(u)$. If $x\notin\gS_{G,i}(u)$, then $\disR_G(w,x)=\disR_G(w,u)+\disR_G(u,x)$ (\cref{thm:proof_rdwl_part1_0}); otherwise, $\disR_G(w,x)\neq\disR_G(w,u)+\disR_G(u,x)$. Similarly, 
$$|\chi_H^{-1}(c)\cap\gS_{H,j}(u')|=|\chi_H^{-1}(c)|-|\gD_H(w',c)\cap(\gD_H(u',c)+\disR_H(w',u'))|.$$
Noting that $|\chi_G^{-1}(c)|=|\chi_H^{-1}(c)|$, $\gD_G(w,c)=\gD_H(w',c)$, and $\disR_G(w,u)=\disR_H(w',u')$ (\cref{thm:proof_rdwl_part2_0}), we obtain $|\chi_G^{-1}(c)\cap\gS_{G,i}(u)|=|\chi_H^{-1}(c)\cap\gS_{H,j}(u')|$ and conclude the proof.
\end{proof}

\begin{remark}
As a special case, \cref{thm:proof_rdwl_part2_0,thm:proof_rdwl_part2_1} also hold when $G=H$. For example, \cref{thm:proof_rdwl_part2_1} implies that for any $\gS_{G,i}(u)$ and $\gS_{G,j}(u)$ such that $\gS_{G,i}(u)\cap\chi_G^{-1}(\chi_G(u))=\gS_{G,j}(u)\cap\chi_G^{-1}(\chi_G(u))=\emptyset$, either of the two items in \cref{thm:proof_rdwl_part2_1} holds.
\end{remark}

\cref{thm:proof_rdwl_part2_0,thm:proof_rdwl_part2_1} leads to the following key corollary:
\begin{corollary}
\label{thm:proof_rdwl_part2_2}
Let $u\in\gV$ be a vertex in $G$ and $u'\in\gV$ be a vertex in $H$. If $\chi_G(u)=\chi_H(u')$, then $m_G(u)=m_H(u')$ and
$$\ldblbrace \ldblbrace\chi_G(w):w\in\gS_{G,i}(u)\rdblbrace \rdblbrace_{i=1}^{m_G(u)}=\ldblbrace \ldblbrace\chi_H(w):w\in\gS_{H,i}(u')\rdblbrace \rdblbrace_{i=1}^{m_H(u')}.$$
\end{corollary}
\begin{proof}
If both $u$ and $u'$ are not cut vertices, \cref{thm:proof_rdwl_part2_2} trivially holds since $m_G(u)=m_H(u')=1$ and $\gS_{G,1}(u)=\gV\backslash\{u\}$, $\gS_{H,1}(u')=\gV\backslash\{u'\}$. Now assume $u$ and $u'$ are both cut vertices. We first claim that
\begin{equation}
\label{eq:proof_rdwl_part2_2_0}
    \textstyle\ldblbrace\chi_G(w):w\in\bigcup_{i\in\gM_G(u)}\gS_{G,i}(u)\rdblbrace=\ldblbrace\chi_H(w):w\in\bigcup_{i\in\gM_H(u')}\gS_{H,i}(u')\rdblbrace.
\end{equation}
To prove the claim, it suffices to prove that for each color $c\in\gC$, 
\begin{equation}
\label{eq:proof_rdwl_part2_2_1}
    \left|\bigcup_{i\in\gM_G(u)}\gS_{G,i}(u)\cap\chi_G^{-1}(c)\right|=\left|\bigcup_{i\in\gM_H(u')}\gS_{H,i}(u')\cap\chi_H^{-1}(c)\right|.
\end{equation}
Note that $|\chi_G^{-1}(c)|=|\chi_H^{-1}(c)|$. Also note that by \cref{thm:proof_rdwl_part2_0}, for any two nodes $w_1\in\bigcup_{i\in\gM_G(u)}\gS_{G,i}(u)\cap\chi_G^{-1}(c)$ and  $w_2\in\bigcup_{i\notin\gM_G(u)}\gS_{G,i}(u)\cap\chi_G^{-1}(c)$, we have $\disR_G(u,w_1)<\disR_G(u,w_2)$. In other words, the following two sets does not intersect:
\begin{align*}
    \textstyle\gD_G(u,c):=\ldblbrace\disR_G(w,u):w\in\bigcup_{i\in\gM_G(u)}\gS_{G,i}(u)\cap\chi_G^{-1}(c)\rdblbrace, \\
    \textstyle\widetilde\gD_G(u,c):=\ldblbrace\disR_G(w,u):w\in\bigcup_{i\notin\gM_G(u)}\gS_{G,i}(u)\cap\chi_G^{-1}(c)\rdblbrace.
\end{align*}
Since $\chi_G(u)=\chi_H(u')$, we have $\gD_G(u,c)\cup\widetilde\gD_G(u,c)=\gD_H(u',c)\cup\widetilde\gD_H(u',c)$. Then $\gD_G(u,c)\cap\widetilde\gD_G(u,c)=\gD_H(u',c)\cap\widetilde\gD_H(u',c)=\emptyset$ implies that $\gD_G(u,c)=\gD_H(u',c)$ and $\widetilde\gD_G(u,c)=\widetilde\gD_H(u',c)$. This proves (\ref{eq:proof_rdwl_part2_2_1}) and thus (\ref{eq:proof_rdwl_part2_2_0}) holds.

We next claim that 
\begin{equation}
\label{eq:proof_rdwl_part2_2_2}
    \ldblbrace \ldblbrace\chi_G(w):w\in\gS_{G,i}(u)\rdblbrace: i\in\gM_G(u) \rdblbrace=\ldblbrace \ldblbrace\chi_H(w):w\in\gS_{H,i}(u')\rdblbrace:   i\in\gM_H(u')\rdblbrace.
\end{equation}
This simply follows by using (\ref{eq:proof_rdwl_part2_2_0}) and \cref{thm:proof_rdwl_part2_1}. Finally, (\ref{eq:proof_rdwl_part2_2_2}) already yields the desired conclusion because:
\begin{itemize}[topsep=0pt,leftmargin=30pt]
\setlength{\itemsep}{0pt}
    \item If $|\gM_G(u)|=m_G(u)$, then (\ref{eq:proof_rdwl_part2_2_0}) implies that $$\textstyle\left|\bigcup_{i\in\gM_H(u')}\gS_{H,i}(u')\right|=\left|\bigcup_{i\in\gM_G(u)}\gS_{G,i}(u)\right|=|\gV|-1$$ and thus $|\gM_H(u')|=m_H(u')$.
    \item If $|\gM_G(u)|=m_G(u)-1$, then analogously $|\gM_H(u')|=m_H(u)-1$. Furthermore,
    \begin{align*}
    \ldblbrace\ldblbrace\chi_G(w):w\in\gS_{G,i}(u)\rdblbrace: i\notin\gM_G(u) \rdblbrace
    =\ldblbrace\ldblbrace\chi_G(w):w\in\gS_{H,i}(u')\rdblbrace: i\notin\gM_H(u') \rdblbrace
    \end{align*}
    because $\ldblbrace\chi_G(w):w\in\gV\backslash\{u\}\rdblbrace=\ldblbrace\chi_H(w):w\in\gV\backslash\{u'\}\rdblbrace$.
\end{itemize}
In both cases, \cref{thm:proof_rdwl_part2_2} holds.
\end{proof}

We are now ready to prove that $\ldblbrace \chi_G(w):w\in\gV\rdblbrace=\ldblbrace \chi_H(w):w\in\gV\rdblbrace$ implies  $\operatorname{BCVTree}(G)\simeq\operatorname{BCVTree}(H)$. Recall that in a block cut-vertex tree $\operatorname{BCVTree}(G)$, there are two types of nodes: all cut vertices of $G$, and all biconnected components of $G$. Each edge in $\operatorname{BCVTree}(G)$ is connected between a cut vertex $u\in \gV$ and a biconnected component $\gB\subset\gV$ such that $u\in\gB$.

Given a fixed RD-WL graph representation $\gR$, consider any graph $G=(\gV,\gE_G)$ satisfying $\ldblbrace\chi_G(w):w\in\gV\rdblbrace=\gR$. First, all cut vertices of $G$ can be determined purely from $\gR$ using the node colors. We denote the cut vertex color multiset as $\gC^\mathrm{V}:=\ldblbrace\chi_G(u):u\text{ is a cut vertex of }G\rdblbrace$. Next, the number $m_G(u)$ for each cut vertex $u$ can be determined only by its color $\chi_G(u)$ (by \cref{thm:proof_rdwl_part2_2}), which is equal to the degree of node $u$ in $\operatorname{BCVTree}(G)$. We now give a procedure to construct $\operatorname{BCVTree}(G)$, which purely depends on $\gR$ rather than the specific graph $G$.

\begin{figure}[t]
    \centering
    \small
    % \setlength\tabcolsep{12pt}
    \begin{tabular}{cc}
        \includegraphics[width=0.43\textwidth]{figure/proof_rdwl_bcvtree_1.pdf} & \includegraphics[width=0.5\textwidth]{figure/proof_rdwl_bcvtree_2.pdf} \\
        (a) The original graph & (b) Illustration of the multisets $\gT(u)$ for each cut vertex $u$.
    \end{tabular}
    \begin{tabular}{cccc}
        \includegraphics[width=0.225\textwidth]{figure/proof_rdwl_bcvtree_3.pdf} & \includegraphics[width=0.225\textwidth]{figure/proof_rdwl_bcvtree_4.pdf} & \includegraphics[width=0.225\textwidth]{figure/proof_rdwl_bcvtree_5.pdf} & \includegraphics[width=0.225\textwidth]{figure/proof_rdwl_bcvtree_6.pdf} \\
        (c) The first step & (d) The second step & (e) The third step & (f) The final step 
    \end{tabular}
    % \vspace{-7pt}
    \caption{Illustrations for constructing the BCVTree given the graph representation $\gR$.}
    \label{fig:proof_rdwl_bcvtree}
    \vspace{-5pt}
\end{figure}

We examine the multisets $\gT(u):=\ldblbrace \ldblbrace\chi_G(w):w\in\gS_{G,i}(u)\rdblbrace \rdblbrace_{i=1}^{m_G(u)}$ for all cut vertices $u$, which only depends on $\gR$ and $\chi_G(u)$ rather than the specific graph $G$ or node $u$ by \cref{thm:proof_rdwl_part2_2}. See \cref{fig:proof_rdwl_bcvtree}(b) for an illustration of $\gT(u)$ for four types of cut vertices $u$. In the first step, we find all cut vertices $u$ such that $\sum_{\gS\in\gT(u)}\mathbf 1[\gC^\mathrm{V}\cap\gS\neq\emptyset]\le 1$ where $\mathbf 1[\cdot]$ is the indicator function. In other words, we find cut vertices $u$ such that there is at most one connected component $\gS_{G,i}(u)$ that contains cut vertices. These cut vertices $u$ will serve as ``leaf (cut vertex) nodes'' in $\operatorname{BCVTree}(G)$, in the sense that it connects to at most one internal node in $\operatorname{BCVTree}(G)$. The number of BCVTree leaf nodes that connect to $u$ are also determined by \cref{thm:proof_rdwl_part2_2}. See \cref{fig:proof_rdwl_bcvtree}(c) for an illustration. After finding all the ``leaf (cut vertex) nodes'', we can then find cut vertex nodes $v$ such that when removing all ``leaf (cut vertex) nodes'' in the BCVTree, $v$ will serve as a ``leaf (cut vertex) node''. To do this, we compute for each cut vertex $v$ and each biconnected component $\gB_v$ associated with $v$, whether $\gB_v$ has no cut vertex or all cut vertices in $\gB_v$ correspond to the ``leaf (cut vertex) nodes'' in $\operatorname{BCVTree}(G)$. Then, we check whether a cut vertex $v$ satisfies $\sum_{\gS\in\gT(v)}\mathbf 1[(\gC^\mathrm{V}\cap\gS)\backslash\gC^\mathrm{V}_u\neq\emptyset]\le 1$, where the set $\gC^\mathrm{V}_u$ contains all colors corresponding to ``leaf (cut vertex) nodes''. These vertices $v$ will serve as new ``leaf (cut vertex) nodes'' when removing all ``leaf (cut vertex) nodes'' in the BCVTree, and the connection between such vertices $v$ and ``leaf (cut vertex) nodes'' can also be determined (see \cref{fig:proof_rdwl_bcvtree}(d) for an illustration). The procedure can be recursively executed until the full BCVTree is constructed (see \cref{fig:proof_rdwl_bcvtree}(f)), and the whole procedure does not depend on the specific graph $G$ and only depends on $\gR$, which completes the proof.

\subsection{Proof of \cref{thm:2fwl_powerful_than_gdwl}}
\label{sec:proof_2fwl_powerful}
Given a graph $G=(\gV,\gE)$, let $\chi_G^t$ be the 2-FWL color mapping after the $t$-th iteration (see \cref{alg:kfwl} for details), and let $\chi_G$ be the stable 2-FWL color mapping. The following result is useful for the subsequent proof:
\begin{lemma}
\label{thm:2fwl_path_base}
Let $u_1,u_2,v_1,v_2\in\gV$ be nodes in graph $G$ and $t$ be an integer. The following holds:
\begin{itemize}
    \item If $\chi_G^t(u_1,v_1)=\chi_G^t(u_2,v_2)$, then $u_1=v_1$ if and only if $u_2=v_2$;
    \item If $\chi_G^t(u_1,v_1)=\chi_G^t(u_2,v_2)$, then $\{u_1,v_1\}\in\gE$ if and only if $\{u_2,v_2\}\in\gE$;
    \item If $\chi_G^t(u_1,v_1)=\chi_G^t(u_2,v_2)$ and $t\ge 1$, then $\deg_G(u_1)=\deg_G(u_2)$ and $\deg_G(v_1)=\deg_G(v_2)$.
\end{itemize}
\end{lemma}
\begin{proof}
By the initial coloring (\ref{eq:kfwl_init}) of 2-FWL, $\chi_G^0(u,v)$ can have the following three types of values:
\begin{equation*}
    \chi_G^0(u_1,v_1)=\left\{\begin{array}{ll}
        c_\mathrm{same} & \text{if }u=v \\
        c_\mathrm{edge} & \text{if }u\neq v\text{ and }\{u,v\}\in\gE\\
        c_\mathrm{other} & \text{if }u\neq v\text{ and }\{u,v\}\notin\gE
    \end{array}\right.
\end{equation*}
where $c_\mathrm{same}, c_\mathrm{edge},c_\mathrm{other}$ are three different colors. Therefore, if $\chi_G^0(u_1,v_1)=\chi_G^0(u_2,v_2)$, then $u_1=v_1$ if and only if $u_2=v_2$, and $\{u_1,v_1\}\in\gE$ if and only if $\{u_2,v_2\}\in\gE$. For the update step,
\begin{equation}
\label{eq:proof_2fwl_base_0}
    \chi_G^t(u,v)=\operatorname{hash}\left(\chi_G^{t-1}(u,v), \ldblbrace (\chi_G^{t-1}(u,w), \chi_G^{t-1}(w,v)):w\in \gV \rdblbrace\right).
\end{equation}
If $\chi_G^1(u_1,v_1)=\chi_G^1(u_2,v_2)$, then (\ref{eq:proof_2fwl_base_0}) implies that $\ldblbrace \chi_G^0(u_1,w):w\in \gV \rdblbrace=\ldblbrace \chi_G^0(u_2,w):w\in \gV \rdblbrace$ and thus $|\ldblbrace w\in\gV:\{u_1,w\}\in\gE\rdblbrace|=|\ldblbrace w\in\gV:\{u_2,w\}\in\gE\rdblbrace|$, namely $\deg_G(u_1)=\deg_G(u_2)$. We can similarly prove that $\deg_G(v_1)=\deg_G(v_2)$.

Finally, note that $\chi_G^t(u_1,v_1)=\chi_G^t(u_2,v_2)$ implies $\chi_G^{t-1}(u_1,v_1)=\chi_G^{t-1}(u_2,v_2)$ using (\ref{eq:proof_2fwl_base_0}). This concludes the proof of the case $t\ge 1$ by a simple induction.
\end{proof}

For a path $P=(x_0,\cdots,x_d)$ (not necessarily simple) in graph $G$ of length $d\ge 1$, define $\omega(P):=(\deg_G(x_1),\cdots,\deg_G(x_{d-1}))$ which is a tuple of length $d-1$. We have the following key lemma:
\begin{lemma}
\label{thm:2fwl_path_key}
Let $t\in\mathbb N$ be a non-negative integer. Given nodes $u_1,u_2,v_1,v_2\in\gV$, if $\chi_G^t(u_1,v_1)=\chi_G^t(u_2,v_2)$, then the following holds:
\begin{itemize}[topsep=0pt,leftmargin=30pt]
\setlength{\itemsep}{0pt}
    \item Denote $\gP_d(u,v)$ be the set of all paths (not necessarily simple) from node $u$ to node $v$ of length $d$. Then $|\gP_{t+1}(u_1,v_1)|=|\gP_{t+1}(u_2,v_2)|$.
    \item Denote $\gQ_d(u,v)$ be the set of all hitting paths (not necessarily simple) from node $u$ to node $v$ of length $d$. Then, $\ldblbrace \omega(Q):Q\in\gQ_{t+1}(u_1,v_1)\rdblbrace=\ldblbrace \omega(Q):Q\in\gQ_{t+1}(u_2,v_2)\rdblbrace$, and $\ldblbrace \omega(Q):Q\in\gQ_{t+1}(v_1,u_1)\rdblbrace=\ldblbrace \omega(Q):Q\in\gQ_{t+1}(v_2,u_2)\rdblbrace$.
\end{itemize}
\end{lemma}
\begin{proof}
We prove the lemma by induction over iteration $t$. We first prove the base case $t=0$. 
\begin{itemize}[topsep=0pt,leftmargin=30pt]
\setlength{\itemsep}{0pt}
    \item If $u_1= v_1$, then by \cref{thm:2fwl_path_base} $u_2=v_2$. Note that obviously $|\gP_1(u,u)|=0$ and $|\gQ_1(u,u)|=0$ for any node $u$, namely $|\gP_1(u_1,u_1)|=|\gP_1(u_2,u_2)|$ and $\gQ_1(u_1,u_1)=\gQ_1(u_2,u_2)=\emptyset$.
    \item Similarly, if $u_1\neq v_1$ and $\{u_1, v_1\}\notin\gE$, then by \cref{thm:2fwl_path_base} $u_2\neq v_2$ and $\{u_2, v_2\}\notin\gE$. We also have $|\gP_1(u_1,v_1)|=|\gP_1(u_2,v_2)|=0$ and $\gQ_1(u_1,v_1)=\gQ_1(u_2,v_2)=\emptyset$.
    \item If $u_1\neq v_1$ and $\{u_1, v_1\}\in\gE$, then by \cref{thm:2fwl_path_base} $u_2\neq v_2$ and $\{u_2, v_2\}\in\gE$. Then $|\gP_1(u_1,v_1)|=|\gP_1(u_2,v_2)|=1$ and $\gQ_1(u_1,v_1)=\gQ_1(u_2,v_2)$ where both sets have a single element that is an empty tuple (0-dimension).
\end{itemize}

Now suppose that the conclusion of \cref{thm:2fwl_path_key} holds in iteration $t$, we will prove that it also holds in iteration $t+1$. First note that for any two nodes $u,v$, $|\gP_{t+1}(u,v)|=\sum_{w\in\gN_G(v)}|\gP_{t+1}(u,w)|$. If $\chi_G^{t+1}(u_1,v_1)=\chi_G^{t+1}(u_2,v_2)$, then by definition of 2-FWL update formula (\ref{eq:proof_2fwl_base_0})
$$\ldblbrace (\chi_G^{t}(u_1,w), \chi_G^t(w,v_1)):w\in \gV \rdblbrace=\ldblbrace (\chi_G^{t}(u_2,w), \chi_G^t(w,v_2)):w\in \gV \rdblbrace.$$
which implies that $\ldblbrace \chi_G^{t}(u_1,w):w\in \gN_G(v_1)\rdblbrace=\ldblbrace \chi_G^{t}(u_2,w):w\in \gN_G(v_2) \rdblbrace$ due to \cref{thm:2fwl_path_base}. Therefore,
\begin{itemize}[topsep=0pt,leftmargin=30pt]
\setlength{\itemsep}{0pt}
    \item By induction, $\ldblbrace |\gP_{t+1}(u_1,w)|:w\in \gN_G(v_1)\rdblbrace=\ldblbrace |\gP_{t+1}(u_2,w)|:w\in \gN_G(v_2) \rdblbrace$. It follows that $\sum_{w\in\gN_G(v_1)}|\gP_{t+1}(u_1,w)|=\sum_{w\in\gN_G(v_2)}|\gP_{t+1}(u_2,w)|$ and thus we have $|\gP_{t+2}(u_1,v_1)|=|\gP_{t+2}(u_2,v_2)|$.
    \item By induction, $\ldblbrace(\chi_G^t(u_1,w),\chi_G^t(w,v_1),\ldblbrace \omega(Q):Q\in\gQ_{t+1}(w,v_1)\rdblbrace):w\in \gN_G(u_1)\rdblbrace=\ldblbrace(\chi_G^t(u_2,w),\chi_G^t(w,v_2),\ldblbrace \omega(Q):Q\in\gQ_{t+1}(w,v_2)\rdblbrace):w\in \gN_G(u_2)\rdblbrace$. Since \cref{thm:2fwl_path_base} says that $\chi_G^t(w,v)\neq \chi_G^t(v,v)$ if $w\neq v$, we have
    \begin{align*}
        &\ldblbrace(\chi_G^t(u_1,w),\ldblbrace \omega(Q):Q\in\gQ_{t+1}(w,v_1)\rdblbrace):w\in \gN_G(u_1)\backslash\{v_1\}\rdblbrace\\
        =&\ldblbrace(\chi_G^t(u_2,w),\ldblbrace \omega(Q):Q\in\gQ_{t+1}(w,v_2)\rdblbrace):w\in \gN_G(u_2)\backslash\{v_2\}\rdblbrace
    \end{align*}
    Further using the third bullet of \cref{thm:2fwl_path_base} and rearranging the two multisets yields
    \begin{align*}
        &\ldblbrace (\deg_G(w),\omega(Q)):w\in \gN_G(u_1)\backslash\{v_1\},Q\in\gQ_{t+1}(w,v_1)\rdblbrace\\
        =& \ldblbrace (\deg_G(w),\omega(Q)):w\in \gN_G(u_2)\backslash\{v_2\},Q\in\gQ_{t+1}(w,v_2)\rdblbrace.
    \end{align*}
    Equivalently, $\ldblbrace \omega(Q):Q\in\gQ_{t+2}(u_1,v_1)\rdblbrace=\ldblbrace \omega(Q):Q\in\gQ_{t+2}(u_2,v_2)\rdblbrace$. We can similarly prove that $\ldblbrace \omega(Q):Q\in\gQ_{t+2}(v_1,u_1)\rdblbrace=\ldblbrace \omega(Q):Q\in\gQ_{t+2}(v_2,u_2)\rdblbrace$.
\end{itemize}
This concludes the proof of the induction step.
\end{proof}

The above lemma directly yields the following corollary:
\begin{corollary}
\label{thm:2fwl_path_key_corollary}
Given nodes $u_1,u_2,v_1,v_2\in\gV$, if $\chi_G(u_1,v_1)=\chi_G(u_2,v_2)$, then $\dis_G(u_1,v_1)=\dis_G(u_2,v_2)$ and $\disR_G(u_1,v_1)=\disR_G(u_2,v_2)$.
\end{corollary}
\begin{proof}
If $\chi_G(u_1,v_1)=\chi_G(u_2,v_2)$, then $\chi_G^t(u_1,v_1)=\chi_G^t(u_2,v_2)$ holds for all $t\ge 0$. By \cref{thm:2fwl_path_key} $|\gP_{t}(u_1,v_1)|=|\gP_{t}(u_2,v_2)|$ holds for all $t\ge 0$ (the case $t=0$ trivially holds). Since $\dis_G(u,v)=\min\{t:|\gP_{t}(u_1,v_1)|>0\}$, we conclude that $\dis_G(u_1,v_1)=\dis_G(u_2,v_2)$. As for the Resistance Distance $\disR_G$, it is equivalent to the Commute Time Distance multiplied by a constant \citep[see also \cref{sec:detail_of_rdwl}]{chandra1996electrical}, i.e. $\disC_G(u,w)=2|\gE|\disR_G(u,w)$. Since $\disC_G(u,v)=\sum_{i=0}^\infty  i\cdot(\sum_{P\in\gQ_i(u,v)} q(P)+\sum_{P\in\gQ_i(v,u)} q(P))$ where $\gQ_i(u,v)$ is the set containing all hitting paths of length $i$ from $u$ to $v$, and $q(P)=1/\left(\deg_G(u)\prod_{i=1}^{d-1} \deg(x_i)\right)$ for a path $P=(x_0,\cdots,x_d)$. By \cref{thm:2fwl_path_key}, we have $\sum_{P\in\gQ_i(u_1,v_1)} q(P)=\sum_{P\in\gQ_i(u_2,v_2)} q(P)$ and $\sum_{P\in\gQ_i(v_1,u_1)} q(P)=\sum_{P\in\gQ_i(v_2,u_2)} q(P)$ for all $i\ge 0$ (the case $i=0$ trivially holds) and thus $\disC_G(u_1,v_1)=\disC_G(u_2,v_2)$, namely $\disR_G(u_1,v_1)=\disR_G(u_2,v_2)$.
\end{proof}

We are now ready to prove \cref{thm:2fwl_powerful_than_gdwl}. 
\begin{theorem}
The 2-FWL algorithm is more powerful than both SPD-WL and RD-WL. Formally, given a graph $G$, let $\chi_G^\mathrm{2FWL}$, $\chi_G^\mathrm{SPDWL}$ and $\chi_G^\mathrm{RDWL}$ be the vertex color mappings for these algorithms, respectively. Then the partition induced by $\chi_G^\mathrm{2FWL}$ is finer than both $\chi_G^\mathrm{SPDWL}$ and $\chi_G^\mathrm{RDWL}$.
\end{theorem}
\begin{proof}
Note that by definition (see \cref{sec:kfwl}), we have $\chi_G(v):=\chi_G(v,v)$ for any node $v\in\gV$. If $\chi_G(v_1)=\chi_G(v_2)$, then by definition of 2-FWL aggregation formula,
$$\ldblbrace (\chi_G(v_1,w),\chi_G(w,v_1)):w\in\gV\rdblbrace=\ldblbrace (\chi_G(v_2,w),\chi_G(w,v_2)):w\in\gV\rdblbrace.$$
Using \cref{thm:kfwl_lemma}, if $\chi_G(v_1,w_1)=\chi_G(v_2,w_2)$ for some nodes $w_1$ and $w_2$, then $\chi_G(w_1)=\chi_G(w_2)$. Therefore, by using \cref{thm:2fwl_path_key_corollary} we obtain that if $\chi_G(v_1)=\chi_G(v_2)$, then
\begin{align*}
    \ldblbrace (\chi_G(w),\dis_G(w,v_1)):w\in\gV\rdblbrace&=\ldblbrace (\chi_G(w),\dis_G(w,v_2)):w\in\gV\rdblbrace,\\
    \ldblbrace (\chi_G(w),\disR_G(w,v_1)):w\in\gV\rdblbrace&=\ldblbrace (\chi_G(w),\disR_G(w,v_2)):w\in\gV\rdblbrace.
\end{align*}
The above equantions show that the partition induced by $\chi_G^\mathrm{2FWL}$ is finer than both $\chi_G^\mathrm{SPDWL}$ and $\chi_G^\mathrm{RDWL}$ and conclude the proof.
\end{proof}

Finally, the following proposition trivially holds and will be used to prove \cref{thm:2fwl_biconnectivity}.
\begin{proposition}
\label{thm:finer_partition}
Given a graph $G=(\gV,\gE_G)$, let $\chi_G$ and $\tilde \chi_G$ be two color mappings induced by two different (general) color refinement algorithms, respectively. If the vertex partition induced by the mapping $\chi_G$ is finer than that of $\tilde \chi_G$, then:
\begin{itemize}[topsep=0pt,leftmargin=30pt]
\setlength{\itemsep}{0pt}
    \item The mapping $\chi_G$ can distinguish cut vertices/edges if $\tilde \chi_G$ can distinguish cut vertices/edges;
    \item The mapping $\chi_G$ can distinguish the isomorphism type of $\operatorname{BCVTree}(G)$/$\operatorname{BCETree}(G)$ if $\tilde\chi_G$ can distinguish the isomorphism type of $\operatorname{BCVTree}(G)$/$\operatorname{BCETree}(G)$.
\end{itemize}
\end{proposition}
\cref{thm:2fwl_biconnectivity} is a simple consequence of \cref{thm:2fwl_powerful_than_gdwl,thm:finer_partition}.

\subsection{Proof of \cref{thm:distance_regular_maintext}}
\label{sec:distance_regular}
In this subsection, we give more fine-grained theoretical results on the expressiveness upper bound of GD-WL by considering the special problem of distinguishing \emph{distance-regular graphs}, a class of hard graphs that are highly relevant to the GD-WL framework. We provide a full characterization of what types of distance-regular graphs different GD-WL algorithms can or cannot distinguish, with both proofs and counterexamples.

Given a graph $G=(\gV,\gE)$, let $\gN_G^i(u)=\{w\in\gV:\dis_G(u,w)=i\}$ be the $i$-hop neighbors of $u$ in $G$ and let $D(G):=\max_{u,v\in\gV}\dis_G(u,v)$ be the diameter of $G$. We say $G$ is distance-regular if for all $i,j\in\mathbb [D(G)]$ and all nodes $u,v,w,x\in\gV$ with $\dis_G(u,v)=\dis_G(w,x)$, we have $|\gN_G^i(u)\cap\gN_G^j(v)|=|\gN_G^i(w)\cap\gN_G^j(x)|$. From the definition, it is straightforward to see that for all $u,v\in\gV$ and $i\in[D(G)]$, $|\gN_G^i(u)|=|\gN_G^i(v)|$, i.e., the number of $i$-hop neighbors is the same for all nodes. We thus denote $\kappa(G)=(k_1,\cdots,k_{D(G)})$ as the $k$-hop-neighbor array where $k_i:=|\gN_G^i(u)|$ with $u\in\gV$ chosen arbitrarily. We next define another important array:
\begin{definition}
    \normalfont (\textbf{Intersection array}) The intersection array of a distance-regular graph $G$ is denoted as $\iota(G)=\{b_0,\cdots,b_{D(G)-1};c_1,\cdots,c_{D(G)}\}$ where $b_i=|\gN_G(u)\cap\gN_G^{i+1}(v)|$ and $c_i=|\gN_G(u)\cap\gN_G^{i-1}(v)|$ with $\dis_G(u,v)=i$.
\end{definition}
We now present our main results.
\begin{theorem}
\label{thm:distance_regular}
    Let $G$ and $H$ be two connected distance-regular graphs. Then the following holds:
    \begin{itemize}[topsep=0pt,leftmargin=30pt]
    \setlength{\itemsep}{0pt}
        \item SPD-WL can distinguish the two graphs if and only if their $k$-hop-neighbor arrays differ, i.e. $\kappa(G)\neq \kappa(H)$.
        \item RD-WL can distinguish the two graphs if and only if their intersection arrays differ, i.e. $\iota(G)\neq\iota(H)$.
        \item 2-FWL can distinguish the two graphs if and only if their intersection arrays differ, i.e. $\iota(G)\neq\iota(H)$.
    \end{itemize}
\end{theorem}
\cref{thm:distance_regular} precisely characterizes the equivalence class of all distance-regular graphs for different types of algorithms. Combined the fact that $\iota(G)=\iota(H)$ implies $\kappa(G)= \kappa(H)$ (see e.g. \citet[page 8]{van2014distance}), we immediately arrive at the following corollary:
\begin{corollary}
\label{thm:distance_regular_corollary}
    RD-WL is strictly more powerful than SPD-WL in distinguishing non-isomorphic distance-regular graphs. Moreover, RD-WL is as powerful as 2-FWL in distinguishing non-isomorphic distance-regular graphs.
\end{corollary}
% \cref{thm:distance_regular_corollary} shows that RD-WL is highly expressive in distinguishing these hard graphs. Importantly, to our knowledge, this is the first result showing that a more efficient algorithm can \emph{match} the expressive power of 2-FWL in distinguishing distance-regular graphs.

\textbf{Counterexamples}. We provide representitive counterexamples in \cref{fig:distance_regular_graphs} for both SPD-WL and RD-WL. In \cref{fig:distance_regular_graphs}(a), both the Dodecahedron and the Desargues graph have 20 vertices and the same $k$-hop-neighbor array $(3,6,6,3,1)$, and thus SPD-WL cannot distinguish them. However, they have the different intersection array (i.e., $\{3,2,1,1,1;1,1,1,2,3\}$ for Dodecahedron and $\{3,2,2,1,1;1,1,2,2,3\}$ for the Desargues graph), and thus RD-WL can distinguish them. In \cref{fig:distance_regular_graphs}(b), we make use of the well-known 4x4 rooks graph and the Shrikhande graph, both of which are strongly regular and thus distance-regular. They have the same intersection array $\{6,3;1,2\}$ and thus both RD-WL and 2-FWL cannot distinguish them although they are non-isomorphic.

\subsubsection{Proof of \cref{thm:distance_regular}}
We first present a lemma that links the definition of distance-regular graph to its intersection array. The proof is based on the Bose-Mesner algebra and its association scheme, and please refer to \citet[Sections 2.5 and 2.6]{van2014distance} for details.
% \begin{lemma}

%     Let $G$ and $H$ be two graphs with the same intersection array, and suppose nodes $u,v,w,x$ satisfy $\dis_G(u,v)=\dis_G(w,x)$. Then $|\gP^G_i(u,v)|=|\gP^H_i(w,x)|$ for all $i\in\mathbb N$, where $\gP^G_i(u, v)$ is the set including all paths (not necessarily simple) from $u$ to $v$ of length $i$ in $G$.
% \end{lemma}
\begin{lemma}
\label{thm:distance_regular_lemma}
    Let $G$ and $H$ be two graphs with the same intersection array, and suppose nodes $u,v,w,x$ satisfy $\dis_G(u,v)=\dis_G(w,x)$. Then $|\gN_G^i(u)\cap\gN_G^j(v)|=|\gN_H^i(w)\cap\gN_H^j(x)|$ for all $i,j\in\mathbb N$.
\end{lemma}

\begin{proof}[Proof of the first item of \cref{thm:distance_regular}]
This part is straightforward. Consider the SPD-WL color mapping $\chi^1_G$ of graph $G$ after the first iteration. Then for two graphs $G,H$ with $n$ nodes, $\chi^1_G(u)=\chi^1_H(v)$ if and only if $|\gN_G^{i}(u)|=|\gN_H^{i}(v)|$ for all $i\in[n-1]$. Therefore, if $\kappa(G)\neq \kappa(H)$, then for any node $u$ in $G$ and $v$ in $H$, $|\gN_G^{j}(u)|=|\gN_H^{j}(v)|$ holds for some $j\in [\max(D(G),D(H))]$ and thus $\chi^1_G(u)\neq \chi^1_H(v)$. Namely, $\chi_G(u)\neq \chi_H(v)$ for all nodes $u$ in $G$ and $v$ in $H$, implying that SPD-WL can distinguish the two graphs. On the other hand, if $\kappa(G)= \kappa(H)$, then for any node $u$ in $G$ and $v$ in $H$ we have $\chi_G^1(u)= \chi_H^1(v)$. Similarly, $\chi_G^t(u)= \chi_H^t(v)$ for any iteration $t\in\mathbb N$, and thus SPD-WL cannot distinguish the two graphs.
\end{proof}

\begin{proof}[Proof of the second item of \cref{thm:distance_regular}]
The key insight is that given a distance-regular graph, the resistance distance between a pair of nodes $(u,v)$ only depends on its SPD. Formally, for any nodes $u,v,w,x$ in a distance-regular graph $G$, $\dis_G(u,v)= \dis_G(w,x)$ implies that $\disR_G(u,v)=\disR_G(w,x)$. 
Actually, we have the following stronger result:
\begin{theorem}
\label{thm:rd_distance_regular}
For any two nodes $u,v$ in a connected distance-regular graph $G$, $\disR_G(u,v)=r_{\dis_G(u,v)}$ where the sequence $\{r_d\}_{d=0}^{D(G)}$ is recursively defined as follows:
\begin{equation}
\label{eq:rd_in_distance_regular_graph}
    r_d=\left\{\begin{array}{ll}
        0 & \text{if }d=0, \\
        \displaystyle r_{d-1}+\frac 2 {n k_{d-1}b_{d-1}}\sum_{i=d}^{D(G)}k_i & \text{if }d\in[D(G)],
    \end{array}\right.
\end{equation}
where $\iota(G)=\{b_0,\cdots,b_{D(G)-1};c_1,\cdots,c_{D(G)}\}$ is the intersection array of $G$ and $\kappa(G)=(k_1,\cdots,k_{D(G)})$ is its $k$-hop-neighbor array.
\end{theorem}
\begin{proof}
Let $\mathbf R\in\mathbb R^{n\times n}$ be the RD matrix. Based on \cref{thm:resiatnce_and_laplacian}, $\mathbf R$ can be expressed as $\mathbf R=\diag(\mathbf M)\mathbf 1\mathbf 1^\top+\mathbf 1\mathbf 1^\top\diag(\mathbf M)-2\mathbf M$, where $\mathbf M=\left(\mathbf L+\frac 1 n \mathbf 1\mathbf 1^\top\right)^{-1}$ and $\mathbf L$ is the graph Laplacian matrix. Now let $\widetilde {\mathbf R}=[r_{\dis_G(u,v)}]_{u,v\in\gV}$ be the matrix with elements defined in (\ref{eq:rd_in_distance_regular_graph}). The key step is to prove that $2\mathbf M=c\mathbf 1\mathbf 1^\top-\widetilde {\mathbf R}$ for some $c\in\mathbb R$. This will yield
$$\mathbf R=\frac 1 2 \left(\diag(c\mathbf 1\mathbf 1^\top-\widetilde {\mathbf R})\mathbf 1\mathbf 1^\top+\mathbf 1\mathbf 1^\top\diag(c\mathbf 1\mathbf 1^\top-\widetilde {\mathbf R})\right)-c\mathbf 1\mathbf 1^\top+\widetilde {\mathbf R}=\widetilde {\mathbf R}$$
(since $\diag(\widetilde {\mathbf R})=\mathbf O$) and finish the proof.

We now prove $2\mathbf M=c\mathbf 1\mathbf 1^\top-\widetilde {\mathbf R}$ for some $c\in\mathbb R$, namely $\left(\mathbf L+\frac 1 n \mathbf 1\mathbf 1^\top\right)\left(c\mathbf 1\mathbf 1^\top-\widetilde {\mathbf R}\right)=2\mathbf I$. Note that $\widetilde{\mathbf R}$ is a symmetric matrix and satisfy $\widetilde{\mathbf R}\mathbf 1=c_1\mathbf 1$ for some $c_1\in\mathbb R$ because $G$ is distance-regular. Combined the fact that $\mathbf L\mathbf 1=\mathbf 0$, we have
$$\left(\mathbf L+\frac 1 n \mathbf 1\mathbf 1^\top\right)\left(c\mathbf 1\mathbf 1^\top-\widetilde {\mathbf R}\right)=\left(c-\frac {c_1} n\right)\mathbf 1\mathbf 1^\top-\mathbf L\widetilde {\mathbf R}.$$
It thus suffices to prove that $\mathbf L\widetilde {\mathbf R}=c\mathbf 1\mathbf 1^\top-2\mathbf I$ for some $c\in\mathbb R$. Let us calculate each element $[\mathbf L\widetilde {\mathbf R}]_{uv}$ ($u,v\in\gV$). We have 
\begin{equation}
\label{eq:proof_distance_regular_graph_0}
    [\mathbf L\widetilde {\mathbf R}]_{uv}=k_1 r_{\dis_G(u,v)}-\sum_{d=0}^{D(G)} r_{d}|\gN_G(u)\cap\gN^d_G(v)|.
\end{equation}
Now consider the following three cases:
\begin{itemize}[topsep=0pt,leftmargin=30pt]
    \setlength{\itemsep}{0pt}
    \item $u=v$. In this case, $r_{\dis_G(u,v)}=0$ and we have $$\sum_{d=1}^{D(G)} r_{d}|\gN_G(u)\cap\gN^d_G(v)|=r_1 k_1=\frac {2(n-1)} n$$ by using $b_0=k_1$ and $k_0=0$. Thus $[\mathbf L\widetilde {\mathbf R}]_{uv}=-\frac {2(n-1)} n$.
    \item $u\neq v$ and $\dis_G(u,v)<D(G)$. Denote $j=\dis_G(u,v)$. In this case, in (\ref{eq:proof_distance_regular_graph_0}) the term $\gN_G(u)\cap\gN^d_G(v)\neq\emptyset$ only when $d\in\{j-1,j,j+1\}$, and by definition of intersection array we have $|\gN_G(u)\cap\gN^{j-1}_G(v)|=c_j$, $|\gN_G(u)\cap\gN^{j+1}_G(v)|=b_j$, and $|\gN_G(u)\cap\gN^{j}_G(v)|=|\gN_G(u)|-c_j-b_j=k_1-c_j-b_j$. Therefore,
    \begin{align*}
        [\mathbf L\widetilde {\mathbf R}]_{uv}&=k_1 r_{j}- r_{j-1}c_j-r_{j}(k_1-b_j-c_j)-r_{j+1}b_j\\
        &=c_j(r_j-r_{j-1})+b_j(r_j-r_{j+1})\\
        &=\frac {2c_j} {n k_{j-1}b_{j-1}}\sum_{i=j}^{D(G)}k_i-\frac {2b_j} {n k_{j}b_{j}}\sum_{i=j+1}^{D(G)}k_i\\
        &=\frac 2 {nk_j}\left(\sum_{i=j}^{D(G)}k_j-\sum_{i=j+1}^{D(G)}k_j\right)=\frac 2 n,
    \end{align*}
    where in the second last step we use the recursive relation of $r_j$, and in the last step we use the fact that $k_j=\frac {k_{j-1}b_{j-1}}{c_j}$ for any $j\in[D(G)]$ (see e.g. \citet[page 8]{van2014distance}).
    \item $u\neq v$ and $\dis_G(u,v)=D(G)$. This case is similar as the previous one. Denote $j=\dis_G(u,v)$, and $\gN_G(u)\cap\gN^d_G(v)\neq\emptyset$ only when $d\in\{j-1,j\}$. We have
    \begin{align*}
        [\mathbf L\widetilde {\mathbf R}]_{uv}&=k_1 r_{j}- r_{j-1}c_j-r_{j}(k_1-c_j)\\
        &=c_j(r_j-r_{j-1})\\
        &=\frac {2c_j} {n k_{j-1}b_{j-1}}k_j=\frac 2 n,
    \end{align*}
    where we again use $k_j=\frac {k_{j-1}b_{j-1}}{c_j}$.
\end{itemize}
Combining the above three cases, we conclude that $\mathbf L\widetilde{\mathbf R}=\frac 2 n \mathbf 1\mathbf 1^\top-2\mathbf I$, which finishes the proof.
\end{proof}

We are now ready to prove the main result. Let $G=(\gV_G,\gE_G)$ and  $H=(\gV_H,\gE_H)$ be two distance-regular graphs. We first prove that if $\iota(G)=\iota(H)$, then RD-WL cannot distinguish the two graphs. This is a simple consequence of \cref{thm:rd_distance_regular}. Combined with the fact that $\kappa(G)=\kappa(H)$, we have $\{\disR_G(u,w):w\in\gV_G\}=\{\disR_H(v,w):w\in\gV_H\}$ for any nodes $u\in\gV_G$ and $v\in\gV_H$. Therefore, after the first iteration, the RD-WL color mappings $\chi^1_G$ and $\chi^1_H$ satisfy $\chi_G^1(u)=\chi_H^1(v)$ for all $u\in\gV_G$ and $v\in\gV_H$. Similarly, after the $t$-th iteration we still have $\chi_G^t(u)=\chi_H^t(v)$ for all $u\in\gV_G$ and $v\in\gV_H$ and thus RD-WL cannot distinguish the two graphs.

It remains to prove that if $\iota(G)\neq\iota(H)$, then RD-WL can distinguish the two graphs. First observe that in \cref{thm:rd_distance_regular}, $r_i<r_j$ holds for any $i<j$. Therefore, for any nodes $u\in\gV_G$ and $v\in\gV_H$, $\{\disR_G(u,w):w\in\gV_G\}=\{\disR_H(v,w):w\in\gV_H\}$ if and only if 
\begin{equation}
\label{eq:proof_rd_distance_regular_2}
    \{\ldblbrace r_i(G)\rdblbrace\times k_i(G):i\in[D(G)]\}=\{\ldblbrace r_i(H)\rdblbrace\times k_i(H):i\in[D(H)]\},
\end{equation}
where $\ldblbrace r\rdblbrace\times k$ is a multiset containing $k$ repeated elements of value $r$. If $\iota(G)\neq\iota(H)$, then there exists a minimal index $d$ such that $b_i(G)=b_i(H)$ and $c_{i+1}(G)=c_{i+1}(H)$ for all $i<d$ but $b_i(G)\neq b_i(H)$ or $c_{i+1}(G)\neq c_{i+1}(H)$. It follows by \cref{thm:rd_distance_regular} that $r_i(G)=r_i(H)$ and $k_i(G)=k_i(H)$ for all $i\le d$ but either $r_{d+1}(G)\neq r_{d+1}(H)$ (if $b_d(G)\neq b_d(H)$) or $k_{d+1}(G)\neq k_{d+1}(H)$ (if $b_d(G)= b_d(H)$ and $c_{d+1}(G)\neq c_{d+1}(H)$). Therefore, (\ref{eq:proof_rd_distance_regular_2}) does not hold and $\chi_G^1(u)\neq \chi_H^1(v)$ for any $u\in\gV_G$ and $v\in\gV_H$, namely, RD-WL can distinguish the two graphs.
\end{proof}

\begin{proof}[Proof of the third item of \cref{thm:distance_regular}]
First, if $\iota(G)\neq\iota(H)$, then 2-FWL can distinguish graphs $G$ and $H$. This is simply due to the fact that 2-FWL is more powerful than RD-WL (\cref{thm:2fwl_powerful_than_gdwl}). It remains to prove that if $\iota(G)=\iota(H)$, then 2-FWL cannot distinguish graphs $G$ and $H$.

Let $\chi_G^t:\gV_G\times\gV_G\to\gC$ be the 2-FWL color mapping of graph $G$ after $t$ iterations. We aim to prove that for any nodes $u,v\in\gV_G$ and $w,x\in\gV_H$, if $\dis_G(u,v)=\dis_G(w,x)$, then $\chi_G^t(u,v)=\chi_H^t(w,x)$ for any $t\in\mathbb N$. We prove it by induction. The base case of $t=0$ trivially holds. Now suppose the case of $t$ holds and let us consider the color mapping after $t+1$ iterations. By the 2-FWL update rule (\ref{alg:kfwl}),
\begin{equation}
    \chi_G^{t+1}(u,v)=\operatorname{hash}\left(\chi_G^{t}(u,v), \ldblbrace (\chi_G^{t}(u,z), \chi_G^{t}(z,v)):z\in \gV_G \rdblbrace\right).
\end{equation}
It thus suffices to prove that
\begin{equation}
\label{eq:proof_2fwl_distance_regular_1}
    \ldblbrace (\chi_G^t(u,z), \chi_G^t(z,v)):z\in \gV_G \rdblbrace=\ldblbrace (\chi_H^t(w,z), \chi_H^t(z,x)):z\in \gV_H \rdblbrace.
\end{equation}
By \cref{thm:distance_regular_lemma}, we have
\begin{equation*}
    \ldblbrace (\dis_G(u,z), \dis_G(z,v)):z\in \gV_G \rdblbrace=\ldblbrace (\dis_H(w,z), \dis_H(z,x)):z\in \gV_H \rdblbrace.
\end{equation*}
This already yields (\ref{eq:proof_2fwl_distance_regular_1}) by the  induction result of iteration $t$. We thus complete the proof.
\end{proof}

\section{Further Discussions with Prior Works}

\subsection{Known metrics for measuring the expressive power of GNNs}
\label{sec:other_metrics}
In this subsection, we review existing metrics used in prior works to measure the expressiveness of GNNs. We will discuss the limitations of these metrics and argue why biconnectivity may serve as a more reasonable and compelling criterion in designing powerful GNN architectures.

\textbf{WL hierarchy}. Since the discovery of the relationship between MPNNs and 1-WL test \citep{xu2019powerful,morris2019weisfeiler}, the WL hierarchy has been considered as the most standard metric to guide designing expressive GNNs. However, achieving an expressive power that matches the 2-FWL test is already highly difficult. Indeed, each iteration of the 2-FWL algorithm already requires a complexity of $\Omega(n^3)$ time and $\Theta(n^2)$ space for a graph with $n$ vertices \citep{immerman1990describing}. Therefore, it is impossible to design expressive GNNs using this metric while maintaining its computational efficiency. Moreover, whether achieving higher-order WL expressiveness is necessary and helpful for real-world tasks has been questioned by recent works \citep{velivckovic2022message}.


\textbf{Structural metrics}. Another line of works thus sought different metrics to measure the expressive power of GNNs. Several popular choices are the ability of counting substructures \citep{arvind2020weisfeiler,chen2020can,bouritsas2022improving}, detecting cycles \citep{loukas2020graph,vignac2020building,huang2023boosting}, calculating the graph diameter \citep{garg2020generalization,loukas2020graph} or other graph-related (combinatorial) problems \citep{sato2019approximation}. Yet, all these metrics have a common drawback: the corresponding problems may be \emph{too hard} for GNNs to solve. Indeed, we show in \cref{tab:complexity} that solving any above task requires a computation complexity that grows super-linear w.r.t. the graph size even using advanced algorithms. Therefore, it is quite natural that standard MPNNs are not expressive for these metrics, since \emph{no GNNs can solve these tasks while being efficient}. Consequently, instead of using GNNs to directly \emph{learn} these metrics, these works had to use a precomputation step which can be costly in the worst case.


\begin{table}[h]
    \vspace{-8pt}
    \centering
    \small
    \caption{The best computational complexity of known algorithms for solving different graph problems. Here $n$ and $m$ are the number of nodes and edges of a given graph, respectively. }
    \label{tab:complexity}
    \vspace{2pt}
    \begin{tabular}{lll}
    \toprule
    Metric & Complexity & Reference\\
    \midrule
    $k$-FWL & $\Omega(n^{k+1})$ &\citep{immerman1990describing}\\
    Counting/detecting triangles & $O(\min(n^{2.376},m^{3/2}))$ &\citep{alon1997finding}\\
    Detecting cycles of an odd length $k\ge 3$ & $O(\min(n^{2.376},m^2))$ &\citep{alon1997finding}\\
    Detecting cycles of an even length $k\ge 4$ & $O(n^2)$ &\citep{yuster1997finding}\\
    Calculating the graph diameter & $O(nm)$ & --\\
    \midrule
    Detecting cut vertices & $\Theta(n+m)$ &\citep{tarjan1972depth}\\
    Detecting cut edges & $\Theta(n+m)$ &\citep{tarjan1972depth}\\
    \bottomrule
    \end{tabular}
    % \vspace{-7pt}
\end{table}

Due to the lack of proper metrics, most subsequent works mainly justify the expressive power of their proposed GNNs by focusing on regular graphs \citep[to list a few]{li2020distance,bevilacqua2022equivariant,bodnar2021topological,feng2022powerful,velingker2022affinity}, which hardly appear in practice. In contrast, the biconnectivity metrics proposed in this paper are different from all prior metrics, in that $(\mathrm{i})$ it is a basic graph property and has significant values in both theory and applications; $(\mathrm{i})$ it can be efficiently calculated with a complexity \emph{linear} in the graph size, and thus it is reasonable to expect that these metrics should be learned by expressive GNNs.

\subsection{GNNs with distance encoding}
\label{sec:related_work_distance}
In this subsection, we review prior works that are related to our proposed GD-WL. In the research field of expressive GNNs, the idea of incorporating distance first appeared in \citet{li2020distance}, where the authors mainly considered using distance encoding as \emph{node features} and showed that distance can help distinguish regular graphs. They also considered an approach similar to $k$-hop aggregation by incorporating distance into the message-passing procedure (but without a systematic study). \citet{zhang2021nested} designed a subgraph GNN that also uses (generalized) distance encoding as node features in each subgraph. \citet{ying2021transformers} designed a Transformer architecture that incorporates distance information and \emph{empirically} showed excellent performance. Very recently, \citet{feng2022powerful} formally studied the expressive power of $k$-hop GNNs. Yet, they still restricted the analysis to regular graphs. The concurrent work of \citet{abboud2022shortest} designed the shortest path network which is highly similar to our proposed SPD-WL. They showed the resulting model can alleviate the bottlenecks and over-squashing problems for MPNNs \citep{alon2021bottleneck,topping2022understanding} due to the increased receptive field. 

Compared with prior works, our contribution lies in the following three aspects:
\begin{itemize}[topsep=0pt,leftmargin=30pt]
\setlength{\itemsep}{0pt}
    \item We formalize the principled and more expressive GD-WL framework, which comprises SPD-WL as a special case. Our framework is theoretically clean and generalizes all prior works in a unified manner.
    \item We systematically and theoretically analyze the expressive power of SPD-WL for \emph{general} graphs and highlight a fundamental advantage in distinguishing edge-biconnectivity.
    \item We design a Transformer-based GNN that is \emph{provably as expressive as} GD-WL. Thus, our framework is not only for theoretical analysis, but can also be easily implemented with good empirical performance on real-world tasks.
\end{itemize}

\textbf{Discussions with the concurrent work of \citet{velingker2022affinity}}. After the initial submission, we became aware of a concurrent work \citep{velingker2022affinity} which also explored the use of Resistance Distance to enhance the expressiveness of standard MPNNs. Here, we provide a comprehensive comparison of these two works. Overall, the main difference is that their approach incorporates RD (and several related affinity measures) into \emph{node/edge features} (like \citet{zhang2021nested}), while we combine RD to design a new \emph{WL aggregation} procedure. As for the theoretical analysis, they only give a few toy examples of regular graphs to justify the expressive power beyond the 1-WL test, while we give a systematic analysis of the power of RD-WL for \emph{general} graphs and point out that it is fully expressive for vertex-biconnectivity. In \citet{velingker2022affinity}, the authors also made comparisons to SPD and conjectured that RD may have additional advantages than SPD in terms of expressiveness. In fact, this question is formally answered in our work, by proving that RD-WL is expressive for vertex-biconnectivity while SPD-WL is not. Another important contribution of our work is that we provide an \emph{upper bound} of the expressive power of RD-WL to be 2-FWL (3-WL), which reveals the limit of incorporating RD information. We also provide a precise and complete characterization for the expressiveness of RD-WL in distinguishing distance-regular graphs, which reveals that RD-WL can \emph{match} the power of 2-FWL in distinguishing these hard graphs.


\section{Implementation of Generalized Distance Weisfeiler-Lehman}
\label{sec:detail_of_gdwl}
In this section, we give implementation details of GD-WL and our proposed GNN architecture. We also give detailed analysis of its computation complexity. Below, assume the input graph $G=(\gV,\gE)$ has $n$ vertices and $m$ edges.

\subsection{Preprocessing Shortest Path Distance}
\label{sec:detail_of_spdwl}
Shortest Path Distance can be easily calculated using the Floyd-Warshall algorithm \citep{floyd1962algorithm}, which has a complexity of $\Theta(n^3)$. For sparse graphs typically encountered in practice (i.e. $m=o(n^2)$), a more clever way is to use breadth-first search that computes the distance from a given node to all other nodes in the graph. The time complexity can be improved to $\Theta(nm)$.

\subsection{Preprocessing Resistance Distance}
\label{sec:detail_of_rdwl}
In this subsection, we first describe several important properties of Resistance Distance. Based on these properties, we give a simple yet efficient algorithm to calculate Resistance Distance.

\textbf{Equivalence between Resistance Distance (RD) and Commute Time Distance (CTD)}. \citet{chandra1996electrical} established an important relationship between RD and CTD, by proving that $\disC_G(u,v)=2m\disR_G(u,v)$ holds for any graph $G$ and any nodes $u,v\in\gV$. Here, the Commute Time Distance is defined as $\disC_G(u,v):=h_G(u,v)+h_G(v,u)$ where $h_G(u,v)$ is the average \emph{hitting} time from $u$ to $v$ in a random walk. Concretely, $h_G(u,v)$ is equal to the average number of edges passed in a random walk when starting from $u$ and reaching $v$ for the first time. Mathmatically, it satisfies the following recursive relation:
\begin{equation}
\label{eq:commute_time}
    h_G(u,v)=\left\{\begin{array}{ll}
        0 & \text{if }u=v, \\
        \infty & \text{if }u\text{ and }v \text{ are in different connected components},\\
        1+\frac 1 {\deg_G(u)}\sum_{w\in\gN_G(u)}h_G(u,v) & \text{otherwise}.
    \end{array}\right.
\end{equation}
The above equation can be used to calculate CTD and thus RD, as we will show later.

\textbf{Resistance Distance is a graph metric}. We say a function $d_G:\gV\times\gV\to\mathbb R$ is a graph metric if it is non-negative, positive semidefinite, symmetric, and satisfies triangular inequality. Let $G$ be a connected graph. Then Resistance Distance $\disR_G$ is a valid graph metric because:
\begin{itemize}[topsep=0pt,leftmargin=30pt]
\setlength{\itemsep}{0pt}
    \item (Positive semidefiniteness) $\disR_G(u,v)\ge 0$ holds for any $u,v\in\gV$. Moreover, $\disR_G(u,v)= 0$ iff $u=v$.
    \item (Symmetry) $\disR_G(u,v)=\disR_G(v,u)$ holds for any $u,v\in\gV$.
    \item (Triangular Inequality) For any $u,v,w\in\gV$, $\disR_G(u,v)+\disR_G(v,w)\ge \disR_G(u,w)$. This can be seen from the definition of CTD, since $\disC_G(u,v)+\disC_G(v,w)$ is equal to the average hitting time from $u$ to $w$ under the condition of passing node $v$, which is obviously larger than $\disR_G(u,w)$.
\end{itemize}

\textbf{Comparing RD with SPD}. It is easy to see that RD is always no larger than SPD, i.e. $\disR_G(u,v)\le \dis_G(u,v)$. This is because for any subgraph $G'$ of $G$, we have $\disR_G(u,v)\le \disR_{G'}(u,v)$, and when $G'$ is chosen to contain only the edges that belong to the shortest path between $u$ and $v$, we have $\disR_{G'}(u,v)= \dis_{G}(u,v)$. Therefore, the range of RD is the same as SPD, i.e. $0\le\disR_G(u,v)\le n-1$. However, unlike SPD which is an integer, RD can be a general rational number. RD can thus be seen as a more fine-grained distance metric than SPD. Nevertheless, RD is still discrete and there are only finitely many possible values of $\disR_G(u,v)$ when $n$ is fixed.

\textbf{Relationship to graph Laplacian}. We have the following theorem:
\begin{theorem}
\label{thm:resiatnce_and_laplacian}
Let $G=(\gV,\gE)$ be a connected graph, $\gV=[n]$, and let $\mathbf L\in\mathbb S^{n}$ be the graph Laplacian. Then $$\disR_G(i,j)=M_{i,i}+M_{j,j}-2M_{i,j},$$ where $\mathbf M\in\mathbb S^n$ is a symmetric matrix defined as $$\mathbf M=\left(\mathbf L+\frac 1 n \mathbf 1\mathbf 1^\top\right)^{-1}.$$
\end{theorem}
\begin{proof}
Denote $\vd=(\deg_G(1),\cdots,\deg_G(n))^\top$. Define the probability matrix $\mathbf P$ such that $P_{ij}=0$ if $\{i,j\}\notin\gE$ and $P_{ij}=1/\deg_G(i)$ if $\{i,j\}\in\gE$. Then for any $i\neq j$, (\ref{eq:commute_time}) can be equivalently written as
\begin{equation}
    h(i,j)=1+\sum_{k=1}^n P_{ik}h(k,j)-P_{ij} h(j,j).
\end{equation}
Now define a matrix $\tilde{\mathbf H}$ such that $\tilde H_{ij}=1+\sum_{k=1}^n P_{ik}\tilde H_{kj}-P_{ij} \tilde H_{jj}$, then $\tilde H_{ij}=h(i,j)$ for all $i\neq j$ (although $\tilde H_{ii}\neq 0=h(i,i)$). $\tilde{\mathbf H}$ can be equivalently written as
\begin{equation}
\label{eq:proof_rd_1}
    \tilde{\mathbf H}=\mathbf 1\mathbf 1^\top+\mathbf P\tilde{\mathbf H}-\mathbf P\diag (\tilde{\mathbf H}),
\end{equation}
where $\diag(\tilde{\mathbf H})$ is the diagnal matrix with elements $\tilde H_{ii}$ for $i\in[n]$.

We first calculate $\diag(\tilde{\mathbf H})$. Noting that $\vd^\top\mathbf P=\vd$, we have
\begin{equation*}
    \vd^\top\tilde{\mathbf H}=\vd^\top\mathbf 1\mathbf 1^\top+\vd^\top(\tilde {\mathbf H}-\diag (\tilde{\mathbf H})),
\end{equation*}
and thus $\vd^\top\diag (\tilde{\mathbf H})=\vd^\top\mathbf 1\mathbf 1^\top$, namely
\begin{equation}
\label{eq:proof_rd_2}
    \tilde H_{ii}=\frac 1 {d_i} \vd^\top\mathbf 1=\frac {2m}{d_i}.
\end{equation}
Now define $\mathbf H=\tilde{\mathbf H}-\diag (\tilde{\mathbf H})$, then $H_{ij}=h(i,j)$ for all $i,j\in[n]$. We will calculate $\mathbf H$ in the following proof. We first write (\ref{eq:proof_rd_1}) equivalently as $\mathbf H+\diag (\tilde{\mathbf H})=\mathbf 1\mathbf 1^\top+\mathbf P\mathbf H$. Then by multiplying $\mathbf D$, we have
\begin{equation}
    \mathbf D(\mathbf I-\mathbf P)\mathbf H=\mathbf D\mathbf 1\mathbf 1^\top-\mathbf D\diag (\tilde{\mathbf H}).
\end{equation}
Using the fact that $\mathbf D(\mathbf I-\mathbf P)=\mathbf L$ and (\ref{eq:proof_rd_2}), we obtain
\begin{equation}
\label{eq:proof_rd_3}
    \mathbf L\mathbf H=\mathbf D\mathbf 1\mathbf 1^\top-2m\mathbf I.
\end{equation}
Next, noting that $\mathbf L\mathbf 1=\mathbf 0$, we have
\begin{equation}
\label{eq:proof_rd_4}
    \mathbf L=\left(\mathbf L+\frac 1 n \mathbf 1\mathbf 1^\top\right)\left(\mathbf I-\frac 1 n \mathbf 1\mathbf 1^\top\right).
\end{equation}
One important property is that the matrix $\left(\mathbf L+\frac 1 n \mathbf 1\mathbf 1^\top\right)$ is invertible (see \citet[Theorem 4]{gutman2004generalized} for a proof). Combining (\ref{eq:proof_rd_3}) and (\ref{eq:proof_rd_4}) we have
\begin{equation}
\label{eq:proof_rd_5}
    \left(\mathbf I-\frac 1 n \mathbf 1\mathbf 1^\top\right)\mathbf H=\left(\mathbf L+\frac 1 n \mathbf 1\mathbf 1^\top\right)^{-1}\left(\mathbf D\mathbf 1\mathbf 1^\top-2m\mathbf I\right)=\mathbf M\left(\mathbf D\mathbf 1\mathbf 1^\top-2m\mathbf I\right).
\end{equation}
By taking diagonal elements and noting that $\diag(\mathbf H)=\mathbf O$, we otain
\begin{equation}
    -\frac 1 n\diag\left( \mathbf 1\mathbf 1^\top\mathbf H\right)=\diag\left(\mathbf M\mathbf D\mathbf 1\mathbf 1^\top\right)-2m\diag\left(\mathbf M\right)
\end{equation}
Namely, 
\begin{equation}
\label{eq:proof_rd_6}
    \frac 1 n \mathbf H^\top\mathbf 1=-\mathbf M\mathbf D\mathbf 1+2m\diag\left(\mathbf M\right)\mathbf 1.
\end{equation}
Substituting (\ref{eq:proof_rd_6}) into (\ref{eq:proof_rd_5}) yields
\begin{equation}
    \mathbf H=\mathbf M\left(\mathbf D\mathbf 1\mathbf 1^\top-2m\mathbf I\right)-\mathbf 1\mathbf 1^\top\mathbf D\mathbf M +2m\mathbf 1\mathbf 1^\top\diag\left(\mathbf M\right).
\end{equation}
Therefore,
\begin{equation}
    \mathbf H+\mathbf H^\top=2m(\mathbf 1\mathbf 1^\top\diag\left(\mathbf M\right)+\diag\left(\mathbf M\right)\mathbf 1\mathbf 1^\top-2\mathbf M).
\end{equation}
This finally yields $\disR_G(i,j)=\frac 1 {2m} \disC_G(i,j)=\frac 1 {2m}(\mathbf H+\mathbf H^\top)=M_{i,i}+M_{j,j}-2M_{i,j}$ and concludes the proof.
\end{proof}

\textbf{Computational Complexity}. The graph Laplacian can be calculated in $O(n^2)$ time, and $\mathbf M$ can be calculated by matrix inversion which requires $O(n^3)$ time. Therefore, the overall computational complexity is $O(n^3)$ (or $O(n^{2.376})$ using advanced matrix multiplication algorithms).

For sparse graphs typically encountered in practice (i.e. $m=o(n^2)$), one may similarly ask whether a complexity that depends on $m$ can be achieved. We conjecture that it should be possible. Below, we give another algorithm to calculate $\left(\mathbf L+\frac 1 n \mathbf 1\mathbf 1^\top\right)^{-1}$. Note that the graph Laplacian $\mathbf L$ can be equivalently written as $\mathbf L=\mathbf E\mathbf E^\top$, where $\mathbf E\in\mathbb R^{n\times m}$ is defined as
\begin{equation}
    E_{ij}=\left\{\begin{array}{ll}
        1 &  \text{if }\epsilon_j=\{i,k\} \text{ and } k>i \\
        -1 & \text{if }\epsilon_j=\{i,k\} \text{ and } k<i \\
        0 &  \text{if }i\notin \epsilon_j
    \end{array}\right.
\end{equation}
where we denote $\gE=\{\epsilon_1,\cdots,\epsilon_m\}$. Let $\mathbf E=[\ve_1,\cdots,\ve_m]$ where $\ve_i\in\mathbb R^n$, then $\mathbf M=\left(\frac 1 n\mathbf 1\mathbf 1^\top+\sum_{i=1}^m \ve_i\ve_i^\top\right)^{-1}$. Noting that each $\ve_i$ is highly sparse with only two non-zero elements. We suspect that one can obtain an $O(nm)$ complexity using techniques similar to the Sherman-Morrison-Woodbury update. We leave it as an open problem.

% Let us assume that the first $n-1$ edges form a spanning tree in $G$ (which can be obtained by rearranging the order of the edges). Denote $\mathbf A^{(k)}=\frac 1 n\mathbf 1\mathbf 1^\top+\sum_{i=1}^k \ve_i\ve_i^\top$ for $k\in[m]$. Note that $\mathbf A^{(k)}$ is not invertible for $k\in[n-2]$ because the subgraph corresponding to the first $k$ edges is not connected. On the other hand, $\mathbf A^{(k)}$ is invertible for $k\ge n-1$ since the subgraph is connected (see \citet[Theorem 4]{gutman2004generalized}).

% We first calculate $(\mathbf A^{(n-1)})^{-1}$, which can be done by recursively computing $(\mathbf A^{(k)})^{\dag}$ for $k\in[n-1]$ where ${\dag}$ denotes the pseudo-inverse. We use the following key result:

% \begin{theorem}
% \citep[Theorem 1]{meyer1973generalized} Let $\mathbf A\in\mathbb S^n$ be a positive-semidefinite matrix and $\vv\in\mathbb R^n$. If $\vv$ is not in the colomn space of $\mathbf A$, then
% \begin{equation}
% \label{eq:meyer}
%     (\mathbf A+\vv\vv^\top)^{\dag}=\mathbf A^{\dag}-\frac {\mathbf A^{\dag}\vv\vu^\top+\vu\vv^\top\mathbf A^{\dag}}{\vu^\top\vu}+(1+\vv^\top\mathbf A^{\dag}\vv)\frac{\vu\vu^\top}{(\vu^\top\vu)^2},
% \end{equation}
% where $\vu=(\mathbf I-\mathbf A\mathbf A^\dag)\vv$.
% \end{theorem}
% Let $\mathbf A$ in (\ref{eq:meyer}) be $\mathbf A^{(k)}$ and $\vv$ be $\ve_{k+1}$, then (\ref{eq:meyer}) provides a way to calculate $(\mathbf A^{(k+1)})^{\dag}$, because $\ve_{k+1}$ is not in the column space of $\mathbf A^{(k)}$ by the assumption that $\{\epsilon_1,\cdots,\epsilon_{n-1}\}$ is a spanning tree. We claim that (\ref{eq:meyer}) can be computed in $\Theta(n)$ time due to the following three observations:
% \begin{itemize}[topsep=0pt,leftmargin=30pt]
% \setlength{\itemsep}{0pt}
%     \item Since $\vv$ is sparse with only two non-nero elements, $\mathbf A^{\dag}\vv$ can be calculated in $\Theta(n)$ time;
%     \item Since $\mathbf A$ is a sparse matrix with $O(k)$ elements, combined with the previous item, $\vu$ can be calculated in $\Theta(n)$ time;
%     \item Since the matrix $\mathbf I-\mathbf A\mathbf A^\dag$ is diagonal, $\vu$ is sparse with no more than two elements. Therefore, combined with the first item, the matrix $\mathbf A^{\dag}\vv\vu^\top$ is sparse with only $\Theta(n)$ elements, and $\vu\vu^\top$ is sparse with only $\Theta(1)$ elements. Therefore, we only needs to modify $\Theta(n)$ elements from $\mathbf A^\dag$ to obtain $(\mathbf A+\vv\vv^\top)^{\dag}$.
% \end{itemize}
% We next calculate $(\mathbf A^{(m)})^{-1}$ based on $(\mathbf A^{(n-1)})^{-1}$, which again can be done by recursively computing $(\mathbf A^{(k)})^{\dag}$ for $k\in\{n,\cdots,m\}$. We use the Sherman-Morrison-Woodbury formula:
% \begin{equation}
% \label{eq:woodbury}
%     (\mathbf A+\vv\vv^\top)^{-1}=\mathbf A^{-1}-\frac{\mathbf A^{-1}\vv\vv^\top\mathbf A^{-1}}{1+\vv^\top\mathbf A^{-1}\vv}.
% \end{equation}
% (\ref{eq:woodbury}) provides a way to calculate $(\mathbf A^{(k+1)})^{-1}$ based on $(\mathbf A^{(k)})^{-1}$. Again using the sparsity of $\vv$, each iteration can be computed in $\Theta(n)$ time.

% Therefore, we finally conclude that the whole procedure can be calculated in $\Theta(nm)$ time.

\subsection{Transformer-based implementation}
\label{sec:transformer}
\textbf{Graphormer-GD}. The model is built on the Graphormer~\citep{ying2021transformers} model, which use the Transformer \citep{vaswani2017attention} as the backbone network. A Transformer block consists of two layers: a self-attention layer followed by a feed-forward layer, with both layers having normalization (e.g., LayerNorm~\citep{ba2016layer}) and skip connections~\citep{he2016deep}. Denote $\mathbf X^{(l)}\in\mathbb{R}^{n\times d}$ as the input to the $(l+1)$-th block and define $\mathbf X^{(0)}=\mathbf X$, where $n$ is the number of nodes and $d$ is the feature dimension. For an input $\mathbf X^{(l)}$, the $(l+1)$-th block works as follows:
\begin{eqnarray}
    \label{eqn:attn-mat}
    \mathbf A^h(\mathbf X^{(l)}) &=& \mathrm{softmax}\left(\mathbf X^{(l)} \mathbf W_Q^{l,h}(\mathbf X^{(l)} \mathbf W_K^{l,h})^{\top}\right);\\
    \label{eqn:attn}
    \hat{\mathbf X}^{(l)} &=& \mathbf X^{(l)}+ \sum_{h=1}^H\mathbf A^h(\mathbf X^{(l)}) \mathbf X^{(l)} \mathbf W_V^{l,h}\mathbf W_O^{l,h};\\
    \label{eqn:ffn}
    \mathbf X^{(l+1)}&=&\hat{\mathbf X}^{(l)}+ \mathrm{GELU} (\hat{\mathbf X}^{(l)}\mathbf W_1^l)\mathbf W_2^l,
\end{eqnarray}
where $\mathbf W_O^{l,h} \in \R^{d_H \times d}$, $\mathbf W_Q^{l,h}, \mathbf W_K^{l,h}, \mathbf W_V^{l,h} \in \R^{d \times d_H}$, $\mathbf W_1^l \in \R^{d \times r}, \mathbf W_2^l \in \R^{r \times d}$, $H$ is the number of attention heads, $d_H$ is the dimension of each head, and $r$ is the dimension of the hidden layer. $\mathbf A^h(\mathbf X)$ is usually referred to as the attention matrix.

Note that the self-attention layer and the feed-forward layer introduced in (\ref{eqn:attn}) and (\ref{eqn:ffn}) do not encode any structural information of the input graph. As stated in Section~\ref{sec:gdwl}, we incorporate the distance information into the attention layers of our Graphormer-GD model. The calculation of the attention matrix in (\ref{eqn:attn-mat}) is modified as:
\begin{equation}
    \label{eqn:attn-mat-gd}
    \mathbf A^h(\mathbf X^{(l)}) = \phi_1^{l,h}(\mathbf D)\odot\mathrm{softmax}\left(\mathbf X^{(l)} \mathbf W_Q^{l,h}(\mathbf X^{(l)} \mathbf W_K^{l,h})^{\top}+\mathbf \phi_2^{l,h}(\mathbf D)\right);
\end{equation}
where $\mathbf D\in\mathbb R^{n\times n}$ is the distance matrix such that $D_{uv}=d_G(u,v)$, $\phi_1^h$ and $\phi_2^h$ are element-wise functions applied to $\mathbf D$, and $\odot$ denotes the element-wise multiplication. In this way, the graph structural information can be captured by our Graphormer-GD model.

As stated in Section~\ref{sec:gdwl}, we mainly consider two distance metrics: Shortest Path Distance $\dis_G$ and Resistance Distance $\disR_G$. For SPD, we follow~\citet{ying2021transformers} to use their shortest path distance encoding. Formally, let $\mathbf D^{\text{SPD}}$ be the SPD matrix such that $D^\mathrm{SPD}_{uv}=\dis_G(u,v)$. The function $\phi_1$ and $\phi_2$ can simply be parameterized by two learnable vectors $\vv^1$ and $\vv^2$, so that $\phi_1(D^{\mathrm{SPD}}_{uv})$ is a learnable scalar corresponding to $v^1_{D^{\mathrm{SPD}}_{uv}}$ (and similarly for $\phi_2$). If two nodes $u$ and $v$ are not in the same connected component, i.e., $D^{\mathrm{SPD}}_{uv}=\infty$, a special learnable scalar is assigned. For RD, we use the Gaussian Basis kernels~\citep{scholkopf1997comparing} to encode the value since it may not be an integer. The encoded values from different Gaussian Basis kernels are concatenated and further transformed by a two-layer MLP. We integrate both the SPD encoding and the RD encoding to obtain $\phi_1^{l,h}(\mathbf D)$ and $\phi_2^{l,h}(\mathbf D)$. Note that these two matrices are parameterized by different sets of parameters. Following \citet{ying2021transformers}, we also incorporate the degree of each node in the input layer using a degree embedding.


\textbf{Relationship between Graphormer-GD and GD-WL}.
As stated in Section~\ref{sec:gdwl}, the expressive power of Graphormer-GD is at most as powerful as GD-WL. We will prove that it is actually as powerful as GD-WL under mild assumptions. We first restate the Lemma 5 from~\citet{xu2019powerful}, which shows that sum aggregators can represent injective functions over multisets.
\begin{lemma}
    \label{thm:lemma-injective-multiset}
    \citep[Lemma 5]{xu2019powerful}
    Assume the set $\mathcal{X}$ is countable. Then there exists a function $f:\mathcal{X}\to\mathbb{R}^n$ so that the function $h(\hat\gX):=\sum_{x\in \hat\gX}f(x)$ is unique for each multiset $\hat \gX\subset\mathcal{X}$ of bounded size. Moreover, any multiset function $g$ can be decomposed as $g(\hat\gX)=\phi(\sum_{x\in \hat\gX}f(x))$ for some function $\phi$.
\end{lemma}
We are now ready to present the detailed proof of the \cref{thm:Graphormer-GD-gdwl}, which is restated as follows: 
\begin{theorem}
\label{thm:Graphormer-GD-gdwl-restate}
Graphormer-GD is at most as powerful as GD-WL. Moreover, when choosing proper functions $\phi_1^h$ and $\phi_2^h$ and using a sufficiently large number of heads and layers, Graphormer-GD is as powerful as GD-WL.
\end{theorem}

\begin{proof}
Consider all graphs with no more than $n$ nodes. The total number of possible values of both SPD and RD are thus finite and depends on $n$ (see \cref{sec:detail_of_rdwl}). Let
\begin{equation*}
    \gD_n=\{(\dis_G(u,v),\disR_G(u,v)):G=(\gV,\gE), |\gV|\le n,u,v\in\gV\}
\end{equation*}
denote the set of all possible pairs $(\dis_G(u,v),\disR_G(u,v))$. Since $\gD_n$ is finite, we can list its elements as $\gD_n=\{d_{G,1},\cdots,d_{G,|\gD_n|}\}$. Without abuse of notation, denote $d_G(u,v)=(\dis_G(u,v),\disR_G(u,v))$. Then the GD-WL aggregation in (\ref{eq:gdwl}) can be reformulated as follows:
\begin{equation}
    \label{eq:gdwl-equivalent}
    \begin{aligned}
    \chi_G^t(v)&:= \operatorname{hash}\left((
        \chi_G^{t,1}(v),
        \chi_G^{t,2}(v),
        \cdots,
        \chi_G^{t,|\gD_n|}(v)
    )\right), \\
    \text{where }\chi_G^{t,k}(v)&:=
    \ldblbrace \chi_G^{t-1}(u):u\in \gV, \textcolor{black}{d_G(u,v)=d_{G,k}} \rdblbrace.
    \end{aligned}
\end{equation}
Intuitively, this reformulation indicates that in each iteration, GD-WL updates the color of node $v$ by hashing a tuple of color multisets, where each multiset is obtained by injectively aggregating the colors of all nodes $u\in \gV$ with certain distance configuration to node $v$. Therefore, to express GD-WL, the model suffices to update the representation of each node following the above procedure.

We show Graphormer-GD can achieve this goal. Recall that for the $h$-th head, the attention matrix is defined as $\phi_1^{h}(\mathbf D)\odot\mathrm{softmax}\left(\mathbf X \mathbf W_Q^{h}(\mathbf X \mathbf W_K^{h})^{\top}+\mathbf \phi_2^{h}(\mathbf D)\right)$. For the function $\phi_1^{h}$, we define it to be the indicator function $\phi_1^{h}(d):=\mathbb I(d=d_{G,h})$. For the function $\phi_2^{h}$, we set it to be constant irrespective to the matrix $\mathbf D$. Let $\mathbf W_Q^{h},\mathbf W_K^{h}$ be zero matrices. It can be seen that the term $\mathrm{softmax}\left(\mathbf X \mathbf W_Q^{h}(\mathbf X \mathbf W_K^{h})^{\top}+\mathbf \phi_2^{h}(\mathbf D)\right)$ reduces to $\frac{1}{|\gV|}\vone \vone^{\top}$, and thus for each node $v$, the output in the $h$-th attention head is the sum aggregation of representations of node $u$ satisfying $d_G(u,v)=d_{G,h}$. Formally, 
\begin{equation*}
    \left[\mathbf A^h(\mathbf X^{(l)})\mathbf X^{(l)}\right]_{v}=\frac 1 {|\gV|}\sum_{d_G(u,v)=d_{G,h}} \left[\mathbf X^{(l)}\right]_u.
\end{equation*}
Note that the constant $\frac{1}{|\gV|}$ can be extracted with an additional head and be concatenated to the node representations. Moreover, the node representation $\mathbf X$ is processed via the feed-forward network in the previous layer (see (\ref{eqn:ffn}). Thus, we can invoke \cref{thm:lemma-injective-multiset} and prove that the $h$-th attention head in Graphormer-GD can implement an injective aggregating function for $\ldblbrace \chi_G^{t-1}(u):u\in \gV, {d_G(u,v)=d_{G,h}} \rdblbrace$. Therefore, by using a sufficiently large number of attention heads, the multiset representations $\chi_G^{t,k},k\in[|\gD_n|]$ can be injectively obtained.

Finally, the multi-head attention defined in (\ref{eqn:attn}) is equivalent to first concatenating the output of each attention head and then using a linear mapping to transform the results. Thus, the concatenation is clearly an injective mapping of the tuple of multisets $\left(\chi_G^{t,1},\chi_G^{t,2},...,\chi_G^{t,|\gD_n|}\right)$. When the linear mapping has irrelational weights, the projection will also be injective. Therefore, one attention layer followed by the feed-forward network can implement the aggregation formula (\ref{eq:gdwl-equivalent}). Thus, our Graphormer-GD is able to simulate the GD-WL when using a sufficiant number of layers, which concludes the proof.
\end{proof}

\section{Experimental Details}

\subsection{Synthetic Tasks}
\label{sec:synthetic_detail}
\textbf{Data Generation and Evaluation Metrics.} 
We carefully design several graph generators to examine the expressive power of compared models on graph biconnectivity tasks. First, we include the two families of graphs presented in \cref{example:1,example:2} (\cref{sec:counterexample_proof}). We further introduce a rich family of regular graphs with both cut vertices and cut edges. Each graph in this family is constructed by first randomly generating several connected components and then linking them via cut edges while simultaneously ensuring that each node has the same degree. Combining the above three families of hard graphs, we online generate data instances to train the compared models. For each data instance, the total number of nodes is upper bounded by 120. We use graph-level accuracy as the metric. That is, for each graph, the prediction of the model is considered correct only when all and only the cut vertices/edges are correctly identified. We use different seeds to repeat the experiments 5 times and report the average accuracy.

\textbf{Baselines.} We choose several baselines with their expressive power being at different levels. First, we consider classic MPNNs including GCN~\citep{kipf2017semisupervised}, GAT~\citep{velivckovic2018graph}, and GIN~\citep{bouritsas2022improving}. The expressive power of these GNNs is proven to be at most as powerful as the 1-WL test~\citep{xu2019powerful}. We also compare the Graph Substructure Network~\citep{bouritsas2022improving}, which extracts graph substructures to improve the expressive power of MPNNs. The substructure counts are incorporated into node features or the aggregation procedure. Lastly, we also compare the Graphormer model~\citep{ying2021transformers}, which achieved impressive performance in several world competitions~\citep{ying2021first,shi2022benchmarking,luo2022one}.

\textbf{Settings.} 
We employ a 6-layer Graphormer-GD model. The dimension of hidden layers and feed-forward layers is set to 768. The number of Gaussian Basis kernels is set to 128. The number of attention heads is set to 64. The batch size is set to 32. We use AdamW~\citep{kingma2014adam} as the optimizer and set its hyperparameter $\epsilon$ to 1e-8 and $(\beta_1,\beta_2)$ to (0.9, 0.999). The peak learning rate is set to 9e-5. The model is trained for 100k steps with a 6K-step warm-up stage. After the warm-up stage, the learning rate decays linearly to 0. All models are trained on 1 NVIDIA Tesla V100 GPU.

\subsection{Real-world Tasks}
\label{sec:realworld_detail}

We conduct experiments on the popular benchmark dataset: ZINC from Benchmarking-GNNs \citep{dwivedi2020benchmarking}. It is a real-world dataset that consists of 250K molecular graphs. The task is to predict the constrained solubility of a molecule, which is an important chemical property for drug discovery. We train our models on both the ZINC-Full and ZINC-Subset (12K selected graphs following \citet{dwivedi2020benchmarking}).

\textbf{Baselines.} For a fair comparison, we set the parameter budget of the model to be around 500K following~\cite{dwivedi2020benchmarking}. We compare our Graphormer-GD with several competitive baselines, which mainly fall into five categories: Message Passing Neural Networks (MPNNs), High-order GNNs, Substructure-based GNNs, Subgraph GNNs, and Graph Transformers.

First, we compare several classic MPNNs including Graph Convolution Network (GCN)~\citep{kipf2017semisupervised}, Graph Isomorphism Network (GIN)~\citep{xu2019powerful}, Graph Attention Network (GAT)~\citep{velivckovic2018graph}, GraphSAGE~\citep{hamilton2017inductive} and MPNN(sum)~\citep{gilmer2017neural}. Besides, we also include several popularly used models. Mixture Model Network (MoNet)~\citep{monti2017geometric} introduces a general architecture to
learn on graphs and manifolds using the Bayesian Gaussian Mixture Model. Gated Graph ConvNet (GatedGCN) considers residual connections, batch normalization, and edge gates to design an anisotropic variant of GCN. We compare the GatedGCN with positional encodings. Principal Neighborhood Aggregation (PNA)~\citep{corso2020principal} combines multiple aggregators with degree-scalers.

% Hierarchical Inter Message Passing (HIMP)~\citep{fey2020hierarchical} extracts both molecular graph representations and associated junction trees.

Second, we compare two higher-order Graph Neural Networks: RingGNN~\citep{chen2019equivalence} and 3WLGNN~\citep{maron2019provably} following~\citet{dwivedi2020benchmarking}. RingGNN extends the family of order-2 Graph $G$-invariant Networks without going into higher order tensors and is able to distinguish between non-isomorphic regular graphs where order-2 $G$-invariant networks provably fail. 3WLGNN uses rank-2 tensors to build the neural network and is proved to be equivalent to the 3-WL test on graph isomorphism problems. 


Third, we compare two representative types of substructure-based GNNs. The Graph Substructure Network~\citep{bouritsas2022improving} extracts graph substructures to improve the expressive power of MPNNs. The substructure counts are incorporated into node features or the aggregation procedure. We also compare the Cellular Isomorphism Network~\citep{bodnar2021cellular}, which extends theoretical results on Simplicial Complexes to regular Cell Complexes. Such generalization provides a powerful set of graph lifting transformations with a hierarchical message passing procedure.

Moreover, we compare several Subgraph GNNs. Nested Graph Neural Network (NGNN)~\citep{zhang2021nested} represents a graph with rooted subgraphs instead of rooted subtrees. It extracts a local subgraph around each node and applies a base GNN to each subgraph to learn a subgraph representation. The whole-graph representation is then obtained by pooling these subgraph representations. GNN-AK~\citep{zhao2022stars} follows a similar manner to develop Subgraph GNNs with different generation policies. Equivariant Subgraph Aggregation Networks (ESAN)~\citep{bevilacqua2022equivariant} develops a unified framework that includes per-layer aggregation across subgraphs, which are generated using pre-defined policies like edge deletion and ego-networks. Subgraph Union Network (SUN)~\citep{frasca2022Understanding} is developed based on the symmetry analysis of a series of existing Subgraph GNNs and an upper bound on their expressive power, which theoretically unifies previous architectures and performs well across several graph representation learning benchmarks.

Last, we compare several Graph Transformer models. GraphTransformer (GT)~\citep{dwivedi2021generalization} uses the Transformer model on graph tasks, which only aggregates the information from neighbor nodes to ensure graph sparsity, and proposes to use Laplacian eigenvector as positional encoding. Spectral Attention Network (SAN)~\citep{kreuzer2021rethinking} uses a learned positional encoding (LPE) that can take advantage of the full Laplacian spectrum to learn the position of each node in a given graph. Graphormer~\citep{ying2021transformers} develops the centrality encoding, spatial encoding, and edge encoding to incorporate the graph structure information into the Transformer model. Universal RPE (URPE)~\citep{luo2022your} first shows that there exist continuous sequence-to-sequence functions which RPE-based Transformers cannot approximate, and develops a novel and universal attention module called Universal RPE-based Attention. The effectiveness of URPE has been verified across language and graph benchmarks (e.g., the ZINC dataset).

\textbf{Settings.} Our Graphormer-GD consists of 12 layers. The dimension of hidden layers and feed-forward layers are set to 80. The number of Gaussian Basis kernels is set to 128. The number of attention heads is set to 8. The batch size is selected from [128, 256, 512]. We use AdamW~\citep{kingma2014adam} as the optimizer, and set its hyperparameter $\epsilon$ to 1e-8 and $(\beta_1,\beta_2)$ to (0.9, 0.999). The peak learning rate is selected from [4e-4, 5e-4]. The model is trained for 600k and 800k steps with a 60K-step warm-up stage for ZINC-Subset and ZINC-Full respectively. After the warm-up stage, the learning rate decays linearly to zero. The dropout ratio is selected from [0.0, 0.1]. The weight decay is selected from [0.0, 0.01]. All models are trained on 4 NVIDIA Tesla V100 GPUs.


\subsection{More Tasks}
\label{sec:node_task}
\looseness=-1\textbf{Node-level Tasks.} We further conduct experiments on real-world node-level tasks. Following~\citet{li2020distance}, we benchmark our model on two real-world graphs: Brazil-Airports and Europe-Airports, both of which are air traffic networks and are collected by \citet{ackland2005mapping} from the government websites. The nodes in each graph represent airports and each edge represents that there are commercial flights between the connected nodes. The Brazil-Airports graph has 131 nodes, 1038 edges in total and its diameter is 5. The Europe-Airports graph has 399 nodes, 5995 edges in total and its diameter is 5. The airport nodes are divided into 4 different levels according to the annual passenger flow distribution by 3 quantiles: 25\%, 50\%, and 75\%. The task is to predict the level of each airport node. We follow~\citet{li2020distance} to split the nodes of each graph into train/validation/test subsets with the ratio being 0.8/0.1/0.1, respectively. The test accuracy of the best checkpoint on the validation set is reported. We use different seeds to repeat the experiments 20 times and report the average accuracy.

Following~\citet{li2020distance}, we choose several competitive baselines including classical MPNNs (GCN, GraphSAGE, GIN), Struc2vec and Distance-encoding based GNNs (DE-GNN-SPD, DE-GNN-LP, DEA-GNN-SPD). We refer interested readers to~\citet{li2020distance} for detailed descriptions of baselines. For our Graphormer-GD, the dimension of hidden layers and feed-forward layers are set to 80. The number of layers is selected from [3, 6]. The number of Gaussian Basis kernels is set to 128. The number of attention heads is set to 8. The batch size is selected from [4, 8, 16, 32]. We use AdamW~\citep{kingma2014adam} as the optimizer, and set its hyperparameter $\epsilon$ to 1e-8 and $(\beta_1,\beta_2)$ to (0.9, 0.999). The peak learning rate is selected from [2e-4, 7e-5, 4e-5]. The total number of training steps is selected from [500, 1000, 2000]. The ratio of the warm-up stage is set to 10\%.  After the warm-up stage, the learning rate decays linearly to zero. The dropout ratio is selected from [0.0, 0.1, 0.5]. All models are trained on 1 NVIDIA Tesla V100 GPUs.

The results are presented in Table~\ref{tab:node-level-tasks}. We can see that our model outperforms these baselines on both datasets with a slightly larger variance value due to the small scale of the datasets.


\begin{table}[h]
    \vspace{-5pt}
    % \setlength\tabcolsep{2pt}
    \small
    \centering
    \caption{Average Accuracy on Brazil-Airports and Europe-Airports datasets. Experiments are repeated for 20 times with different seeds. We use * to indicate the best performance.}
    \label{tab:node-level-tasks}
    \vspace{2pt}
    % \resizebox{1.0\textwidth}{!}{ \renewcommand{\arraystretch}{1.0}
    \begin{tabular}{lll}
    \toprule
    Model & Brazil-Airports & Europe-Airports\\
    \midrule
    GCN~\citep{kipf2017semisupervised} & 64.55$\pm$4.18 & 54.83$\pm$2.69\\
    GraphSAGE~\citep{hamilton2017inductive} & 70.65$\pm$5.33 & 56.29$\pm$3.21\\
    GIN~\citep{xu2019powerful} & 71.89$\pm$3.60 & 57.05$\pm$4.08\\
    Struc2vec~\citep{ribeiro2017struc2vec} & 70.88$\pm$4.26 & 57.94$\pm$4.01 \\
    DE-GNN-SPD~\citep{li2020distance} & 73.28$\pm$2.47 & 56.98$\pm$2.79 \\
        DE-GNN-LP~\citep{li2020distance} & 75.10$\pm$3.80 & 58.41$\pm$3.20 \\
    DEA-GNN-SPD~\citep{li2020distance} & 75.37$\pm$3.25 & 57.99$\pm$2.39 \\
    \midrule
    Graphormer-GD (ours) & 77.69$\pm$6.39* & 59.23$\pm$4.05*\\
    \bottomrule
    \end{tabular}
    % }
\end{table}

\subsection{Efficiency Evaluation}
We further conduct experiments to measure the efficiency of our approach by profiling the time cost per training epoch.  We
compare the efficiency of Graphormer-GD with other baselines along with the number of model parameters on the ZINC-subset from \citet{dwivedi2020benchmarking}. The number of layers and the hidden dimension of our Graphormer-GD are set to 12 and 80 respectively. The number of attention heads is set to 8. The batch size is set to 128, which is the same as the settings of all baselines. We run profiling of all models on a 16GB NVIDIA Tesla V100 GPU. For all baselines, we evaluate the time costs based on the publicly available codes of~\citet{dwivedi2020benchmarking} and~\citet{ying2021transformers}. The results are presented in Table~\ref{tab:efficiency-evaluation}.

From Table~\ref{tab:efficiency-evaluation}, we can draw the following conclusions. Firstly, the efficiency of Graphormer-GD is in the same order of magnitude as classic MPNNs despite the fact that the computation complexity of Graphormer-GD is higher than MPNNs (i.e., $\Theta(n^2)$ v.s. $\Theta(n+m)$ for a graph with $n$ nodes and $m$ edges). This may be due to the high parallelizability of the Transformer layers. Secondly, Graphormer-GD is much more efficient than higher-order GNNs as reflected by the computation complexity in \cref{tab:summary_of_results}. Finally, Graphormer-GD is almost as efficient as the original Graphormer, since the newly introduced module to encode the Resistance Distance takes negligible additional time compared to that of the whole architecture.

\begin{table}[h]
    \vspace{-5pt}
    % \setlength\tabcolsep{2pt}
    \small
    \centering
    \caption{Efficiency Evaluation of different GNN models. We report the time per training epoch (seconds) as well as the number of model parameters.}
    \label{tab:efficiency-evaluation}
    \vspace{2pt}
    % \resizebox{1.0\textwidth}{!}{ \renewcommand{\arraystretch}{1.0}
    \begin{tabular}{llc}
    \toprule
    Model & \# Params & Time (s)\\
    \midrule
    GCN~\citep{kipf2017semisupervised} & 505,079 & 5.85 \\
    GraphSAGE~\citep{hamilton2017inductive} & 505,341 & 6.02 \\
        MoNet~\citep{monti2017geometric} & 504,013 & 7.19 \\
    GIN~\citep{xu2019powerful} & 509,549 & 8.05 \\
    GAT~\citep{velivckovic2018graph} & 531,345 & 8.28 \\
        GatedGCN-PE~\citep{bresson2017residual} & 505,011 & 10.74 \\
        RingGNN~\citep{chen2019equivalence} & 527,283 & 178.03  \\
        3WLGNN~\citep{maron2019provably} & 507,603 & 179.35 \\
        % ESAN~\citep{bevilacqua2022equivariant,frasca2022Understanding} & 525,913 & 15.21 \\
        Graphormer~\citep{ying2021transformers} & 489,321 & 12.26 \\
        \midrule
    Graphormer-GD (ours) & 502,793 & 12.52\\
    \bottomrule
    \end{tabular}
    % }
\end{table}

% For ESAN with 500k parameters, we evaluate the time costs based on the publicly available codes of~\citet{bevilacqua2022equivariant} and~\citet{frasca2022Understanding}.
% \begin{table}[h]
%     \vspace{-5pt}
% 	\centering
% 	\small
% 	\caption{Efficiency Evaluation of different GNN models.}
% 	\label{tab:efficiency-evaluation}
% 	\vspace{2pt}
% 	\begin{tabular}{llc}
% 	\toprule
% 	Model & \# Params & Time per Epoch (s)\\
% 	\midrule
% 	GCN~\citep{kipf2017semisupervised} & 505,079 & 5.85 \\
% 	GraphSAGE~\citep{hamilton2017inductive} & 505,341 & 6.02 \\
%         MoNet~\citep{monti2017geometric} & 504,013 & 7.19 \\
% 	GIN~\citep{xu2019powerful} & 509,549 & 8.05 \\
% 	GAT~\citep{velivckovic2018graph} & 531,345 & 8.28 \\
%         GatedGCN-PE~\citep{bresson2017residual} & 505,011 & 10.74 \\
%         RingGNN~\citep{chen2019equivalence} & 527,283 & 178.03  \\
%         3WLGNN~\citep{maron2019provably} & 507,603 & 179.35 \\
%         % ESAN~\citep{bevilacqua2022equivariant,frasca2022Understanding} & 525,913 & 15.21 \\
%         Graphormer~\citep{ying2021transformers} & 489,321 & 12.26 \\
%         \midrule
% 	Graphormer-GD (ours) & 502,793 & 12.52\\
% 	\bottomrule
% 	\end{tabular}
% \end{table}

% \vspace{-2pt}
% \subsection{Real-world tasks}
% \label{exp:real}
% \vspace{-2pt}
% \begin{table}[t]
% \vspace{-15pt}
% \caption{Mean Absolute Error (MAE) on ZINC test set. Following~\citet{dwivedi2020benchmarking}, the parameter budget of compared models is set to 500k. We use $^{*}$ to indicate the best performance.}
%   \vspace{2px}
%   \label{tab:zinc}
%   \small
%   \centering
%   \resizebox{0.9\textwidth}{!}{ \renewcommand{\arraystretch}{1.08}
%     \begin{tabular}{c|lccc}
%     \toprule
%     \multirow{2}{*}{Method} & \multirow{2}{*}{Model} & \multirow{2}{*}{\#Param} & \multicolumn{2}{c}{Test MAE}\\
%     & & & ZINC-Subset & ZINC-Full \\ \midrule
    
%     \multirow{9}{*}{MPNNs}
%     & GIN~\citep{xu2019powerful} & 509,549 & 0.526$\pm$0.051 &  0.088$\pm$0.002 \\
%     & GraphSAGE~\citep{hamilton2017inductive} & 505,341 & 0.398$\pm$0.002 & 0.126$\pm$0.003 \\
%     & GAT~\citep{velivckovic2018graph} & 531,345 & 0.384$\pm$0.007 & 0.111$\pm$0.002 \\
%     & GCN~\citep{kipf2017semisupervised} & 505,079 & 0.367$\pm$0.011 & 0.113$\pm$0.002 \\
%     & MoNet~\citep{monti2017geometric} & 504,013 & 0.292$\pm$0.006 & 0.090$\pm$0.002 \\
%     % & GatedGCN-PE~\citep{bresson2017residual} & 0.214$\pm$0.006 & - \\
%     & MPNN(sum)~\citep{gilmer2017neural} & 480,805 & 0.145$\pm$0.007 & - \\
%     % & HIMP~\citep{fey2020hierarchical} & 614,516 & 0.151$\pm$0.006 & 0.036$\pm$0.002 \\
%     & PNA~\citep{corso2020principal} & 387,155 & 0.142$\pm$0.010 & - \\ \midrule
    
%     \multirow{2}{*}{}
%     Substructure Count & GSN~\citep{bouritsas2022improving} & - & 0.101$\pm$0.010 & - \\ \midrule
%     Cellular Complex & CIN-Small~\citep{bodnar2021cellular} & - & 0.094$\pm$0.004 & - \\ \midrule
    
%     \multirow{3}{*}{Subgraph GNNs}
%     & NGNN~\citep{zhang2021nested} & - & 0.111$\pm$0.003 & - \\
%     % & DS-GNN (EGO)~\citep{bevilacqua2022equivariant} & - & 0.115$\pm$0.004 & - \\
%     % & DS-GNN (EGO+)~\citep{bevilacqua2022equivariant} & - & 0.105$\pm$0.003 & - \\
%     % & DSS-GNN (EGO)~\citep{bevilacqua2022equivariant} & - & 0.099$\pm$0.003 & - \\
%     & DSS-GNN (EGO+)~\citep{bevilacqua2022equivariant} & - & 0.097$\pm$0.006 & - \\
%     & GNN-AK~\citep{zhao2022stars} & - & 0.105$\pm$0.010 & - \\
%     % & GNN-AK-CTX~\citep{zhao2022stars} & - & 0.093$\pm$0.002 & - \\
%     & GNN-AK+~\citep{zhao2022stars} & - & 0.091$\pm$0.011 & - \\ \midrule
    
%     \multirow{3}{*}{Graph Transformers}
%     & GT~\citep{dwivedi2021generalization} & 588,929 & 0.226$\pm$0.014 & - \\
%     & SAN~\citep{kreuzer2021rethinking} & 508,577 & 0.139$\pm$0.006 & - \\
%     & Graphormer~\citep{ying2021transformers} & 489,321 & 0.122$\pm$0.006 & 0.052$\pm$0.005 \\ 
%     % & URPE~\citep{luo2022your} & - & 0.086$\pm$0.007 & 0.028$\pm$0.002 \\
%     \midrule
    

%     GD-WL & Graphormer-GD (ours) & 502,793 &   ~~0.081$\pm$0.009$^{*}$ & ~~0.025$\pm$0.004$^{*}$ \\\bottomrule
%     \end{tabular}
%     }
%     \vspace{-10pt}
% \end{table}
% \textbf{PCQM4Mv2.} PCQM4Mv2 is a large-scale graph representation benchmark from the OGB Large-Scale Challenge~\cite{hu2021ogb}. It is a quantum chemistry dataset originally curated under the PubChemQC project~\citep{maho2015pubchemqc,nakata2017pubchemqc}. The number of molecules is 3.7 million in total. For each molecule, the model is required to predict the DFT-calculated HOMO-LUMO energy gap given its 2D molecular graph. The HOMO-LUMO gap is one of the most practically-relevant quantum chemical properties of molecules since it is related to reactivity, photoexcitation, and charge transport. On this task, we follow~\citep{ying2021transformers} to set the parameter budget of our Graphormer-GD for fair comparison. In particular, our model consists of 12 layers. The dimension of hidden layers and feed-forward layers are set to 768. The number of attention heads is set to 32.
\end{document}
